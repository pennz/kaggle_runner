{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "id,colab_type,colab,language,-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "shonenkov_training_pipeline.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjXcuhX9BBaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ojbkqifBBaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "29e110ff-15bd-44b5-9e54-fcce03c8d114"
      },
      "source": [
        "%%bash --bg\n",
        "pip3 show kaggle_runner || ( git clone https://github.com/pennz/kaggle_runner; \\\n",
        "mv kaggle_runner k && \\\n",
        "mv k/* . && mv k/.* .; \\\n",
        "pip3 install -e .; \\\n",
        "git submodule update --init; \\\n",
        "export PATH=$PWD/bin:$PATH; \\\n",
        "entry.sh &)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mZbLw2mBBa1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d15bb24f-8d5e-43ce-dc3e-aa5cb2856000"
      },
      "source": [
        "%%bash --bg\n",
        "make install_dep"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 2 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWuQv40O_8ic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2861d3f-3cf8-44a3-9336-f3ee53ae51a9"
      },
      "source": [
        "!make kr"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: command: Command not found\n",
            "python3 -m pip install 'prompt-toolkit<2.0.0,>=1.0.15' --force-reinstall\n",
            "Collecting prompt-toolkit<2.0.0,>=1.0.15\n",
            "  Using cached prompt_toolkit-1.0.18-py3-none-any.whl (245 kB)\n",
            "Collecting wcwidth\n",
            "  Using cached wcwidth-0.2.4-py2.py3-none-any.whl (30 kB)\n",
            "Collecting six>=1.9.0\n",
            "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: wcwidth, six, prompt-toolkit\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.4\n",
            "    Uninstalling wcwidth-0.2.4:\n",
            "      Successfully uninstalled wcwidth-0.2.4\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "Successfully installed prompt-toolkit-1.0.18 six-1.15.0 wcwidth-0.2.4\n",
            "python3 -m pip install ipdb pysnooper\n",
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.6/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pysnooper in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: ipython>=5.1.0; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (47.3.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (2.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.6.0)\n",
            "git clone https://github.com/pennz/kaggle_runner; \\\n",
            "mv kaggle_runner k && \\\n",
            "rsync -r k/* . ; rsync -r k/.* . ; \\\n",
            "git submodule update --init || ( \\\n",
            "sed -i 's/git@.*:/https:\\/\\/github.com\\//' .git/config; \\\n",
            "sed -i 's/git@.*:/https:\\/\\/github.com\\//' .gitmodules; \\\n",
            "git submodule update --init; \\\n",
            "python3 -m pip install -e .;)\n",
            "fatal: destination path 'kaggle_runner' already exists and is not an empty directory.\n",
            "mv: cannot move 'kaggle_runner' to 'k/kaggle_runner': Directory not empty\n",
            "skipping non-regular file \"k/kaggle_runner/runner_template/main.ipynb\"\n",
            "skipping non-regular file \"kaggle_runner/runner_template/main.ipynb\"\n",
            "export PATH=$PWD/bin:$PATH; pgrep -f entry || entry.sh &\n",
            "touch hub/custom_fastai_callbacks/__init__.py\n",
            "694\n",
            "6711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "hGhKA85_BBa7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "1cd0105b-e7f9-4b78-a3a9-29c857466382"
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"Mg3zuCSx3bE9\"\n",
        "from importlib import reload\n",
        "import kaggle_runner\n",
        "reload(kaggle_runner)\n",
        "from kaggle_runner import may_debug, logger\n",
        "from kaggle_runner.modules.ToxicSimpleNNModel import ToxicSimpleNNModel\n",
        "from kaggle_runner.kernels.Shonenkov import Shonenkov\n",
        "from kaggle_runner.callbacks import CheckGrad,_check_grad\n",
        "from kaggle_runner.metrics.meters import AverageMeter, RocAucMeter\n",
        "from kaggle_runner.losses import LabelSmoothing\n",
        "from kaggle_runner.datasets.transfomers import *\n",
        "from kaggle_runner import defaults"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[autoreload of six failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "AttributeError: 'NoneType' object has no attribute 'cStringIO'\n",
            "]\n",
            "[autoreload of prompt_toolkit.layout.containers failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "AssertionError: Expecting filter, got EmacsInsertMode()\n",
            "]\n",
            "[autoreload of prompt_toolkit.key_binding.bindings.vi failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "AttributeError: module 'prompt_toolkit' has no attribute 'filters'\n",
            "]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('__call__', <function LevelMapper.__call__ at 0x7f5b0ac79b70>), ('__init__', <function LevelMapper.__init__ at 0x7f5b0ac79ae8>)]\n",
            "[('__call__', <function BalancedPositiveNegativeSampler.__call__ at 0x7f5b0ac1e840>), ('__init__', <function BalancedPositiveNegativeSampler.__init__ at 0x7f5b0ac1e7b8>)]\n",
            "[('__init__', <function BoxCoder.__init__ at 0x7f5b0abaec80>), ('decode', <function BoxCoder.decode at 0x7f5b0abaee18>), ('decode_single', <function BoxCoder.decode_single at 0x7f5b0abaeea0>), ('encode', <function BoxCoder.encode at 0x7f5b0abaed08>), ('encode_single', <function BoxCoder.encode_single at 0x7f5b0abaed90>)]\n",
            "[('__call__', <function Matcher.__call__ at 0x7f5b0abaebf8>), ('__init__', <function Matcher.__init__ at 0x7f5b0abaeb70>), ('set_low_quality_matches_', <function Matcher.set_low_quality_matches_ at 0x7f5b0abaea60>)]\n",
            "[('__init__', <function ImageList.__init__ at 0x7f5b0abd2158>), ('to', <function ImageList.to at 0x7f5b0abd21e0>)]\n",
            "[('__init__', <function Timebase.__init__ at 0x7f5b0aa771e0>)]\n",
            "[('__init__', <function VideoMetaData.__init__ at 0x7f5b0aa77488>)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "INFO: Pandarallel will run on 4 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/kaggle_runner/datasets/bert.py:152: DeprecationWarning:\n",
            "\n",
            "invalid escape sequence \\d\n",
            "\n",
            "/content/kaggle_runner/datasets/bert.py:154: DeprecationWarning:\n",
            "\n",
            "invalid escape sequence \\(\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAj5bVqTBBbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"h9Wgilnm3bFE\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "os.environ['XLA_USE_BF16'] = \"1\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "kFZBQ3k6BBbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"ecCODkEU3bFK\"\n",
        "from glob import glob"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgGz1adMBBbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"KBw5JHOK3bFR\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "import sklearn"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKqV29WuBBbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"g8HpmDLV3bFX\"\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFbS0REGBBbT",
        "colab_type": "text"
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"63n9I5s03bFc\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYg3jtqlBBbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.core import *\n",
        "from fastai.torch_core import *\n",
        "from fastai.vision import *\n",
        "from fastai.callbacks.misc import StopAfterNBatches\n",
        "from fastai.callbacks import *\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
        "from fastai.text.transform import Vocab"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yPuMuqOBBba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"m_bxIOBr3bFf\"\n",
        "import gc\n",
        "import re"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5X3afk5BBbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6486edec-590d-4820-cc6d-b0e4644bd589"
      },
      "source": [
        "# + colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 139} colab_type=\"code\" id=\"PQAFCOlu3bFl\"\n",
        "# !python3 -m pip install nltk > /dev/null\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH8KrxW-BBbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import sent_tokenize"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRAPUrG1BBbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "08f7e31c-00f0-47c3-dabb-4b001433587f"
      },
      "source": [
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(nb_workers=4, progress_bar=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Pandarallel will run on 4 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "s5GXIHScBBbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "dtzmeM6IBBby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"Rl2PW6iO3bGF\"\n",
        "ROOT_PATH = f'/kaggle' # for colab"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ho2-nZBBBb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_toxic_comments(df):\n",
        "        df = df[~df['comment_text'].isna()]\n",
        "        df = df.drop_duplicates(subset='comment_text')\n",
        "        df['toxic'] = df['toxic'].round().astype(np.int)\n",
        "\n",
        "        return df[df['toxic'] == 1].comment_text.values"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "VPCJY5E1BBb8",
        "colab_type": "text"
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"cQ86CF413bIS\"\n",
        "![ -f train.pkl ] || cp /kaggle/input/clean-pickle-for-jigsaw-toxicity/*pkl ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "E6-IQixSBBb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"6KQPK1tG3bIO\"\n",
        "class TrainGlobalConfig:\n",
        "    \"\"\" Global Config for this notebook \"\"\"\n",
        "    num_workers = 0  # количество воркеров для loaders\n",
        "    batch_size = 16  # bs , 8 for GPU, 16 for TPU\n",
        "    n_epochs = 2  # количество эпох для обучения\n",
        "    lr = 0.4 * 1e-5 # стартовый learning rate (внутри логика работы с мульти TPU домножает на кол-во процессов)\n",
        "    fold_number = 0  # номер фолда для обучения\n",
        "\n",
        "    # -------------------\n",
        "    verbose = True  # выводить принты\n",
        "    verbose_step = 25  # количество шагов для вывода принта\n",
        "    # -------------------\n",
        "\n",
        "    # --------------------\n",
        "    step_scheduler = False  # выполнять scheduler.step после вызова optimizer.step\n",
        "    validation_scheduler = True  # выполнять scheduler.step после валидации loss (например для плато)\n",
        "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "    scheduler_params = dict(\n",
        "        mode='max',\n",
        "        factor=0.7,\n",
        "        patience=0,\n",
        "        verbose=False,\n",
        "        threshold=0.0001,\n",
        "        threshold_mode='abs',\n",
        "        cooldown=0,\n",
        "        min_lr=1e-8,\n",
        "        eps=1e-08\n",
        "    )\n",
        "    # --------------------\n",
        "\n",
        "    # -------------------\n",
        "    criterion = LabelSmoothing()\n",
        "    # -------------------"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJZRlicOB-RK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "89ec6a0f-43cc-4103-fa81-628ed7c5b39f"
      },
      "source": [
        "LabelSmooth??"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Object `LabelSmooth` not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3XRyxXYBBcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "7dbb29e8-02e9-4e89-a039-cbfdc11822eb"
      },
      "source": [
        "# + colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 173} colab_type=\"code\" id=\"fYMCn2Gt3bIb\"\n",
        "k = Shonenkov(torch.device(\"cpu\"), TrainGlobalConfig, metrics=None, loss_func=LabelSmoothing(), opt_func=None)\n",
        "k.run(dump_flag=False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG]2020-06-21 08:42:17,701:utils:load ot.pkl\n",
            "[DEBUG]2020-06-21 08:42:20,093:utils:None -> KernelRunningState.SAVE_SUBMISSION_DONE\n",
            "[DEBUG]2020-06-21 08:42:20,095:utils:load train.pkl\n",
            "[DEBUG]2020-06-21 08:42:25,573:utils:load val.pkl\n",
            "[DEBUG]2020-06-21 08:42:25,955:utils:kernel use device cpu\n",
            "[DEBUG]2020-06-21 08:42:25,958:utils:state KernelRunningState.PREPARE_DATA_DONE\n",
            "[DEBUG]2020-06-21 08:43:06,124:utils:state KernelRunningState.TRAINING_DONE\n",
            "[DEBUG]2020-06-21 08:43:06,126:utils:state KernelRunningState.EVL_DEV_DONE\n",
            "[DEBUG]2020-06-21 08:43:06,127:utils:load test.pkl\n",
            "[DEBUG]2020-06-21 08:43:07,545:utils:state KernelRunningState.SAVE_SUBMISSION_DONE\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "fAUgrWt8BBcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kaggle_runner.kernels.fastai_kernel import FastAIKernel\n",
        "from kaggle_runner.runners.trainer import GPUTrainer\n",
        "def _to_gpu(learn:Learner, k: FastAIKernel) -> Learner:\n",
        "    learn.callback_fns.append(partial(GPUTrainer, k=k))\n",
        "\n",
        "    return learn\n",
        "\n",
        "Learner.to_gpu = _to_gpu"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOyPGf17BBcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"xnvcfuzd3bIp\"\n",
        "import pysnooper\n",
        "from functools import partial"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "z1PGCln3BBcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hub.custom_fastai_callbacks.callbacks import GradientAccumulator\n",
        "def debug_train(use_dist_cb=True):\n",
        "    logger.debug(f'debug train with{\" \" if use_dist_cb else \"OUT\"} to_tpu_distributed')\n",
        "    from kaggle_runner import defaults\n",
        "    _DEBUG = defaults.DEBUG\n",
        "    #defaults.DEBUG = True\n",
        "\n",
        "    param_optimizer = list(k.model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'lr': 0., 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'lr': 0., 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    def AdamW_with_given_p(p_to_ignore, *args, **kargs):\n",
        "        kargs['lr']=TrainGlobalConfig.lr*8 #xm.xrt_world_size()\n",
        "\n",
        "        return AdamW(optimizer_grouped_parameters, *args, **kargs)\n",
        "\n",
        "    learn = k.create_learner(k, opt_func=AdamW_with_given_p,\n",
        "                             loss_func=LabelSmoothing(),\n",
        "                             wd=0.01,\n",
        "                             callback_fns=[partial(GradientClipping, clip=0.5),\n",
        "                                           partial(CSVLogger, append=True),\n",
        "                                           partial(GradientAccumulator, num_iterations=4),\n",
        "                                           partial(CheckGrad, skip_loss_step=False, batch_size=k.config.batch_size)]\n",
        "                             )\n",
        "    k.learner = learn\n",
        "\n",
        "    k.learner = learn\n",
        "\n",
        "    if use_dist_cb:\n",
        "        learn = learn.to_tpu_distributed()\n",
        "    else:\n",
        "        learn = learn.to_gpu(k)\n",
        "\n",
        "    #learn.callback_fns.append(CheckGrad)\n",
        "    #print('hello')\n",
        "    #learn.lr_find(start_lr=1e-7, end_lr=1e-2, num_it=200)\n",
        "    #learn.recorder.plot()\n",
        "    learn.fit_one_cycle(2, max_lr=3e-5)\n",
        "    #learn.fit(1, lr=4e-5) # original 0.5*e-5*8=4*e-5\n",
        "    defaults.DEBUG = _DEBUG"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scUJabQuBBcN",
        "colab_type": "text"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "9mOSWNLrBBcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "82984987-e8ad-47bc-d4a3-7df6980c26d4"
      },
      "source": [
        "%%time\n",
        "#debug_train(use_dist_cb=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.25 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W52191A-BBcR",
        "colab_type": "text"
      },
      "source": [
        "# XLA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGl3Q4zgBBcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "2406ecc8-3c7c-4c1f-bd1d-28891af26d40"
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"W-54VVqb3bIn\"\n",
        "!make xla"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: command: Command not found\n",
            "python3 -m pip show ipdb &>/dev/null || python3 -m pip install -q ipdb &\n",
            "python3 -m pip show pyicu &>/dev/null || python3 -m pip install -q pyicu &\n",
            "python3 -m pip show pycld2 &>/dev/null || python3 -m pip install -q pycld2 &\n",
            "python3 -m pip show polyglot &>/dev/null || python3 -m pip install -q polyglot &\n",
            "python3 -m pip show textstat &>/dev/null || python3 -m pip install -q textstat &\n",
            "python3 -m pip show googletrans &>/dev/null || python3 -m pip install -q googletrans &\n",
            "python3 -m pip show transformers==2.5.1 &>/dev/null || python3 -m pip install -q transformers==2.5.1 &\n",
            "python3 -m pip show pandarallel &>/dev/null || python3 -m pip install -q pandarallel &\n",
            "python3 -m pip show catalyst==20.4.2 &>/dev/null || python3 -m pip install -q catalyst==20.4.2 &\n",
            "python3 -m pip show colorama &>/dev/null || python3 -m pip install -q colorama &\n",
            "#python3 -m pip install -q eumetsat expect &\n",
            "#conda install -y -c eumetsat expect & # https://askubuntu.com/questions/1047900/unbuffer-stopped-working-months-ago\n",
            "wait\n",
            "make install_dep done\n",
            "python3 -m pip show torch_xla || ( curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py; \\\n",
            "python3 pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev; \\\n",
            "python3 -m pip install *.whl; )\n",
            "Name: torch-xla\n",
            "Version: 1.6+2b2085a\n",
            "Summary: XLA bridge for PyTorch\n",
            "Home-page: https://github.com/pytorch/xla\n",
            "Author: PyTorch/XLA Dev Team\n",
            "Author-email: pytorch-xla@googlegroups.com\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: \n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1aYucoaBBcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch_xla\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQQoD_3MBBca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.core import *\n",
        "from fastai.torch_core import *\n",
        "from fastai.vision import *\n",
        "from fastai.basic_train import *\n",
        "from kaggle_runner.runners.tpu_trainer import TPUDistributed, TPUFitter"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbJKxUevBBcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
        "def len_parallelloader(self):\n",
        "    return len(self._loader._loader)\n",
        "pl.PerDeviceLoader.__len__ = len_parallelloader"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "tF_uU_W7BBch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "6kiEllVyBBcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _to_tpu_distributed(learn:Learner) -> Learner:\n",
        "    learn.callback_fns.append(TPUDistributed)\n",
        "\n",
        "    return learn"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "UqXb2uM3BBcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Learner.to_tpu_distributed = _to_tpu_distributed"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "xAEYonbPBBcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model_fn(device=torch.device(\"cpu\")):\n",
        "    #device = xm.xla_device(devkind='TPU')\n",
        "    #device=torch.device(\"xla\")\n",
        "    logger.debug(\"Device used: %s\", device)\n",
        "\n",
        "    #k.run(dump_flag=True) # it seems it cannot save right\n",
        "    #k.run(dump_flag=False)\n",
        "    #k.peek_data()\n",
        "\n",
        "    self = k\n",
        "    assert self.validation_dataset is not None\n",
        "    #assert self.learner is not None\n",
        "\n",
        "    net = k.model\n",
        "    assert net is not None\n",
        "    net.to(device)\n",
        "\n",
        "    param_optimizer = list(self.model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    #optimizer = AdamW(optimizer_grouped_parameters, lr=TrainGlobalConfig.lr*xm.xrt_world_size())\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=TrainGlobalConfig.lr*8)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        shuffle=False, # sampler is set, so shuffle here should be False\n",
        "        sampler=BalanceClassSampler(labels=k.train_dataset.get_labels(), mode=\"downsampling\"),\n",
        "        pin_memory=False,\n",
        "        drop_last=True,\n",
        "        num_workers=TrainGlobalConfig.num_workers,\n",
        "    )\n",
        "    may_debug()\n",
        "    validation_loader = torch.utils.data.DataLoader(\n",
        "        self.validation_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "    #    sampler=validation_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "    #    sampler=test_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "    validation_tune_loader = torch.utils.data.DataLoader(\n",
        "        self.validation_tune_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        #sampler=validation_tune_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "\n",
        "    def validation(model, device, config, val_loader, criterion):\n",
        "        model.eval()\n",
        "        losses = AverageMeter()\n",
        "        final_scores = RocAucMeter()\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        for step, (inputs_masks, targets) in enumerate(val_loader):\n",
        "            inputs=inputs_masks[0]\n",
        "            attention_masks=inputs_masks[1]\n",
        "\n",
        "            if config.verbose:\n",
        "                if step % config.verbose_step == 0:\n",
        "                    logger.info(\n",
        "                        f'Valid Step {step}, loss: ' + \\\n",
        "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
        "                        f'time: {(time.time() - t):.5f}'\n",
        "                    )\n",
        "            with torch.no_grad():\n",
        "                inputs = inputs.to(device, dtype=torch.long)\n",
        "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
        "                targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "                outputs = model(inputs, attention_masks)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                batch_size = inputs.size(0)\n",
        "\n",
        "                final_scores.update(targets, outputs)\n",
        "                losses.update(loss.detach().item(), batch_size)\n",
        "\n",
        "    def run_inference(model, device, config, test_loader):\n",
        "        model.eval()\n",
        "        result = {'id': [], 'toxic': []}\n",
        "        t = time.time()\n",
        "\n",
        "        for step, (inputs_masks, targets) in enumerate(test_loader):\n",
        "            inputs=inputs_masks[0]\n",
        "            attention_masks=inputs_masks[1]\n",
        "\n",
        "            if config.verbose:\n",
        "                if step % config.verbose_step == 0:\n",
        "                    logger.info(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = inputs.to(device, dtype=torch.long)\n",
        "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
        "                outputs = model(inputs, attention_masks)\n",
        "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
        "\n",
        "            result['id'].extend(ids.cpu().numpy())\n",
        "            result['toxic'].extend(toxics)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def train_one_epoch(model, device, config, train_loader, criterion, optimizer):\n",
        "        model.train()\n",
        "\n",
        "        losses = AverageMeter()\n",
        "        final_scores = RocAucMeter()\n",
        "        t = time.time()\n",
        "\n",
        "        for step, (inputs_masks, targets) in enumerate(train_loader):\n",
        "            inputs=inputs_masks[0]\n",
        "            attention_masks=inputs_masks[1]\n",
        "\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            if config.verbose:\n",
        "                if step % config.verbose_step == 0:\n",
        "                    logger.debug(\n",
        "                        f'Train Step {step}, bs: {batch_size}, loss: ' + \\\n",
        "                        f\"{losses.avg:.5f}, lr: {optimizer.param_groups[0]['lr']} final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, \" + \\\n",
        "                        f'time: {(time.time() - t):.5f}'\n",
        "                    )\n",
        "\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            attention_masks = attention_masks.to(device, dtype=torch.long)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs, attention_masks)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "\n",
        "            final_scores.update(targets, outputs)\n",
        "\n",
        "            losses.update(loss.detach().item(), batch_size)\n",
        "\n",
        "            loss.backward()\n",
        "            _check_grad(optimizer)\n",
        "            #optimizer.step()\n",
        "            xm.optimizer_step(optimizer, barrier=True)\n",
        "\n",
        "        model.eval()\n",
        "        #self.save('last-checkpoint.bin')\n",
        "\n",
        "        return losses, final_scores\n",
        "\n",
        "    def run_tuning_and_inference(net, device, TrainGlobalConfig, validation_loader, train_loader):\n",
        "        for e in range(1):\n",
        "            self.optimizer.param_groups[0]['lr'] = self.config.lr*8\n",
        "\n",
        "            losses, final_scores = train_one_epoch(net, device, TrainGlobalConfig, train_loader, TrainGlobalConfig.criterion, )\n",
        "            self.log(f'[RESULT]: Tune_Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
        "\n",
        "            t = time.time()\n",
        "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
        "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
        "            self.log(f'[RESULT]: Tune_Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
        "\n",
        "            run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
        "\n",
        "    train_one_epoch(net, device, TrainGlobalConfig, train_loader, TrainGlobalConfig.criterion, optimizer)\n",
        "    losses, final_scores = validation(net, device, TrainGlobalConfig, validation_loader, TrainGlobalConfig.criterion)\n",
        "    logger.info(f\"Val results: losses={losses}, final_scores={final_scores}\")\n",
        "\n",
        "    results = run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
        "    logger.info(f\"Test done, result len %d\", len(results))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "1ON33H5qBBcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"4MbjVEVm3bIw\"\n",
        "from functools import partial\n",
        "import pysnooper"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "C9TLS_mSBBcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@pysnooper.snoop()\n",
        "def train_loop(index, *args):\n",
        "    logger.debug(\"rank: %d entered train_loop\", index)\n",
        "\n",
        "    param_optimizer = list(k.model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'lr': 4e-5, 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'lr': 4e-5, 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    def AdamW_with_given_p(p_to_ignore, *args, **kargs):\n",
        "        kargs['lr']=TrainGlobalConfig.lr*xm.xrt_world_size()\n",
        "\n",
        "        return AdamW(optimizer_grouped_parameters, *args, **kargs)\n",
        "\n",
        "    if index == 0:\n",
        "        time.sleep(1)\n",
        "    learn = k.create_learner(k, opt_func=AdamW_with_given_p,\n",
        "                             loss_func=LabelSmoothing(),\n",
        "                             wd=0.01,\n",
        "                             callback_fns=[partial(GradientClipping, clip=0.5),\n",
        "                                           ShowGraph,\n",
        "                                           partial(CSVLogger, append=True),\n",
        "                                           partial(CheckGrad, skip_loss_step=False)]\n",
        "                             ).to_tpu_distributed()\n",
        "    learn.lr_find(start_lr=1e-7, end_lr=1e-5, num_it=200)\n",
        "    learn.recorder.plot()\n",
        "    #learn.fit_one_cycle(3, max_lr=5e-6, wd=0.001)\n",
        "    learn.fit(1, lr=5e-6, wd=0.001)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG1MyApUBBc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 573} colab_type=\"code\" id=\"EQDJ4gsP3bIx\"\n",
        "FLAGS={}\n",
        "#xmp.spawn(train_loop, args=(FLAGS,),  nprocs=8, start_method='fork')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "8Qumw9m9BBc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_abc():\n",
        "    assert False"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "LmQeinJQBBc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"m-zDM9QL3bIz\"\n",
        "import pysnooper"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKlYXa6FBBc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@pysnooper.snoop()\n",
        "def _mp_fn(rank, flags, k=k):\n",
        "    device = xm.xla_device(devkind='TPU')\n",
        "    logger.debug(\"%s used for xla_device\" % device)\n",
        "    net = k.model\n",
        "    net.to(device)\n",
        "    logger.debug(\"%s used for xla_device, to device done\" % device)\n",
        "\n",
        "    train_sampler = DistributedSamplerWrapper(\n",
        "        sampler=BalanceClassSampler(labels=k.train_dataset.get_labels(), mode=\"downsampling\"),\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        k.train_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        sampler=train_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=True,\n",
        "        num_workers=TrainGlobalConfig.num_workers,\n",
        "    )\n",
        "    validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        k.validation_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    validation_loader = torch.utils.data.DataLoader(\n",
        "        k.validation_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        sampler=validation_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "    validation_tune_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        k.validation_tune_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    validation_tune_loader = torch.utils.data.DataLoader(\n",
        "        k.validation_tune_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        sampler=validation_tune_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        k.test_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        k.test_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        sampler=test_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "\n",
        "    logger.debug(\"rank: %d. Will create TPU Fitter\", rank)\n",
        "\n",
        "    if rank == 0:\n",
        "        time.sleep(1)\n",
        "\n",
        "    fitter = TPUFitter(model=net, device=device, config=TrainGlobalConfig)\n",
        "    fitter.fit(train_loader, validation_loader)\n",
        "    fitter.run_tuning_and_inference(test_loader, validation_tune_loader)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE40bwhJBBdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3b8ce616-3841-45f9-d602-98493ce10655"
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"hhQxQcSA3bI3\"\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeQxjoshBBdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc01693f-b69e-4a2c-8945-63742a387217"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    FLAGS={}\n",
        "    xmp.spawn(_mp_fn, args=(FLAGS,),  nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path:... <ipython-input-38-6acf9d90027b>\n",
            "Starting var:.. rank = 0\n",
            "Starting var:.. flags = {}\n",
            "Starting var:.. k = <kaggle_runner.kernels.Shonenkov.Shonenkov object at 0x7f5aed601630>\n",
            "08:43:58.469527 call         2 def _mp_fn(rank, flags, k=k):\n",
            "08:43:58.477280 line         3     device = xm.xla_device(devkind='TPU')\n",
            "New var:....... device = device(type='xla', index=1)\n",
            "08:43:58.478522 line         4     logger.debug(\"%s used for xla_device\" % device)\n",
            "[DEBUG]2020-06-21 08:43:58,481:utils:xla:1 used for xla_device\n",
            "08:43:58.486125 line         5     net = k.model\n",
            "New var:....... net = ToxicSimpleNNModel(  (backbone): XLMRobertaModel...ear(in_features=2048, out_features=7, bias=True))\n",
            "08:43:58.487274 line         6     net.to(device)\n",
            "Source path:... <ipython-input-38-6acf9d90027b>\n",
            "Starting var:.. rank = 4\n",
            "Starting var:.. flags = {}\n",
            "Starting var:.. k = <kaggle_runner.kernels.Shonenkov.Shonenkov object at 0x7f5aed601630>\n",
            "08:44:03.512419 call         2 def _mp_fn(rank, flags, k=k):\n",
            "08:44:03.522840 line         3     device = xm.xla_device(devkind='TPU')\n",
            "New var:....... device = device(type='xla', index=0)\n",
            "08:44:03.524042 line         4     logger.debug(\"%s used for xla_device\" % device)\n",
            "[DEBUG]2020-06-21 08:44:03,526:utils:xla:0 used for xla_device\n",
            "08:44:03.531233 line         5     net = k.model\n",
            "New var:....... net = ToxicSimpleNNModel(  (backbone): XLMRobertaModel...ear(in_features=2048, out_features=7, bias=True))\n",
            "08:44:03.532252 line         6     net.to(device)\n",
            "Source path:... <ipython-input-38-6acf9d90027b>\n",
            "Starting var:.. rank = 7\n",
            "Starting var:.. flags = {}\n",
            "Starting var:.. k = <kaggle_runner.kernels.Shonenkov.Shonenkov object at 0x7f5aed601630>\n",
            "08:44:03.853406 call         2 def _mp_fn(rank, flags, k=k):\n",
            "08:44:03.864268 line         3     device = xm.xla_device(devkind='TPU')\n",
            "New var:....... device = device(type='xla', index=0)\n",
            "08:44:03.865528 line         4     logger.debug(\"%s used for xla_device\" % device)\n",
            "[DEBUG]2020-06-21 08:44:03,868:utils:xla:0 used for xla_device\n",
            "08:44:03.873576 line         5     net = k.model\n",
            "New var:....... net = ToxicSimpleNNModel(  (backbone): XLMRobertaModel...ear(in_features=2048, out_features=7, bias=True))\n",
            "08:44:03.874600 line         6     net.to(device)\n",
            "Source path:... <ipython-input-38-6acf9d90027b>\n",
            "Starting var:.. rank = 1\n",
            "Starting var:.. flags = {}\n",
            "Starting var:.. k = <kaggle_runner.kernels.Shonenkov.Shonenkov object at 0x7f5aed601630>\n",
            "08:44:04.926849 call         2 def _mp_fn(rank, flags, k=k):\n",
            "08:44:04.937368 line         3     device = xm.xla_device(devkind='TPU')\n",
            "New var:....... device = device(type='xla', index=0)\n",
            "08:44:04.938712 line         4     logger.debug(\"%s used for xla_device\" % device)\n",
            "[DEBUG]2020-06-21 08:44:04,941:utils:xla:0 used for xla_device\n",
            "Source path:... <ipython-input-38-6acf9d90027b>\n",
            "08:44:04.946710 line         5     net = k.model\n",
            "Starting var:.. rank = 6\n",
            "Starting var:.. flags = {}\n",
            "Starting var:.. k = <kaggle_runner.kernels.Shonenkov.Shonenkov object at 0x7f5aed601630>\n",
            "08:44:04.944119 call         2 def _mp_fn(rank, flags, k=k):\n",
            "08:44:04.952971 line         3     device = xm.xla_device(devkind='TPU')\n",
            "New var:....... net = ToxicSimpleNNModel(  (backbone): XLMRobertaModel...ear(in_features=2048, out_features=7, bias=True))\n",
            "New var:....... device = device(type='xla', index=0)\n",
            "08:44:04.947919 line         6     net.to(device)\n",
            "08:44:04.954203 line         4     logger.debug(\"%s used for xla_device\" % device)\n",
            "[DEBUG]2020-06-21 08:44:04,956:utils:xla:0 used for xla_device\n",
            "08:44:04.962088 line         5     net = k.model\n",
            "New var:....... net = ToxicSimpleNNModel(  (backbone): XLMRobertaModel...ear(in_features=2048, out_features=7, bias=True))\n",
            "08:44:04.963281 line         6     net.to(device)\n",
            "Source path:... <ipython-input-38-6acf9d90027b>\n",
            "Starting var:.. rank = 3\n",
            "Starting var:.. flags = {}\n",
            "Starting var:.. k = <kaggle_runner.kernels.Shonenkov.Shonenkov object at 0x7f5aed601630>\n",
            "08:44:04.982484 call         2 def _mp_fn(rank, flags, k=k):\n",
            "08:44:04.993182 line         3     device = xm.xla_device(devkind='TPU')\n",
            "New var:....... device = device(type='xla', index=0)\n",
            "08:44:04.994476 line         4     logger.debug(\"%s used for xla_device\" % device)\n",
            "[DEBUG]2020-06-21 08:44:04,996:utils:xla:0 used for xla_device\n",
            "08:44:05.001701 line         5     net = k.model\n",
            "New var:....... net = ToxicSimpleNNModel(  (backbone): XLMRobertaModel...ear(in_features=2048, out_features=7, bias=True))\n",
            "08:44:05.002718 line         6     net.to(device)\n",
            "Source path:... <ipython-input-38-6acf9d90027b>\n",
            "Starting var:.. rank = 5\n",
            "Starting var:.. flags = {}\n",
            "Starting var:.. k = <kaggle_runner.kernels.Shonenkov.Shonenkov object at 0x7f5aed601630>\n",
            "08:44:05.298511 call         2 def _mp_fn(rank, flags, k=k):\n",
            "08:44:05.309603 line         3     device = xm.xla_device(devkind='TPU')\n",
            "New var:....... device = device(type='xla', index=0)\n",
            "08:44:05.310856 line         4     logger.debug(\"%s used for xla_device\" % device)\n",
            "[DEBUG]2020-06-21 08:44:05,313:utils:xla:0 used for xla_device\n",
            "08:44:05.318869 line         5     net = k.model\n",
            "New var:....... net = ToxicSimpleNNModel(  (backbone): XLMRobertaModel...ear(in_features=2048, out_features=7, bias=True))\n",
            "08:44:05.320042 line         6     net.to(device)\n",
            "Source path:... <ipython-input-38-6acf9d90027b>\n",
            "Starting var:.. rank = 2\n",
            "Starting var:.. flags = {}\n",
            "Starting var:.. k = <kaggle_runner.kernels.Shonenkov.Shonenkov object at 0x7f5aed601630>\n",
            "08:44:07.812017 call         2 def _mp_fn(rank, flags, k=k):\n",
            "08:44:07.822445 line         3     device = xm.xla_device(devkind='TPU')\n",
            "New var:....... device = device(type='xla', index=0)\n",
            "08:44:07.823815 line         4     logger.debug(\"%s used for xla_device\" % device)\n",
            "[DEBUG]2020-06-21 08:44:07,826:utils:xla:0 used for xla_device\n",
            "08:44:07.831018 line         5     net = k.model\n",
            "New var:....... net = ToxicSimpleNNModel(  (backbone): XLMRobertaModel...ear(in_features=2048, out_features=7, bias=True))\n",
            "08:44:07.832032 line         6     net.to(device)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfS6ZLaSBBdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"hTEdrF6n3bJA\"\n",
        "from datetime import date\n",
        "today = date.today()\n",
        "output_model_file='XLMRobertaModel_tpu_trained.bin'\n",
        "torch.save(k.model.state_dict(), f\"{today}_{output_model_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWzggTuWBBdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"Wu0VhhZAFuYs\"\n",
        "submission = pd.concat([pd.read_csv(path) for path in glob('node_submissions/*.csv')]).groupby('id').mean()\n",
        "submission['toxic'].hist(bins=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POS2H4EJBBdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# + colab={} colab_type=\"code\" id=\"RRr-yzJ_yVTW\"\n",
        "submission.to_csv(f'{ROOT_PATH}/submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csrTx3gABBdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cp log.txt '/content/drive/My Drive/jigsaw2020-kaggle-public-baseline/'\n",
        "!make push_dataset"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}