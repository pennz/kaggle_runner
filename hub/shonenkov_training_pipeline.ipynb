{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzoWM76m0Rfg"
   },
   "source": [
    "### Dont forget turn on TPU & HIGH-RAM modes :)\n",
    "\n",
    "Author: [Alex Shonenkov](https://www.kaggle.com/shonenkov) //  shonenkov@phystech.edu\n",
    "Have a good day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_43yMxyEvW-q"
   },
   "outputs": [],
   "source": [
    "!echo $HOSTNAME\n",
    "!echo $TPU_NAME\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6uGvKL3upio"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6uGvKL3epio"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run('[ -f setup.py ] || (git clone https://github.com/pennz/kaggle_runner; '\n",
    "'git submodule update --init --recursive; '\n",
    "'rsync -r kaggle_runner/.* .; '\n",
    "'rsync -r kaggle_runner/* .;); '\n",
    "'python3 -m pip install -e .', shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "x5uJSXQmfnNb",
    "lines_to_next_cell": 2,
    "outputId": "2cd4fe6f-9500-4d07-ba92-b093230587f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/kaggle_runner/utils/kernel_utils.py:155: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\g\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from kaggle_runner.utils.kernel_utils import get_obj_or_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wV017Cj1CRlg"
   },
   "outputs": [],
   "source": [
    "with open(\"runner.sh\", \"w\") as f:\n",
    "    f.write(\n",
    "r\"\"\"#!/bin/bash -x\n",
    "export PS4='Line ${LINENO}: ' # for debug\n",
    "NC=ncat\n",
    "\n",
    "USER=$1\n",
    "shift\n",
    "REPO=$1\n",
    "shift\n",
    "BRANCH=$1\n",
    "shift\n",
    "PHASE=$1\n",
    "shift\n",
    "ENABLE_RVS=$1\n",
    "shift\n",
    "\n",
    "SERVER=$1\n",
    "shift\n",
    "PORT=$1\n",
    "shift\n",
    "\n",
    "ORIG_PORT=23454\n",
    "\n",
    "CHECK_PORT=$((ORIG_PORT + 1))\n",
    "python3 -m pip install --upgrade pip\n",
    "conda install -y -c eumetsat expect & # https://askubuntu.com/questions/1047900/unbuffer-stopped-working-months-ago\n",
    "apt update && apt install -y netcat nmap screen time locales >/dev/null 2>&1\n",
    "apt install -y mosh iproute2 fish tig ctags htop tree pv tmux psmisc >/dev/null 2>&1 &\n",
    "\n",
    "conda init bash\n",
    "cat >> ~/.bashrc << EOF\n",
    "conda activate base # as my dotfiles will fiddle with conda\n",
    "export SERVER=$SERVER\n",
    "export CHECK_PORT=$CHECK_PORT\n",
    "EOF\n",
    "\n",
    "source rpt # rvs IDE env setup\n",
    "export SERVER=$SERVER\n",
    "export CHECK_PORT=$CHECK_PORT\n",
    "\n",
    "wait_ncat() {\n",
    "    wait_for_ncat=$1\n",
    "\n",
    "    while [ $wait_for_ncat -gt 0 ]; do\n",
    "        wait_for_ncat=$((wait_for_ncat - 1))\n",
    "        which ncat >/dev/null && return 0\n",
    "    done\n",
    "}\n",
    "wait_ncat 60\n",
    "\n",
    "which $NC >/dev/null || NC=nc\n",
    "export NC\n",
    "\n",
    "if [ \"x${ENABLE_RVS}\" = x1 ]; then\n",
    "    if [ -z $(pgrep -f 'jupyter-notebook') ]; then\n",
    "        bash ./rvs.sh $SERVER $PORT 2>&1 &\n",
    "    else\n",
    "        screen -d -m bash -c \"{ echo [REMOTE]: rvs log below.; bash -x ./rvs.sh $SERVER $PORT 2>&1; } | $NC --send-only --no-shutdown -w 120s -i $((3600 * 2))s $SERVER $CHECK_PORT\"\n",
    "    fi\n",
    "fi &\n",
    "\n",
    "python3 -m pip install ripdb pydicom parse pytest-logger python_logging_rabbitmq coverage &\n",
    "python3 -m pip install pyvim neovim msgpack==1.0.0 & # for vim\n",
    "\n",
    "# SRC_WORK_FOLDER=/kaggle/working # it is just current working folder\n",
    "# [ -d ${SRC_WORK_FOLDER} ] || mkdir -p ${SRC_WORK_FOLDER}\n",
    "#\n",
    "# cd ${SRC_WORK_FOLDER}\n",
    "\n",
    "if [ -d ${REPO} ]; then rm -rf ${REPO}; fi\n",
    "\n",
    "# get code\n",
    "{\n",
    "    mvdir() {\n",
    "        [[ \"$2\"/\"$1\" -ef \"${PWD}\" ]] || {\n",
    "            rm -rf \"$2\"/\"$1\" &&\n",
    "                mkdir \"$2\"/\"$1\"\n",
    "        }\n",
    "\n",
    "        bash -c \"mv \"\"$1\"\"/*\"\" $2\"\"/\"\"$1\"\n",
    "    }\n",
    "    export -f mvdir\n",
    "\n",
    "    if [ ! -d ${REPO} ]; then\n",
    "        git clone --single-branch --branch ${BRANCH} --depth=1 \\\n",
    "            https://github.com/${USER}/${REPO}.git ${REPO} && pushd ${REPO} &&\n",
    "        sed -i 's/git@\\(.*\\):\\(.*\\)/https:\\/\\/\\1\\/\\2/' .gitmodules &&\n",
    "        sed -i 's/git@\\(.*\\):\\(.*\\)/https:\\/\\/\\1\\/\\2/' .git/config &&\n",
    "        git submodule update --init --recursive\n",
    "        find . -maxdepth 1 -name \".??*\" -o -name \"??*\" -type f | xargs -I{} mv {} $OLDPWD\n",
    "        find . -maxdepth 1 -name \".??*\" -o -name \"??*\" -type d | xargs -I{} bash -x -c \"mvdir {}  $OLDPWD\"\n",
    "        popd\n",
    "    fi\n",
    "    make install_dep >/dev/null\n",
    "}\n",
    "\n",
    "USE_AMQP=true\n",
    "export USE_AMQP\n",
    "\n",
    "conda init bash\n",
    "source ~/.bashrc\n",
    "conda activate base\n",
    "\n",
    "if [ x\"${PHASE}\" = x\"dev\" ]; then\n",
    "    export PS4='[Remote]: Line ${LINENO}: '\n",
    "    (\n",
    "        echo \"MOSHing\"\n",
    "        make mosh\n",
    "    ) &\n",
    "\n",
    "    make toxic | if [ $USE_AMQP -eq true ]; then cat -; else $NC --send-only -w 120s -i $((60 * 5))s $SERVER $CHECK_PORT; fi &\n",
    "    wait # not exit, when dev\n",
    "fi\n",
    "\n",
    "if [ x\"${PHASE}\" = x\"data\" ]; then\n",
    "    bash ./rvs.sh $SERVER $PORT >/dev/null & # just keep one rvs incase\n",
    "    make dataset\n",
    "fi\n",
    "\n",
    "if [ x\"${PHASE}\" = x\"test\" ]; then\n",
    "    bash ./rvs.sh $SERVER $PORT >/dev/null & # just keep one rvs incase\n",
    "    #make test\n",
    "fi\n",
    "\n",
    "if [ x\"${PHASE}\" = x\"run\" ]; then\n",
    "    bash ./rvs.sh $SERVER $PORT >/dev/null & make m & # just keep one rvs incase\n",
    "    make toxic | if [ $USE_AMQP -eq true ]; then cat -; else $NC --send-only -w 120s -i $((60 * 5))s $SERVER $CHECK_PORT; fi\n",
    "    # basically the reverse of the calling path\n",
    "    pkill make & pkill -f \"mosh\" & pkill sleep & pkill -f \"rvs.sh\" & pkill ncat &\n",
    "    # python main.py \"$@\"\n",
    "fi\n",
    "\"\"\"\n",
    "    )\n",
    "with open(\"rvs.sh\", \"w\") as f:\n",
    "    f.write(\n",
    "r\"\"\"#!/bin/bash -x\n",
    "export PS4='Line ${LINENO}: ' # for debug\n",
    "\n",
    "NC=${NC:-ncat}\n",
    "type $NC || ( echo >&2 \"$NC cannot be found. Exit.\"; exit 1;)\n",
    "# https://stackoverflow.com/questions/57877451/retrieving-output-and-exit-code-of-a-coprocess\n",
    "# coproc { sleep 30 && echo \"Output\" && exit 3; }\n",
    "# Saving the coprocess's PID for later, as COPROC_PID apparently unsets when its finished\n",
    "# COPROC_PID_backup=$COPROC_PID\n",
    "#\n",
    "# Retrieving the coprocess's output\n",
    "# output=$(cat <&$COPROC)\n",
    "#\n",
    "# Retrieving the coprocess's exit code\n",
    "# wait $COPROC_PID_backup\n",
    "#\n",
    "# Echoing out the results\n",
    "# echo $?\n",
    "# echo $output\n",
    "\n",
    "echo BASH NOW: $BASHPID\n",
    "\n",
    "PID_FILE_PATH=/tmp/nc.pid\n",
    "EXIT_FILE_PATH=/tmp/rvs_exit.$BASHPID.pid\n",
    "\n",
    "test -f $EXIT_FILE_PATH && rm $EXIT_FILE_PATH\n",
    "\n",
    "SERVER=$1\n",
    "shift\n",
    "PORT=$1\n",
    "shift\n",
    "\n",
    "ORIG_PORT=23454\n",
    "CHECK_PORT=$((ORIG_PORT + 1))\n",
    "\n",
    "check_exit_status() {\n",
    "  [ -f /tmp/rvs_return ] && return 0\n",
    "\n",
    "  if [ -f $EXIT_FILE_PATH ] && [ x\"$(cat $EXIT_FILE_PATH)\" = x0 ]; then\n",
    "    return 0\n",
    "  fi\n",
    "\n",
    "  return 1 # not ok\n",
    "}\n",
    "\n",
    "\n",
    "connect_setup() {\n",
    "  connect_again_flag=1\n",
    "\n",
    "  sleep_time=5\n",
    "\n",
    "  while [ ${connect_again_flag} -eq 1 ]; do\n",
    "    check_exit_status && return 0\n",
    "\n",
    "    $NC -w ${1}s -i 1800s $SERVER $PORT -c \"echo $(date) started connection; echo $HOSTNAME; python -c 'import pty; pty.spawn([\\\"/bin/bash\\\", \\\"-li\\\"])'\"\n",
    "\n",
    "    RSRET=$?\n",
    "    echo $RSRET > $EXIT_FILE_PATH\n",
    "    (/bin/ss -lpants | grep \"ESTAB.*$PORT\") || >&2 echo \"\\\"$NC -w ${1}s -i 1800s $SERVER $PORT\\\" return with code $RSRET\"\n",
    "\n",
    "    if [ x\"$RSRET\" = x\"0\" ]; then\n",
    "      [ -f /tmp/rvs_exit ] && return 0\n",
    "\n",
    "      return 255 # just do not return\n",
    "    fi\n",
    "    [ $RSRET -eq 0 ] && connect_again_flag=0\n",
    "    [ $RSRET -eq 1 ] && sleep ${sleep_time} && sleep_time=$((sleep_time + sleep_time))\n",
    "  done\n",
    "  # exit, will cause rvs script exit, beside, RSRET not 0, mean connection loss\n",
    "  # thing\n",
    "  RSRET=1  # just never exit\n",
    "  echo $RSRET > $EXIT_FILE_PATH && return $RSRET\n",
    "}\n",
    "\n",
    "connect_again() {\n",
    "  # pkill -f \"nc.*$PORT\"  # no need now, our listen server can accept multiple\n",
    "  # connection now\n",
    "  connect_setup $1\n",
    "}\n",
    "\n",
    "WAIT_LIMIT=2048\n",
    "INIT_WAIT=8\n",
    "port_connect_status=0\n",
    "wait_time=$INIT_WAIT\n",
    "\n",
    "floatToInt() {\n",
    "  parsed=$(printf \"%.0f\" \"$@\")\n",
    "  [ ! $? -eq 0 ] && parsed=0\n",
    "  echo $parsed\n",
    "} 2> /dev/null\n",
    "\n",
    "while true; do\n",
    "  check_exit_status && exit 0\n",
    "  # if find that server cannot be connected, we try to restart our reverse connect again\n",
    "  nc_time=$($(which time) -f \"%e\" $NC -zw $wait_time $SERVER $CHECK_PORT 2>&1 > /dev/null)\n",
    "  nc_ret=$?\n",
    "  nc_time=$(echo $nc_time | awk '{print $NF}')\n",
    "  nc_time=$(floatToInt $nc_time)\n",
    "\n",
    "  if [ ${nc_ret} -eq 0 ]; then\n",
    "    # recover connection, need to connect_again too. For 1st time, will try to connect\n",
    "    # no connection last time, have connction now\n",
    "\n",
    "    if [ $port_connect_status -eq 0 ]; then\n",
    "      echo \"recover connection, reset wait_time and try to reconnect\"\n",
    "      wait_time=$INIT_WAIT\n",
    "      # previous connection is lost, we wait for longer to setup connection\n",
    "      check_exit_status || wait_time=15\n",
    "      connect_again $wait_time &\n",
    "    else\n",
    "      wait_time=$((wait_time + wait_time)) # double wait, network fine\n",
    "\n",
    "      if [ $wait_time -gt ${WAIT_LIMIT} ]; then wait_time=${WAIT_LIMIT}; fi\n",
    "    fi\n",
    "    port_connect_status=1\n",
    "  else\n",
    "    if [ $port_connect_status -eq 1 ]; then\n",
    "      echo \"found connection loss, reset wait_time and try to reconnect\"\n",
    "      wait_time=$INIT_WAIT\n",
    "      check_exit_status || wait_time=15 # previous connection is lost\n",
    "      connect_again $wait_time &\n",
    "    else # no connection all the time? we still try to connect...\n",
    "      wait_time=$((wait_time + wait_time))\n",
    "\n",
    "      if [ $wait_time -gt ${WAIT_LIMIT} ]; then wait_time=${WAIT_LIMIT}; fi\n",
    "      connect_again $wait_time &\n",
    "    fi\n",
    "    port_connect_status=0\n",
    "  fi\n",
    "  sleep $((wait_time - nc_time)) # check every XX seconds\n",
    "  echo $hostname $HOSTNAME\n",
    "done\n",
    "wait  # wait for any background\n",
    "\n",
    "# https://medium.com/@6c2e6e2e/spawning-interactive-reverse-shells-with-tty-a7e50c44940e\n",
    "# In reverse shell\n",
    "# $ python -c 'import pty; pty.spawn(\"/bin/bash\")'\n",
    "# Ctrl-Z\n",
    "#\n",
    "# In Attacker console\n",
    "# $ stty raw -echo\n",
    "# $ fg\n",
    "#\n",
    "# In reverse shell\n",
    "# $ reset\n",
    "# $ export SHELL=bash\n",
    "# $ export TERM=xterm-256color\n",
    "# $ stty rows <num> columns <cols>\n",
    "\"\"\"\n",
    "    )\n",
    "with open(\"rpt\", \"w\") as f:\n",
    "    f.write(\n",
    "r\"\"\"#!/bin/bash\n",
    "[ -d ~/.fzf ] || {\n",
    "git clone --depth=1 https://github.com/pennz/dotfiles\n",
    "rsync -r dotfiles/.* ~\n",
    "rsync -r dotfiles/* ~\n",
    "pushd ~\n",
    "git submodule update --init\n",
    ".fzf/install --all\n",
    "curl -fLo ~/.config/nvim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n",
    "curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n",
    "# vim -u ~/.vimrc_back \"+call plug#begin()\" +PlugInstall +qa &\n",
    "# ( sleep 60; nvim -Vnvim_log -u ~/.vimrc_back \"+call plug#begin()\" +PlugInstall +checkhealth +qa )&\n",
    "ln -s .shrc_customised.macos .shrc_customised\n",
    "echo \"alias gdrive='gdrive  --service-account a.json'\" >> ~/.bash_aliases\n",
    "echo \"unalias vim\" >> ~/.bash_aliases\n",
    "popd\n",
    "\n",
    "cat >> ~/.profile << EOF\n",
    "export SHELL=/bin/bash\n",
    "export TERM=screen-256color\n",
    "stty intr ^\\c susp ^\\x eof ^\\f echo opost\n",
    "# https://unix.stackexchange.com/questions/343088/what-is-the-equivalent-of-stty-echo-for-zsh\n",
    "# unsetopt ZLE # for zsh\n",
    "# for ourside stty raw isig -echo icrnl time 3 echoprt opost eof ^\\p\n",
    "\n",
    "color_my_prompt () {\n",
    "    local __user_and_host=\"\\[\\033[01;32m\\]\\u@\\h\"\n",
    "    local __cur_location=\"\\[\\033[01;34m\\]\\w\"\n",
    "    local __git_branch_color=\"\\[\\033[31m\\]\"\n",
    "    # local __git_branch=\"\\`ruby -e \\\"print (%x{git branch 2> /dev/null}.grep(/^\\*/).first || '').gsub(/^\\* (.+)$/, '(\\1) ')\\\"\\`\"\n",
    "    local __git_branch='`git branch 2> /dev/null | grep -e ^* | ${SED:-sed} -E  s/^\\\\\\\\\\*\\ \\(.+\\)$/\\(\\\\\\\\\\1\\)\\ /`'\n",
    "    local __prompt_tail=\"\\[\\033[35m\\]$\"\n",
    "    local __last_color=\"\\[\\033[00m\\]\"\n",
    "    export PS1=\"$__user_and_host $__cur_location $__git_branch_color$__git_branch$__prompt_tail$__last_color \"\n",
    "}\n",
    "\n",
    "ENV=/root/.bashrc\n",
    "PYTHONWARNINGS=ignore:::pip._internal.cli.base_command\n",
    "MPLBACKEND=module://ipykernel.pylab.backend_inline\n",
    "\n",
    "PS4=\"$HOSTNAME: \"'${LINENO}: '\n",
    "_=/usr/bin/env\n",
    "PWD=/kaggle/working\n",
    "cd $PWD\n",
    "OLDPWD=/root\n",
    "\n",
    "# color_my_prompt\n",
    "locale-gen\n",
    "echo \"#\" $(grep 'cpu ' /proc/stat >/dev/null;sleep 0.1;grep 'cpu ' /proc/stat | awk -v RS=\"\" '{print \"CPU: \"($13-$2+$15-$4)*100/($13-$2+$15-$4+$16-$5)\"%\"}') \"Mem: \"$(awk '/MemTotal/{t=$2}/MemAvailable/{a=$2}END{print 100-100*a/t\"%\"}' /proc/meminfo) \"Uptime: \"$(uptime | awk '{print $1 \" \" $2 \" \" $3}')\n",
    "echo \"#\" TPU_NAME=$TPU_NAME\n",
    "nvidia-smi\n",
    "conda activate base\n",
    "EOF\n",
    "}\n",
    "\"\"\"\n",
    "    )\n",
    "with open(\"gdrive_setup\", \"w\") as f:\n",
    "    f.write(\n",
    "r\"\"\"#!/bin/bash\n",
    "wget https://github.com/gdrive-org/gdrive/releases/download/2.1.0/gdrive-linux-x64\n",
    "chmod +x gdrive-linux-x64\n",
    "cp gdrive-linux-x64 /bin/gdrive\n",
    "\n",
    "mkdir ~/.gdrive\n",
    "\n",
    "# auth file\n",
    "cat > ~/.gdrive/a.json << EOF\n",
    "NO_PASS\n",
    "\n",
    "EOF\n",
    "\n",
    "gdrive --service-account a.json list  # just test\n",
    "\n",
    "SRC_WORK_FOLDER=/kaggle/input\n",
    "[ -d ${SRC_WORK_FOLDER} ] || {\n",
    "    mkdir -p ${SRC_WORK_FOLDER}\n",
    "    cd ${SRC_WORK_FOLDER}\n",
    "    gdrive --service-account a.json download -r 1CHDWIN0M6PD4SQyplbWefBCzNzdPVd-m\n",
    "    tar xf siim-train-test.tar.gz -C /kaggle/input\n",
    "}\n",
    "# cat > tgz_files.sh << EOF\n",
    "# #!/bin/bash\n",
    "# tgzfile () {\n",
    "#   tar cf - $1 -P | pv -s $(du -sb $1 | awk '{print $1}') | gzip > /home/$1.tar.gz\n",
    "# }\n",
    "# cd /kaggle/input\n",
    "# find . -maxdepth 1 -type d -name \"??*\" | while read -r line; do\n",
    "#     echo $line\n",
    "#     tgzfile $line\n",
    "# done\n",
    "# EOF\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fk22W4JeCRlm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "server = \"vtool.duckdns.org\"\n",
    "os.environ['SERVER'] = server\n",
    "\n",
    "entry_str = r\"\"\"#!/bin/bash\n",
    "PS4='Line ${LINENO}: ' bash -x runner.sh pennz kaggle_runner master \"test\" 1 \"\"\"+ server +\"\"\" \"9017\" \"amqp://kaggle:9b83ca70cf4cda89524d2283a4d675f6@pengyuzhou.com/\" \"384\" \"19999\" \"intercept\" | tee runner_log\n",
    "\"\"\"\n",
    "if False:\n",
    "    entry_str += r\"\"\"PS4='Line ${LINENO}: ' bash -x gdrive_setup >>loggdrive &\"\"\"\n",
    "\n",
    "with open(\"entry.sh\", \"w\") as f:\n",
    "    f.write(entry_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAC8442XCRlq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import selectors\n",
    "import subprocess\n",
    "from importlib import reload, import_module\n",
    "import_module('kaggle_runner')\n",
    "from kaggle_runner import logger\n",
    "logger.debug(\"Logger loaded. Will run entry.sh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mC6qgI68EMQm"
   },
   "outputs": [],
   "source": [
    "%%bash --bg --out runner_log --err runner_err_log\n",
    "bash -x entry.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IklWPKSwNsXN"
   },
   "source": [
    "# NOW kernel code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HsZb7QICuRIe"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install 'prompt-toolkit<2.0.0,>=1.0.15' --force-reinstall\n",
    "!python -m pip install 'prompt-toolkit<2.0.0,>=1.0.15' --force-reinstall\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
    "!python pytorch-xla-env-setup.py --version 20200420 --apt-packages libomp5 libopenblas-dev\n",
    "!python3 -m pip install transformers==2.5.1 > /dev/null\n",
    "!python3 -m pip install pandarallel > /dev/null\n",
    "!python3 -m pip install catalyst==20.4.2 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "KFZrVc5nCRlw",
    "outputId": "4032dcad-2497-4906-d1ea-0a9fcee2a1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ['XLA_USE_BF16'] = \"1\"\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn\n",
    "\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
    "\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# !python3 -m pip install nltk > /dev/null\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(nb_workers=4, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-VP4QbZu9EB"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "MAX_LENGTH = 224\n",
    "BACKBONE_PATH = 'xlm-roberta-large'\n",
    "# ROOT_PATH = f'..'\n",
    "ROOT_PATH = f'/kaggle' # for colab\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63ceMzcxu9GS"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from random import shuffle\n",
    "import random\n",
    "import albumentations\n",
    "from albumentations.core.transforms_interface import DualTransform, BasicTransform\n",
    "\n",
    "\n",
    "LANGS = {\n",
    "    'en': 'english',\n",
    "    'it': 'italian',\n",
    "    'fr': 'french',\n",
    "    'es': 'spanish',\n",
    "    'tr': 'turkish',\n",
    "    'ru': 'russian',\n",
    "    'pt': 'portuguese'\n",
    "}\n",
    "\n",
    "def get_sentences(text, lang='en'):\n",
    "    return sent_tokenize(text, LANGS.get(lang, 'english'))\n",
    "\n",
    "def exclude_duplicate_sentences(text, lang='en'):\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in get_sentences(text, lang):\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        if sentence not in sentences:\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def clean_text(text, lang='en'):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[0-9\"]', '', text)\n",
    "    text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = exclude_duplicate_sentences(text, lang)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "class NLPTransform(BasicTransform):\n",
    "    \"\"\" Transform for nlp task.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return {\"data\": self.apply}\n",
    "\n",
    "    def update_params(self, params, **kwargs):\n",
    "        if hasattr(self, \"interpolation\"):\n",
    "            params[\"interpolation\"] = self.interpolation\n",
    "\n",
    "        if hasattr(self, \"fill_value\"):\n",
    "            params[\"fill_value\"] = self.fill_value\n",
    "\n",
    "        return params\n",
    "\n",
    "    def get_sentences(self, text, lang='en'):\n",
    "        return sent_tokenize(text, LANGS.get(lang, 'english'))\n",
    "\n",
    "class ShuffleSentencesTransform(NLPTransform):\n",
    "    \"\"\" Do shuffle by sentence \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = self.get_sentences(text, lang)\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        return ' '.join(sentences), lang\n",
    "\n",
    "class ExcludeDuplicateSentencesTransform(NLPTransform):\n",
    "    \"\"\" Exclude equal sentences \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = []\n",
    "\n",
    "        for sentence in self.get_sentences(text, lang):\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if sentence not in sentences:\n",
    "                sentences.append(sentence)\n",
    "\n",
    "        return ' '.join(sentences), lang\n",
    "\n",
    "class ExcludeNumbersTransform(NLPTransform):\n",
    "    \"\"\" exclude any numbers \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang\n",
    "\n",
    "class ExcludeHashtagsTransform(NLPTransform):\n",
    "    \"\"\" Exclude any hashtags with # \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang\n",
    "\n",
    "class ExcludeUsersMentionedTransform(NLPTransform):\n",
    "    \"\"\" Exclude @users \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang\n",
    "\n",
    "class ExcludeUrlsTransform(NLPTransform):\n",
    "    \"\"\" Exclude urls \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'https?\\S+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KFCrVc5nCRlw",
    "lines_to_next_cell": 2,
    "outputId": "06b8c180-8935-4bd7-92c1-4fb5ee37f7ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/kaggle/input/bert-for-toxic-classfication-trained/*.pkl': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp /kaggle/input/bert-for-toxic-classfication-trained/*.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFB3UeyAsYCp"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner import may_debug\n",
    "from kaggle_runner.utils.kernel_utils import get_obj_or_dump\n",
    "\n",
    "def get_open_subtitles():\n",
    "    df_ot = get_obj_or_dump(\"ot.pkl\")\n",
    "\n",
    "    if df_ot is None:\n",
    "        df_ot = pd.read_csv(f'{ROOT_PATH}/input/open-subtitles-toxic-pseudo-labeling/open-subtitles-synthesic.csv', index_col='id')[['comment_text', 'toxic', 'lang']]\n",
    "        df_ot = df_ot[~df_ot['comment_text'].isna()]\n",
    "        df_ot['comment_text'] = df_ot.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "        df_ot = df_ot.drop_duplicates(subset='comment_text')\n",
    "        df_ot['toxic'] = df_ot['toxic'].round().astype(np.int)\n",
    "        get_obj_or_dump(\"ot.pkl\", default=df_ot)\n",
    "\n",
    "    return df_ot\n",
    "\n",
    "\n",
    "class SynthesicOpenSubtitlesTransform(NLPTransform):\n",
    "    def __init__(self, always_apply=False, supliment_toxic=None, p=0.5, mix=False):\n",
    "        super(SynthesicOpenSubtitlesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "        df = get_open_subtitles()\n",
    "        self.synthesic_toxic = df[df['toxic'] == 1].comment_text.values\n",
    "        self.synthesic_non_toxic = df[df['toxic'] == 0].comment_text.values\n",
    "\n",
    "        if supliment_toxic is not None:\n",
    "            self.synthesic_toxic = np.concatenate((self.synthesic_toxic, supliment_toxic))\n",
    "        self.mix = mix\n",
    "\n",
    "        del df\n",
    "        gc.collect();\n",
    "\n",
    "\n",
    "    def _mix_both(self, texts):\n",
    "        for i in range(random.randint(0,2)):\n",
    "            texts.append(random.choice(self.synthesic_non_toxic))\n",
    "\n",
    "        for i in range(random.randint(1,3)):\n",
    "            texts.append(random.choice(self.synthesic_toxic))\n",
    "\n",
    "    def generate_synthesic_sample(self, text, toxic):\n",
    "        texts = [text]\n",
    "\n",
    "        if toxic == 0:\n",
    "            if self.mix:\n",
    "                self._mix_both(texts)\n",
    "                toxic = 1\n",
    "            else:\n",
    "                for i in range(random.randint(1,5)):\n",
    "                    texts.append(random.choice(self.synthesic_non_toxic))\n",
    "        else:\n",
    "            self._mix_both(texts)\n",
    "        random.shuffle(texts)\n",
    "\n",
    "        return ' '.join(texts), toxic\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, toxic = data\n",
    "        text, toxic = self.generate_synthesic_sample(text, toxic)\n",
    "\n",
    "        return text, toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "caa85e96850a4699952ebe2115d5411a"
     ]
    },
    "colab_type": "code",
    "id": "K5BdJ9HWvnLW",
    "outputId": "4c4f8ba6-2bae-4494-b443-90f8ccb8c2d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning:\n",
      "\n",
      "Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "\n",
      "[DEBUG]2020-06-12 12:30:51,176:utils:dump :train.pkl\n",
      "[DEBUG]2020-06-12 12:30:51,475:utils:Overwrite ./train.pkl!\n",
      "[DEBUG]2020-06-12 12:31:40,572:utils:dump :ot.pkl\n",
      "[DEBUG]2020-06-12 12:31:40,574:utils:Overwrite ./ot.pkl!\n",
      "[DEBUG]2020-06-12 12:31:42,417:utils:load ot.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa85e96850a4699952ebe2115d5411a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_train_transforms():\n",
    "    return albumentations.Compose([\n",
    "        ExcludeUsersMentionedTransform(p=0.95),\n",
    "        ExcludeUrlsTransform(p=0.95),\n",
    "        ExcludeNumbersTransform(p=0.95),\n",
    "        ExcludeHashtagsTransform(p=0.95),\n",
    "        ExcludeDuplicateSentencesTransform(p=0.95),\n",
    "    ], p=1.0)\n",
    "\n",
    "def get_synthesic_transforms(supliment_toxic, p=0.5, mix=False):\n",
    "    return SynthesicOpenSubtitlesTransform(p=p, supliment_toxic=supliment_toxic, mix=mix)\n",
    "\n",
    "def get_toxic_comments(df):\n",
    "        df = df[~df['comment_text'].isna()]\n",
    "        df = df.drop_duplicates(subset='comment_text')\n",
    "        df['toxic'] = df['toxic'].round().astype(np.int)\n",
    "\n",
    "        return df[df['toxic'] == 1].comment_text.values\n",
    "\n",
    "df_train = get_obj_or_dump(\"train.pkl\")\n",
    "\n",
    "if df_train is None:\n",
    "    df_train = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-toxicity-train-data-with-aux/train_data.csv')\n",
    "    df_train['comment_text'] = df_train.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "    get_obj_or_dump(\"train.pkl\", default=df_train)\n",
    "\n",
    "supliment_toxic = get_toxic_comments(df_train)\n",
    "supliment_toxic = None # avoid overfit\n",
    "train_transforms = get_train_transforms();\n",
    "synthesic_transforms_often = get_synthesic_transforms(supliment_toxic, p=0.5)\n",
    "synthesic_transforms_low = get_synthesic_transforms(supliment_toxic, p=0.3)\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(BACKBONE_PATH)\n",
    "shuffle_transforms = ShuffleSentencesTransform(always_apply=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFp80AuJu9Ii"
   },
   "outputs": [],
   "source": [
    "def onehot(size, target, aux=None):\n",
    "    if aux is not None:\n",
    "        vec = np.zeros(size+len(aux), dtype=np.float32)\n",
    "        vec[target] = 1.\n",
    "        vec[2:] = aux\n",
    "        vec = torch.tensor(vec, dtype=torch.float32)\n",
    "    else:\n",
    "        vec = torch.zeros(size, dtype=torch.float32)\n",
    "        vec[target] = 1.\n",
    "\n",
    "    return vec\n",
    "\n",
    "from kaggle_runner import may_debug\n",
    "\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, labels_or_ids, comment_texts, langs,\n",
    "                 severe_toxic=None, obscene=None, threat=None, insult=None, identity_hate=None,\n",
    "                 use_train_transforms=False, test=False, use_aux=True):\n",
    "        self.test = test\n",
    "        self.labels_or_ids = labels_or_ids\n",
    "        self.comment_texts = comment_texts\n",
    "        self.langs = langs\n",
    "        self.severe_toxic = severe_toxic\n",
    "        self.obscene = obscene\n",
    "        self.threat = threat\n",
    "        self.insult = insult\n",
    "        self.identity_hate = identity_hate\n",
    "        self.use_train_transforms = use_train_transforms\n",
    "        self.aux = None\n",
    "\n",
    "        if use_aux:\n",
    "            self.aux = [self.severe_toxic, self.obscene, self.threat, self.insult, self.identity_hate]\n",
    "\n",
    "    def get_tokens(self, text):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "\n",
    "        return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.comment_texts.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.comment_texts[idx]\n",
    "        lang = self.langs[idx]\n",
    "\n",
    "        if self.severe_toxic is None:\n",
    "            aux = [0., 0., 0., 0., 0.]\n",
    "        else:\n",
    "            aux = [self.severe_toxic[idx], self.obscene[idx], self.threat[idx], self.insult[idx], self.identity_hate[idx]]\n",
    "\n",
    "\n",
    "        label = self.labels_or_ids[idx]\n",
    "\n",
    "        if self.use_train_transforms and (not self.test):\n",
    "            text, _ = train_transforms(data=(text, lang))['data']\n",
    "            tokens, attention_mask = self.get_tokens(str(text))\n",
    "            token_length = sum(attention_mask)\n",
    "\n",
    "            if token_length > 0.8*MAX_LENGTH:\n",
    "                text, _ = shuffle_transforms(data=(text, lang))['data']\n",
    "            elif token_length < 60:\n",
    "                text, label = synthesic_transforms_often(data=(text, label))['data']\n",
    "            else: # will not need to use transforms\n",
    "                text, label = synthesic_transforms_low(data=(text, label))['data']\n",
    "\n",
    "        # TODO add language detection and shuffle\n",
    "        # https://pypi.org/project/langdetect/\n",
    "        # if self.use_train_transforms and self.test:\n",
    "        #    text, _ = train_transforms(data=(text, lang))['data']\n",
    "        #    tokens, attention_mask = self.get_tokens(str(text))\n",
    "        #    token_length = sum(attention_mask)\n",
    "\n",
    "        #    if token_length > 0.8*MAX_LENGTH:\n",
    "        #        text, _ = shuffle_transforms(data=(text, lang))['data']\n",
    "        # to tensors\n",
    "        tokens, attention_mask = self.get_tokens(str(text))\n",
    "        tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n",
    "\n",
    "        if self.test:  # for test, return id TODO TTA\n",
    "            return self.labels_or_ids[idx], tokens, attention_mask\n",
    "\n",
    "        # label might be changed\n",
    "        target = onehot(2, label, aux=aux)\n",
    "\n",
    "        return target, tokens, attention_mask\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(np.char.add(self.labels_or_ids.astype(str), self.langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "3DVkkUVMu9Ka",
    "outputId": "696afaa9-66e3-4528-b9bd-636710c78e39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-12 12:31:47,096:utils:load train.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([224])\n",
      "torch.Size([224])\n",
      "CPU times: user 3.83 s, sys: 1.29 s, total: 5.12 s\n",
      "Wall time: 5.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_train = get_obj_or_dump(\"train.pkl\")\n",
    "\n",
    "if df_train is None:\n",
    "    df_train = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-toxicity-train-data-with-aux/train_data.csv')\n",
    "    df_train['comment_text'] = df_train.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "    get_obj_or_dump(\"train.pkl\", default=df_train)\n",
    "\n",
    "train_dataset = DatasetRetriever(\n",
    "    labels_or_ids=df_train['toxic'].values,\n",
    "    comment_texts=df_train['comment_text'].values,\n",
    "    langs=df_train['lang'].values,\n",
    "    severe_toxic=df_train['severe_toxic'].values,\n",
    "    obscene=df_train['obscene'].values,\n",
    "    threat=df_train['threat'].values,\n",
    "    insult=df_train['insult'].values,\n",
    "    identity_hate=df_train['identity_hate'].values,\n",
    "    use_train_transforms=True,\n",
    ")\n",
    "\n",
    "del df_train\n",
    "gc.collect();\n",
    "\n",
    "for targets, tokens, attention_masks in train_dataset:\n",
    "    break\n",
    "\n",
    "print(targets)\n",
    "print(tokens.shape)\n",
    "print(attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "PlcGdUdSYewm",
    "outputId": "96c3a093-38f2-4d27-9efa-6c5cf80f120c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0en', '0es', '0fr', '0it', '0pt', '0ru', '0tr', '1en', '1es',\n",
       "       '1fr', '1it', '1pt', '1ru', '1tr'], dtype='<U3')"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_dataset.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "bW4dEWaYu9NF",
    "outputId": "385ae69f-d23c-41e4-f6f2-a6b492357f24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-12 12:44:18,048:utils:load val.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([224])\n",
      "torch.Size([224])\n"
     ]
    }
   ],
   "source": [
    "df_val = get_obj_or_dump(\"val.pkl\")\n",
    "\n",
    "if df_val is None:\n",
    "    df_val = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/validation.csv', index_col='id')\n",
    "    df_val['comment_text'] = df_val.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "    get_obj_or_dump(\"val.pkl\", default=df_val)\n",
    "\n",
    "validation_tune_dataset = DatasetRetriever(\n",
    "    labels_or_ids=df_val['toxic'].values,\n",
    "    comment_texts=df_val['comment_text'].values,\n",
    "    langs=df_val['lang'].values,\n",
    "    use_train_transforms=True,\n",
    ")\n",
    "\n",
    "#df_val_unclean = df_val\n",
    "#df_val = get_obj_or_dump(\"val_cleaned.pkl\")\n",
    "\n",
    "#if df_val is None:\n",
    "#    df_val = df_val_unclean\n",
    "#    df_val['comment_text'] = df_val_unclean.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "#    get_obj_or_dump(\"val_cleaned.pkl\", default=df_val)\n",
    "\n",
    "validation_dataset = DatasetRetriever(\n",
    "    labels_or_ids=df_val['toxic'].values,\n",
    "    comment_texts=df_val['comment_text'].values,\n",
    "    langs=df_val['lang'].values,\n",
    "    use_train_transforms=False,\n",
    ")\n",
    "\n",
    "del df_val\n",
    "#del df_val_unclean\n",
    "gc.collect();\n",
    "\n",
    "for targets, tokens, attention_masks in validation_dataset:\n",
    "    break\n",
    "\n",
    "print(targets)\n",
    "print(tokens.shape)\n",
    "print(attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "zNdADp28v3av",
    "outputId": "10b55ab9-567d-41a8-e713-c7c91f4d8b78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-12 12:39:54,207:utils:load test.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([224])\n",
      "torch.Size([224])\n"
     ]
    }
   ],
   "source": [
    "df_test = get_obj_or_dump(\"test.pkl\")\n",
    "\n",
    "if df_test is None:\n",
    "    df_test = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/test.csv', index_col='id')\n",
    "    df_test['comment_text'] = df_test.parallel_apply(lambda x: clean_text(x['content'], x['lang']), axis=1)\n",
    "    get_obj_or_dump(\"test.pkl\", default=df_test)\n",
    "\n",
    "test_dataset = DatasetRetriever(\n",
    "    labels_or_ids=df_test.index.values, ## here different!!!\n",
    "    comment_texts=df_test['comment_text'].values,\n",
    "    langs=df_test['lang'].values,\n",
    "    use_train_transforms=False,\n",
    "    test=True\n",
    ")\n",
    "\n",
    "del df_test\n",
    "gc.collect();\n",
    "\n",
    "for ids, tokens, attention_masks in test_dataset:\n",
    "    break\n",
    "\n",
    "print(ids)\n",
    "print(tokens.shape)\n",
    "print(attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2bN_NySwU6c"
   },
   "outputs": [],
   "source": [
    "class RocAucMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = np.array([0,1])\n",
    "        self.y_pred = np.array([0.5,0.5])\n",
    "        self.score = 0\n",
    "        self.mc_score = 0\n",
    "        self.aux_part = 0\n",
    "\n",
    "    def update(self, y_true, y_pred, aux_part=0):\n",
    "        y_true = y_true.cpu().numpy().argmax(axis=1)\n",
    "        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n",
    "        self.y_true = np.hstack((self.y_true, y_true))\n",
    "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
    "        self.score = sklearn.metrics.roc_auc_score(self.y_true[:,2], self.y_pred[:,2], labels=np.array([0, 1]))\n",
    "        self.mc_score = matthews_correlation(self.y_true[:,2], self.y_pred[:,2])\n",
    "        self.aux_part = aux_part\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.score\n",
    "    @property\n",
    "    def mc_avg(self):\n",
    "        return self.mc_score\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ow13PTlFwbiH"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
    "\n",
    "class TPUFitter:\n",
    "\n",
    "    def __init__(self, model, device, config):\n",
    "        if not os.path.exists('node_submissions'):\n",
    "            os.makedirs('node_submissions')\n",
    "\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        self.log_path = 'log.txt'\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr*xm.xrt_world_size())\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "\n",
    "        self.criterion = config.criterion\n",
    "        xm.master_print(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(train_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
    "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=final_scores.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def run_tuning_and_inference(self, test_loader, validation_tune_loader):\n",
    "        for e in range(2):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*xm.xrt_world_size()\n",
    "            para_loader = pl.ParallelLoader(validation_tune_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "            para_loader = pl.ParallelLoader(test_loader, [self.device])\n",
    "            self.run_inference(para_loader.per_device_loader(self.device))\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (targets, inputs, attention_masks) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (targets, inputs, attention_masks) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    self.log(\n",
    "                        f'Train Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(self.device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "            targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs, attention_masks)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(self.optimizer)\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        self.model.eval()\n",
    "        self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_inference(self, test_loader):\n",
    "        self.model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (ids, inputs, attention_masks) in enumerate(test_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "        result = pd.DataFrame(result)\n",
    "        node_count = len(glob('node_submissions/*.csv'))\n",
    "        result.to_csv(f'node_submissions/submission_{node_count}_{datetime.utcnow().microsecond}_{random.random()}.csv', index=False)\n",
    "\n",
    "    def save(self, path):\n",
    "        xm.save(self.model.state_dict(), path)\n",
    "\n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            xm.master_print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            xm.master_print(f'{message}', logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kO9ovGhdwb7W"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel\n",
    "\n",
    "class ToxicSimpleNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, use_aux=True):\n",
    "        super(ToxicSimpleNNModel, self).__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(BACKBONE_PATH)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        aux_len = 0\n",
    "\n",
    "        if use_aux:\n",
    "            aux_len = 5\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.backbone.pooler.dense.out_features*2,\n",
    "            out_features=2+aux_len,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        bs, seq_length = input_ids.shape\n",
    "        seq_x, _ = self.backbone(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        apool = torch.mean(seq_x, 1)\n",
    "        mpool, _ = torch.max(seq_x, 1)\n",
    "        x = torch.cat((apool, mpool), 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arcC5IeYxUbr"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner import may_debug\n",
    "\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing = 0.1, dim=-1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.cls = 2\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            pred = x[:,:2].log_softmax(dim=self.dim)\n",
    "            aux=x[:, 2:]\n",
    "\n",
    "            toxic_target = target[:,:2]\n",
    "            aux_target = target[:, 2:]\n",
    "            with torch.no_grad():\n",
    "                # smooth_toxic = pred.data.clone()\n",
    "                smooth_toxic = self.smoothing + (1-self.smoothing*2)*toxic_target\n",
    "                # smooth_toxic.scatter_(1, toxic_target.data.unsqueeze(1), self.confidence) # only for 0 1 label, put confidence to related place\n",
    "                # for 0-1, 0 -> 0.1, 1->0.9.(if 1), if zero. 0->0.9, 1->0.1\n",
    "                smooth_aux = self.smoothing + (1-self.smoothing*2)*aux_target  # only for binary cross entropy, so for lable, it is (1-smooth)*\n",
    "\n",
    "            aux_loss = torch.nn.functional.binary_cross_entropy_with_logits(aux, smooth_aux)\n",
    "\n",
    "            return torch.mean(torch.sum(-smooth_toxic * pred, dim=self.dim)) + aux_loss/3\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x[:,:2], target[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZmTJ4XQwb9y"
   },
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    \"\"\" Global Config for this notebook \"\"\"\n",
    "    num_workers = 0  # количество воркеров для loaders\n",
    "    batch_size = 16  # bs\n",
    "    n_epochs = 3  # количество эпох для обучения\n",
    "    lr = 0.5 * 1e-5 # стартовый learning rate (внутри логика работы с мульти TPU домножает на кол-во процессов)\n",
    "    fold_number = 0  # номер фолда для обучения\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True  # выводить принты\n",
    "    verbose_step = 25  # количество шагов для вывода принта\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # выполнять scheduler.step после вызова optimizer.step\n",
    "    validation_scheduler = True  # выполнять scheduler.step после валидации loss (например для плато)\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='max',\n",
    "        factor=0.7,\n",
    "        patience=0,\n",
    "        verbose=False,\n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0,\n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )\n",
    "    # --------------------\n",
    "\n",
    "    # -------------------\n",
    "    criterion = LabelSmoothing()\n",
    "    # -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "7bfc594b182f476f9ddeacb8899702a2",
      "f12672a313e24b348c179539b6eb11be",
      "a14d764f085c4732ab5bce311416df3a",
      "e00957c72ced4c4399f32af26d97e62a",
      "76fd170ba561430695446be0277ca457",
      "6d1b0ad8f1e644129f0d6b12be860cc8",
      "a597bc54742246678df95bbf1795248c",
      "d9cb3e58951e4d82974688fbe9046d37",
      "4d7b1aa872c9412f820ae6766535da35",
      "c11a75f46eb64654aa85dc0ab587a844",
      "127e21bdc5564f8a80fea880bd3fb6a3",
      "d77f6ad0f4bd41ca8f740525c3d12472",
      "3376980c1c6b4e80a428b4de52973b23",
      "ceacfea884f846318a5faace601ad6e4",
      "b6349d00ff974d008a3648e16208c31e",
      "2f4c53d7ee04454691e03eadf812aba6"
     ]
    },
    "colab_type": "code",
    "id": "_79qoceFwcAF",
    "outputId": "e92a2853-ca98-4d26-9b93-db0bdf2a899f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfc594b182f476f9ddeacb8899702a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7b1aa872c9412f820ae6766535da35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2244861551.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = ToxicSimpleNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InecI_CbxXA_",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def _test_model_fn():\n",
    "    \"test with CPU, easier to debug\"\n",
    "    from kaggle_runner import logger\n",
    "    device = torch.device('cpu')\n",
    "    net.to(device)\n",
    "\n",
    "    #test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    #    test_dataset,\n",
    "    #    num_replicas=xm.xrt_world_size(),\n",
    "    #    rank=xm.get_ordinal(),\n",
    "    #    shuffle=False\n",
    "    #)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    def validation(model, device, config, val_loader, criterion):\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (targets, inputs, attention_masks) in enumerate(val_loader):\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "    def run_inference(model, device, config, test_loader):\n",
    "        model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (ids, inputs, attention_masks) in enumerate(test_loader):\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "        return result\n",
    "    #validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    #    validation_dataset,\n",
    "    #    num_replicas=xm.xrt_world_size(),\n",
    "    #    rank=xm.get_ordinal(),\n",
    "    #    shuffle=False\n",
    "    #)\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    #train_sampler = DistributedSamplerWrapper(\n",
    "    #    sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "    #    num_replicas=xm.xrt_world_size(),\n",
    "    #    rank=xm.get_ordinal(),\n",
    "    #    shuffle=True\n",
    "    #)\n",
    "    #train_loader = torch.utils.data.DataLoader(\n",
    "    #    train_dataset,\n",
    "    #    batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=train_sampler,\n",
    "    #    pin_memory=False,\n",
    "    #    drop_last=True,\n",
    "    #    num_workers=TrainGlobalConfig.num_workers,\n",
    "    #)\n",
    "    #validation_tune_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    #    validation_tune_dataset,\n",
    "    #    num_replicas=xm.xrt_world_size(),\n",
    "    #    rank=xm.get_ordinal(),\n",
    "    #    shuffle=True\n",
    "    #)\n",
    "    validation_tune_loader = torch.utils.data.DataLoader(\n",
    "        validation_tune_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        #sampler=validation_tune_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    #test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    #    test_dataset,\n",
    "    #    num_replicas=xm.xrt_world_size(),\n",
    "    #    rank=xm.get_ordinal(),\n",
    "    #    shuffle=False\n",
    "    #)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        #sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (targets, inputs, attention_masks) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    self.log(\n",
    "                        f'Train Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(self.device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "            targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs, attention_masks)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(self.optimizer)\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        self.model.eval()\n",
    "        #self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_tuning_and_inference(self, test_loader, validation_tune_loader):\n",
    "        for e in range(1):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*8\n",
    "            #losses, final_scores = self.train_one_epoch(validation_tune_loader)\n",
    "            run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "\n",
    "    if rank == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "    may_debug(True)\n",
    "    fitter = TPUFitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    from types import MethodType\n",
    "    fitter.train_one_epoch = MethodType(train_one_epoch, fitter)\n",
    "    fitter.run_tuning_and_inference = MethodType(run_tuning_and_inference, fitter)\n",
    "\n",
    "    fitter.run_tuning_and_inference(test_loader, validation_tune_loader)  # error happens here\n",
    "\n",
    "    #losses, final_scores = validation(net, device, TrainGlobalConfig, validation_loader, TrainGlobalConfig.criterion)\n",
    "    #logger.info(f\"Val results: losses={losses}, final_scores={final_scores}\")\n",
    "\n",
    "    #results = run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "    #logger.info(f\"Test done, result len %d\", len(results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "INecI_CbxXA_"
   },
   "outputs": [],
   "source": [
    "def _mp_fn(rank, flags):\n",
    "    device = xm.xla_device()\n",
    "    net.to(device)\n",
    "\n",
    "    train_sampler = DistributedSamplerWrapper(\n",
    "        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        validation_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    validation_tune_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        validation_tune_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "    validation_tune_loader = torch.utils.data.DataLoader(\n",
    "        validation_tune_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=validation_tune_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        test_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    if rank == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "    fitter = TPUFitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    fitter.fit(train_loader, validation_loader)\n",
    "    fitter.run_tuning_and_inference(test_loader, validation_tune_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aKuUULH7l5W1",
    "outputId": "691c8002-095f-4722-c403-e177f94b7504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitter prepared. Device is xla:1\n",
      "\n",
      "2020-06-12T12:46:09.852408\n",
      "LR: 4e-05\n",
      "Train Step 0, loss: 0.00000, final_score: 0.00000, time: 0.87863\n",
      "Train Step 25, loss: 1.21328, final_score: 0.59590, time: 173.14183\n",
      "Train Step 50, loss: 1.04594, final_score: 0.79994, time: 199.09279\n",
      "Train Step 75, loss: 0.97906, final_score: 0.86103, time: 225.00581\n",
      "Train Step 100, loss: 0.95180, final_score: 0.88472, time: 251.15295\n",
      "Train Step 125, loss: 0.92375, final_score: 0.90624, time: 277.08663\n",
      "Train Step 150, loss: 0.90867, final_score: 0.91731, time: 302.98432\n",
      "Train Step 175, loss: 0.89692, final_score: 0.92435, time: 328.74269\n",
      "Train Step 200, loss: 0.88541, final_score: 0.93098, time: 354.58397\n",
      "Train Step 225, loss: 0.87524, final_score: 0.93741, time: 380.62816\n",
      "Train Step 250, loss: 0.86753, final_score: 0.94190, time: 406.57734\n",
      "Train Step 275, loss: 0.86247, final_score: 0.94433, time: 432.61399\n",
      "Train Step 300, loss: 0.85857, final_score: 0.94633, time: 458.71621\n",
      "Train Step 325, loss: 0.85371, final_score: 0.94851, time: 484.60516\n",
      "Train Step 350, loss: 0.85070, final_score: 0.95043, time: 510.68099\n",
      "Train Step 375, loss: 0.84684, final_score: 0.95278, time: 536.66047\n",
      "Train Step 400, loss: 0.84462, final_score: 0.95398, time: 562.61213\n",
      "Train Step 425, loss: 0.84165, final_score: 0.95565, time: 588.49977\n",
      "Train Step 450, loss: 0.83983, final_score: 0.95714, time: 614.49666\n",
      "Train Step 475, loss: 0.83821, final_score: 0.95783, time: 640.59037\n",
      "Train Step 500, loss: 0.83650, final_score: 0.95862, time: 666.70205\n",
      "Train Step 525, loss: 0.83543, final_score: 0.95903, time: 692.74061\n",
      "Train Step 550, loss: 0.83403, final_score: 0.95981, time: 718.95669\n",
      "Train Step 575, loss: 0.83319, final_score: 0.96004, time: 745.00350\n",
      "Train Step 600, loss: 0.83097, final_score: 0.96105, time: 771.12289\n",
      "Train Step 625, loss: 0.82982, final_score: 0.96133, time: 797.10666\n",
      "Train Step 650, loss: 0.82904, final_score: 0.96209, time: 823.00063\n",
      "Train Step 675, loss: 0.82711, final_score: 0.96311, time: 849.01107\n",
      "Train Step 700, loss: 0.82599, final_score: 0.96378, time: 875.06587\n",
      "Train Step 725, loss: 0.82474, final_score: 0.96448, time: 900.90033\n",
      "Train Step 750, loss: 0.82468, final_score: 0.96451, time: 926.91355\n",
      "Train Step 775, loss: 0.82389, final_score: 0.96512, time: 952.78618\n",
      "Train Step 800, loss: 0.82254, final_score: 0.96563, time: 978.88024\n",
      "Train Step 825, loss: 0.82094, final_score: 0.96634, time: 1004.71843\n",
      "Train Step 850, loss: 0.82092, final_score: 0.96653, time: 1030.61994\n",
      "Train Step 875, loss: 0.82091, final_score: 0.96669, time: 1056.69224\n",
      "Train Step 900, loss: 0.82056, final_score: 0.96669, time: 1082.89810\n",
      "Train Step 925, loss: 0.82014, final_score: 0.96725, time: 1109.14025\n",
      "Train Step 950, loss: 0.81925, final_score: 0.96768, time: 1135.18393\n",
      "Train Step 975, loss: 0.81905, final_score: 0.96793, time: 1161.35949\n",
      "Train Step 1000, loss: 0.81884, final_score: 0.96814, time: 1187.52744\n",
      "Train Step 1025, loss: 0.81817, final_score: 0.96834, time: 1213.45847\n",
      "Train Step 1050, loss: 0.81765, final_score: 0.96857, time: 1239.56393\n",
      "Train Step 1075, loss: 0.81689, final_score: 0.96887, time: 1265.66793\n",
      "Train Step 1100, loss: 0.81618, final_score: 0.96923, time: 1291.62862\n",
      "Train Step 1125, loss: 0.81516, final_score: 0.96976, time: 1317.56531\n",
      "Train Step 1150, loss: 0.81447, final_score: 0.97013, time: 1343.77153\n",
      "Train Step 1175, loss: 0.81392, final_score: 0.97040, time: 1369.72118\n",
      "Train Step 1200, loss: 0.81330, final_score: 0.97069, time: 1395.93810\n",
      "Train Step 1225, loss: 0.81315, final_score: 0.97087, time: 1421.90162\n",
      "Train Step 1250, loss: 0.81239, final_score: 0.97120, time: 1447.80092\n",
      "Train Step 1275, loss: 0.81170, final_score: 0.97145, time: 1474.00903\n",
      "Train Step 1300, loss: 0.81079, final_score: 0.97191, time: 1500.07559\n",
      "Train Step 1325, loss: 0.80990, final_score: 0.97227, time: 1526.25362\n",
      "Train Step 1350, loss: 0.80987, final_score: 0.97231, time: 1552.16280\n",
      "Train Step 1375, loss: 0.80905, final_score: 0.97274, time: 1578.33422\n",
      "Train Step 1400, loss: 0.80893, final_score: 0.97292, time: 1604.32399\n",
      "Train Step 1425, loss: 0.80849, final_score: 0.97318, time: 1630.61756\n",
      "Train Step 1450, loss: 0.80821, final_score: 0.97342, time: 1656.79693\n",
      "Train Step 1475, loss: 0.80769, final_score: 0.97353, time: 1682.73507\n",
      "Train Step 1500, loss: 0.80708, final_score: 0.97383, time: 1708.64097\n",
      "Train Step 1525, loss: 0.80671, final_score: 0.97410, time: 1734.47258\n",
      "Train Step 1550, loss: 0.80612, final_score: 0.97448, time: 1760.60930\n",
      "Train Step 1575, loss: 0.80565, final_score: 0.97461, time: 1786.67120\n",
      "Train Step 1600, loss: 0.80508, final_score: 0.97488, time: 1812.83401\n",
      "Train Step 1625, loss: 0.80509, final_score: 0.97481, time: 1839.01322\n",
      "Train Step 1650, loss: 0.80516, final_score: 0.97483, time: 1865.06227\n",
      "Train Step 1675, loss: 0.80457, final_score: 0.97505, time: 1891.14885\n",
      "Train Step 1700, loss: 0.80405, final_score: 0.97526, time: 1917.29643\n",
      "Train Step 1725, loss: 0.80359, final_score: 0.97551, time: 1943.37428\n",
      "Train Step 1750, loss: 0.80317, final_score: 0.97564, time: 1969.55009\n",
      "Train Step 1775, loss: 0.80280, final_score: 0.97580, time: 1995.78043\n",
      "Train Step 1800, loss: 0.80255, final_score: 0.97598, time: 2021.95526\n",
      "Train Step 1825, loss: 0.80238, final_score: 0.97604, time: 2048.09904\n",
      "Train Step 1850, loss: 0.80182, final_score: 0.97633, time: 2074.29952\n",
      "Train Step 1875, loss: 0.80163, final_score: 0.97645, time: 2100.58297\n",
      "Train Step 1900, loss: 0.80149, final_score: 0.97658, time: 2126.90930\n",
      "Train Step 1925, loss: 0.80127, final_score: 0.97666, time: 2153.15548\n",
      "Train Step 1950, loss: 0.80112, final_score: 0.97667, time: 2179.69356\n",
      "Train Step 1975, loss: 0.80068, final_score: 0.97688, time: 2206.62309\n",
      "Train Step 2000, loss: 0.80035, final_score: 0.97704, time: 2232.84670\n",
      "Train Step 2025, loss: 0.80015, final_score: 0.97714, time: 2259.20042\n",
      "Train Step 2050, loss: 0.79963, final_score: 0.97736, time: 2285.51739\n",
      "Train Step 2075, loss: 0.79943, final_score: 0.97744, time: 2311.69063\n",
      "Train Step 2100, loss: 0.79909, final_score: 0.97750, time: 2337.93906\n",
      "Train Step 2125, loss: 0.79895, final_score: 0.97760, time: 2364.25298\n",
      "Train Step 2150, loss: 0.79852, final_score: 0.97774, time: 2390.54560\n",
      "Train Step 2175, loss: 0.79804, final_score: 0.97790, time: 2416.77153\n",
      "Train Step 2200, loss: 0.79778, final_score: 0.97807, time: 2442.99662\n",
      "Train Step 2225, loss: 0.79735, final_score: 0.97824, time: 2469.25733\n",
      "Train Step 2250, loss: 0.79676, final_score: 0.97842, time: 2495.55055\n",
      "Train Step 2275, loss: 0.79664, final_score: 0.97853, time: 2521.68802\n",
      "Train Step 2300, loss: 0.79649, final_score: 0.97865, time: 2547.98694\n",
      "[RESULT]: Train. Epoch: 0, loss: 0.79648, final_score: 0.97871, time: 2587.07327\n",
      "Valid Step 0, loss: 0.00000, final_score: 0.00000, time: 0.03872\n",
      "Valid Step 25, loss: 0.85141, final_score: 0.95961, time: 47.61987\n",
      "Valid Step 50, loss: 0.88922, final_score: 0.95440, time: 59.05258\n",
      "[RESULT]: Validation. Epoch: 0, loss: 0.89216, final_score: 0.95028, time: 98.39787\n",
      "\n",
      "2020-06-12T13:30:55.330406\n",
      "LR: 4e-05\n",
      "Train Step 0, loss: 0.00000, final_score: 0.00000, time: 0.91261\n",
      "Train Step 25, loss: 0.79312, final_score: 0.98196, time: 26.77250\n",
      "Train Step 50, loss: 0.79094, final_score: 0.98377, time: 52.72081\n",
      "Train Step 75, loss: 0.77891, final_score: 0.98630, time: 78.67451\n",
      "Train Step 100, loss: 0.78215, final_score: 0.98407, time: 104.66503\n",
      "Train Step 125, loss: 0.77856, final_score: 0.98553, time: 130.68294\n",
      "Train Step 150, loss: 0.78018, final_score: 0.98430, time: 156.72655\n",
      "Train Step 175, loss: 0.78196, final_score: 0.98425, time: 182.66447\n",
      "Train Step 200, loss: 0.78277, final_score: 0.98465, time: 208.77454\n",
      "Train Step 225, loss: 0.77948, final_score: 0.98566, time: 234.86229\n",
      "Train Step 250, loss: 0.77703, final_score: 0.98596, time: 260.98395\n",
      "Train Step 275, loss: 0.77597, final_score: 0.98633, time: 287.05261\n",
      "Train Step 300, loss: 0.77329, final_score: 0.98695, time: 313.11650\n",
      "Train Step 325, loss: 0.77329, final_score: 0.98694, time: 339.30046\n",
      "Train Step 350, loss: 0.77412, final_score: 0.98708, time: 365.39720\n",
      "Train Step 375, loss: 0.77316, final_score: 0.98729, time: 391.47293\n",
      "Train Step 400, loss: 0.77259, final_score: 0.98744, time: 417.59308\n",
      "Train Step 425, loss: 0.77265, final_score: 0.98764, time: 443.71452\n",
      "Train Step 450, loss: 0.77282, final_score: 0.98768, time: 469.84224\n",
      "Train Step 475, loss: 0.77303, final_score: 0.98756, time: 495.94159\n",
      "Train Step 500, loss: 0.77328, final_score: 0.98703, time: 522.21004\n",
      "Train Step 525, loss: 0.77323, final_score: 0.98697, time: 548.26927\n",
      "Train Step 550, loss: 0.77333, final_score: 0.98704, time: 574.42317\n",
      "Train Step 575, loss: 0.77338, final_score: 0.98695, time: 600.51664\n",
      "Train Step 600, loss: 0.77332, final_score: 0.98690, time: 626.64946\n",
      "Train Step 625, loss: 0.77421, final_score: 0.98670, time: 652.88072\n",
      "Train Step 650, loss: 0.77421, final_score: 0.98694, time: 678.93009\n",
      "Train Step 675, loss: 0.77568, final_score: 0.98618, time: 705.08912\n",
      "Train Step 700, loss: 0.77506, final_score: 0.98637, time: 731.21432\n",
      "Train Step 725, loss: 0.77474, final_score: 0.98655, time: 757.36390\n",
      "Train Step 750, loss: 0.77551, final_score: 0.98642, time: 783.54145\n",
      "Train Step 775, loss: 0.77556, final_score: 0.98648, time: 809.71079\n",
      "Train Step 800, loss: 0.77503, final_score: 0.98665, time: 835.91830\n",
      "Train Step 825, loss: 0.77449, final_score: 0.98679, time: 862.14303\n",
      "Train Step 850, loss: 0.77457, final_score: 0.98679, time: 888.19142\n",
      "Train Step 875, loss: 0.77451, final_score: 0.98687, time: 914.25996\n",
      "Train Step 900, loss: 0.77459, final_score: 0.98676, time: 940.53352\n",
      "Train Step 925, loss: 0.77463, final_score: 0.98679, time: 966.77038\n",
      "Train Step 950, loss: 0.77505, final_score: 0.98668, time: 993.00023\n",
      "Train Step 975, loss: 0.77609, final_score: 0.98649, time: 1019.25163\n",
      "Train Step 1000, loss: 0.77628, final_score: 0.98658, time: 1045.63482\n",
      "Train Step 1025, loss: 0.77613, final_score: 0.98664, time: 1071.64759\n",
      "Train Step 1050, loss: 0.77670, final_score: 0.98654, time: 1097.90355\n",
      "Train Step 1075, loss: 0.77723, final_score: 0.98643, time: 1124.15026\n",
      "Train Step 1100, loss: 0.77743, final_score: 0.98637, time: 1150.35872\n",
      "Train Step 1125, loss: 0.77776, final_score: 0.98625, time: 1176.60826\n",
      "Train Step 1150, loss: 0.77786, final_score: 0.98627, time: 1202.92666\n",
      "Train Step 1175, loss: 0.77754, final_score: 0.98633, time: 1229.22659\n",
      "Train Step 1200, loss: 0.77685, final_score: 0.98641, time: 1255.56952\n",
      "Train Step 1225, loss: 0.77699, final_score: 0.98643, time: 1281.81924\n",
      "Train Step 1250, loss: 0.77691, final_score: 0.98634, time: 1308.06797\n",
      "Train Step 1275, loss: 0.77668, final_score: 0.98641, time: 1334.25358\n",
      "Train Step 1300, loss: 0.77684, final_score: 0.98621, time: 1360.43261\n",
      "Train Step 1325, loss: 0.77635, final_score: 0.98633, time: 1386.68417\n",
      "Train Step 1350, loss: 0.77618, final_score: 0.98635, time: 1412.86410\n",
      "Train Step 1375, loss: 0.77623, final_score: 0.98633, time: 1439.09559\n",
      "Train Step 1400, loss: 0.77600, final_score: 0.98636, time: 1465.32728\n",
      "Train Step 1425, loss: 0.77604, final_score: 0.98634, time: 1491.48573\n",
      "Train Step 1450, loss: 0.77605, final_score: 0.98629, time: 1517.74118\n",
      "Train Step 1475, loss: 0.77588, final_score: 0.98645, time: 1544.29171\n",
      "Train Step 1500, loss: 0.77622, final_score: 0.98638, time: 1570.50605\n",
      "Train Step 1525, loss: 0.77622, final_score: 0.98647, time: 1596.81396\n",
      "Train Step 1550, loss: 0.77618, final_score: 0.98645, time: 1623.06827\n",
      "Train Step 1575, loss: 0.77617, final_score: 0.98650, time: 1649.38955\n",
      "Train Step 1600, loss: 0.77614, final_score: 0.98649, time: 1675.70444\n",
      "Train Step 1625, loss: 0.77597, final_score: 0.98652, time: 1701.86262\n",
      "Train Step 1650, loss: 0.77615, final_score: 0.98652, time: 1728.19692\n",
      "Train Step 1675, loss: 0.77613, final_score: 0.98657, time: 1754.61537\n",
      "Train Step 1700, loss: 0.77633, final_score: 0.98660, time: 1780.92309\n",
      "Train Step 1725, loss: 0.77629, final_score: 0.98663, time: 1807.18535\n",
      "Train Step 1750, loss: 0.77609, final_score: 0.98671, time: 1833.44604\n",
      "Train Step 1775, loss: 0.77573, final_score: 0.98683, time: 1859.60453\n",
      "Train Step 1800, loss: 0.77566, final_score: 0.98689, time: 1885.71118\n",
      "Train Step 1825, loss: 0.77549, final_score: 0.98691, time: 1912.30956\n",
      "Train Step 1850, loss: 0.77557, final_score: 0.98694, time: 1938.55417\n",
      "Train Step 1875, loss: 0.77541, final_score: 0.98698, time: 1964.80233\n",
      "Train Step 1900, loss: 0.77551, final_score: 0.98694, time: 1991.25359\n",
      "Train Step 1925, loss: 0.77534, final_score: 0.98700, time: 2017.46169\n",
      "Train Step 1950, loss: 0.77510, final_score: 0.98708, time: 2043.80217\n",
      "Train Step 1975, loss: 0.77529, final_score: 0.98706, time: 2070.29844\n",
      "Train Step 2000, loss: 0.77517, final_score: 0.98713, time: 2096.70977\n",
      "Train Step 2025, loss: 0.77532, final_score: 0.98713, time: 2122.85761\n",
      "Train Step 2050, loss: 0.77538, final_score: 0.98708, time: 2149.27303\n",
      "Train Step 2075, loss: 0.77522, final_score: 0.98715, time: 2175.68625\n",
      "Train Step 2100, loss: 0.77496, final_score: 0.98724, time: 2202.01682\n",
      "Train Step 2125, loss: 0.77494, final_score: 0.98724, time: 2228.40059\n",
      "Train Step 2150, loss: 0.77497, final_score: 0.98725, time: 2254.73486\n",
      "Train Step 2175, loss: 0.77474, final_score: 0.98726, time: 2281.20231\n",
      "Train Step 2200, loss: 0.77475, final_score: 0.98729, time: 2307.51416\n",
      "Train Step 2225, loss: 0.77491, final_score: 0.98720, time: 2333.89852\n",
      "Train Step 2250, loss: 0.77515, final_score: 0.98713, time: 2360.34441\n",
      "Train Step 2275, loss: 0.77541, final_score: 0.98712, time: 2386.68244\n",
      "Train Step 2300, loss: 0.77536, final_score: 0.98715, time: 2412.98582\n",
      "[RESULT]: Train. Epoch: 1, loss: 0.77541, final_score: 0.98713, time: 2455.28917\n",
      "Valid Step 0, loss: 0.00000, final_score: 0.00000, time: 0.03997\n",
      "Valid Step 25, loss: 0.88266, final_score: 0.95961, time: 11.39989\n",
      "Valid Step 50, loss: 0.91594, final_score: 0.95280, time: 22.81791\n",
      "[RESULT]: Validation. Epoch: 1, loss: 0.91625, final_score: 0.94947, time: 28.61930\n",
      "\n",
      "2020-06-12T14:12:19.276497\n",
      "LR: 2.8e-05\n",
      "Train Step 0, loss: 0.00000, final_score: 0.00000, time: 0.73526\n",
      "Train Step 25, loss: 0.75766, final_score: 0.98885, time: 26.74259\n",
      "Train Step 50, loss: 0.75711, final_score: 0.99217, time: 52.92362\n",
      "Train Step 75, loss: 0.75922, final_score: 0.99243, time: 79.08466\n",
      "Train Step 100, loss: 0.76430, final_score: 0.99136, time: 105.23061\n",
      "Train Step 125, loss: 0.76600, final_score: 0.99127, time: 131.33590\n",
      "Train Step 150, loss: 0.76438, final_score: 0.99123, time: 157.48214\n",
      "Train Step 175, loss: 0.76667, final_score: 0.99059, time: 183.53726\n",
      "Train Step 200, loss: 0.76701, final_score: 0.99081, time: 209.47047\n",
      "Train Step 225, loss: 0.76698, final_score: 0.99031, time: 235.60503\n",
      "Train Step 250, loss: 0.76895, final_score: 0.99039, time: 261.75503\n",
      "Train Step 275, loss: 0.76902, final_score: 0.99009, time: 288.06130\n",
      "Train Step 300, loss: 0.76861, final_score: 0.99001, time: 314.29424\n",
      "Train Step 325, loss: 0.76930, final_score: 0.98977, time: 340.43234\n",
      "Train Step 350, loss: 0.76911, final_score: 0.99004, time: 366.47468\n",
      "Train Step 375, loss: 0.76785, final_score: 0.99021, time: 392.59861\n",
      "Train Step 400, loss: 0.76635, final_score: 0.99060, time: 418.74748\n",
      "Train Step 425, loss: 0.76588, final_score: 0.99060, time: 444.94954\n",
      "Train Step 450, loss: 0.76646, final_score: 0.99034, time: 471.26470\n",
      "Train Step 475, loss: 0.76646, final_score: 0.99038, time: 497.22457\n",
      "Train Step 500, loss: 0.76687, final_score: 0.99046, time: 523.28770\n",
      "Train Step 525, loss: 0.76731, final_score: 0.99024, time: 549.52718\n",
      "Train Step 550, loss: 0.76712, final_score: 0.99011, time: 575.73269\n"
     ]
    }
   ],
   "source": [
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n",
    "from datetime import date; today = date.today(); output_model_file='bert_tpu_trained.bin'\n",
    "torch.save(net.state_dict(), f\"{today}_{output_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wu0VhhZAFuYs"
   },
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.read_csv(path) for path in glob('node_submissions/*.csv')]).groupby('id').mean()\n",
    "submission['toxic'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRr-yzJ_yVTW"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(f'{ROOT_PATH}/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARz9TllfyVVa"
   },
   "outputs": [],
   "source": [
    "# !cp log.txt '/content/drive/My Drive/jigsaw2020-kaggle-public-baseline/'\n",
    "!make push_dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "shonenkov-training-pipeline.ipynb",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "id,outputId,colab,colab_type,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "127e21bdc5564f8a80fea880bd3fb6a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ceacfea884f846318a5faace601ad6e4",
      "max": 2244861551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3376980c1c6b4e80a428b4de52973b23",
      "value": 2244861551
     }
    },
    "2f4c53d7ee04454691e03eadf812aba6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3376980c1c6b4e80a428b4de52973b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4d7b1aa872c9412f820ae6766535da35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_127e21bdc5564f8a80fea880bd3fb6a3",
       "IPY_MODEL_d77f6ad0f4bd41ca8f740525c3d12472"
      ],
      "layout": "IPY_MODEL_c11a75f46eb64654aa85dc0ab587a844"
     }
    },
    "6d1b0ad8f1e644129f0d6b12be860cc8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76fd170ba561430695446be0277ca457": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7bfc594b182f476f9ddeacb8899702a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a14d764f085c4732ab5bce311416df3a",
       "IPY_MODEL_e00957c72ced4c4399f32af26d97e62a"
      ],
      "layout": "IPY_MODEL_f12672a313e24b348c179539b6eb11be"
     }
    },
    "a14d764f085c4732ab5bce311416df3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d1b0ad8f1e644129f0d6b12be860cc8",
      "max": 513,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76fd170ba561430695446be0277ca457",
      "value": 513
     }
    },
    "a597bc54742246678df95bbf1795248c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6349d00ff974d008a3648e16208c31e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c11a75f46eb64654aa85dc0ab587a844": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caa85e96850a4699952ebe2115d5411a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f862b770d9d4b2fa6f94ce5f2ede807",
       "IPY_MODEL_5e6b155a086b4f618d5b309b65c1768b"
      ],
      "layout": "IPY_MODEL_112886c20ae549908a7426a0fc3729e0"
     }
    },
    "ceacfea884f846318a5faace601ad6e4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d77f6ad0f4bd41ca8f740525c3d12472": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f4c53d7ee04454691e03eadf812aba6",
      "placeholder": "​",
      "style": "IPY_MODEL_b6349d00ff974d008a3648e16208c31e",
      "value": " 2.24G/2.24G [00:32&lt;00:00, 68.4MB/s]"
     }
    },
    "d9cb3e58951e4d82974688fbe9046d37": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e00957c72ced4c4399f32af26d97e62a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9cb3e58951e4d82974688fbe9046d37",
      "placeholder": "​",
      "style": "IPY_MODEL_a597bc54742246678df95bbf1795248c",
      "value": " 513/513 [00:33&lt;00:00, 15.4B/s]"
     }
    },
    "f12672a313e24b348c179539b6eb11be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
