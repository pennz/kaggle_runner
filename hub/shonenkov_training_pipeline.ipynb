{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWF5pHpO3bEX"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "f7_Bllh93bEt",
    "lines_to_next_cell": 2,
    "outputId": "d21d7121-3dfa-423a-8323-3e0a08da3406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kaggle-runner\n",
      "Version: 0.1.6\n",
      "Summary: Run kaggle kernels, for fast model prototyping.\n",
      "Home-page: http://github.com/pennz/kaggle_runner\n",
      "Author: pennz\n",
      "Author-email: pengyuzhou.work@gmail.com\n",
      "License: MIT\n",
      "Location: /content\n",
      "Requires: slug, parse, python-logging-rabbitmq, kaggle\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip3 show kaggle_runner || ( git clone https://github.com/pennz/kaggle_runner; \\\n",
    "mv kaggle_runner k && \\\n",
    "mv k/* . && mv k/.* ; \\\n",
    "pip3 install -e .;\\\n",
    "export PATH=$PWD/bin:$PATH; \\\n",
    "entry.sh &)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mg3zuCSx3bE9",
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make[2]: Entering directory '/content/hub'\n",
      "make[2]: *** No rule to make target 'xla'.  Stop.\n",
      "make[2]: Leaving directory '/content/hub'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'kaggle_runner' from '/content/kaggle_runner/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!make xla\n",
    "import torch_xla\n",
    "from importlib import reload\n",
    "import kaggle_runner\n",
    "reload(kaggle_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9Wgilnm3bFE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['XLA_USE_BF16'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecCODkEU3bFK",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBw5JHOK3bFR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8HpmDLV3bFX"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63n9I5s03bFc"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from fastai.text.transform import Vocab\n",
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_bxIOBr3bFf"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "PQAFCOlu3bFl",
    "outputId": "fed638fe-aaf5-432c-9bd8-12605cbd8052"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !python3 -m pip install nltk > /dev/null\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tz4yfcsg3bFq"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krJjgsvu3bFw"
   },
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-dI4yCqa3bF1",
    "outputId": "f5285cdc-22ca-4a41-a09c-b8c6fb1979e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(nb_workers=4, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "Ja1ioFcG3bF7",
    "lines_to_next_cell": 2,
    "outputId": "d0831d82-0bc9-4027-e02e-0ec6fba4f717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__call__', <function LevelMapper.__call__ at 0x7fdd99a86510>), ('__init__', <function LevelMapper.__init__ at 0x7fdd99a86488>)]\n",
      "[('__call__', <function BalancedPositiveNegativeSampler.__call__ at 0x7fdd999b7ea0>), ('__init__', <function BalancedPositiveNegativeSampler.__init__ at 0x7fdd999b7e18>)]\n",
      "[('__init__', <function BoxCoder.__init__ at 0x7fdd99951400>), ('decode', <function BoxCoder.decode at 0x7fdd99951598>), ('decode_single', <function BoxCoder.decode_single at 0x7fdd99951620>), ('encode', <function BoxCoder.encode at 0x7fdd99951488>), ('encode_single', <function BoxCoder.encode_single at 0x7fdd99951510>)]\n",
      "[('__call__', <function Matcher.__call__ at 0x7fdd99951378>), ('__init__', <function Matcher.__init__ at 0x7fdd999512f0>), ('set_low_quality_matches_', <function Matcher.set_low_quality_matches_ at 0x7fdd999511e0>)]\n",
      "[('__init__', <function ImageList.__init__ at 0x7fdd999519d8>), ('to', <function ImageList.to at 0x7fdd99951a60>)]\n",
      "[('__init__', <function Timebase.__init__ at 0x7fdd9987e620>)]\n",
      "[('__init__', <function VideoMetaData.__init__ at 0x7fdd9987e7b8>)]\n"
     ]
    }
   ],
   "source": [
    "from fastai.basic_data import DataBunch\n",
    "from kaggle_runner.kernels.fastai_kernel import FastAIKernel\n",
    "from kaggle_runner import may_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErWqUgQH3bGA"
   },
   "outputs": [],
   "source": [
    "SEED = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rl2PW6iO3bGF"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 224\n",
    "BACKBONE_PATH = 'xlm-roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94IiMvCD3bGJ"
   },
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(BACKBONE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ya6Mxv0G3bGO"
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = f'/kaggle' # for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sA1Da3DB3bGQ"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner.utils.kernel_utils import get_obj_or_dump\n",
    "def get_pickled_data(file_path):\n",
    "    obj = get_obj_or_dump(file_path)\n",
    "\n",
    "    if obj is None:\n",
    "        #may_debug(True)\n",
    "\n",
    "        return get_obj_or_dump(f\"{ROOT_PATH}/input/clean-pickle-for-jigsaw-toxicity/{file_path}\")\n",
    "\n",
    "    return obj\n",
    "vocab = get_pickled_data(\"vocab.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPoNUuvx3bGU",
    "lines_to_next_cell": 2
   },
   "source": [
    "if vocab is None: # vocab file read~~\n",
    "   vocab = [tokenizer.convert_ids_to_tokens(i) for i in range(tokenizer.vocab_size)]\n",
    "   get_obj_or_dump(\"vocab.pkl\", default=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAeLvflH3bGV",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "Nej3KhiY3bGZ",
    "lines_to_next_cell": 2,
    "outputId": "771fe8cf-7736-4f26-fa57-607db4bff17a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from random import shuffle\n",
    "import random\n",
    "import albumentations\n",
    "from albumentations.core.transforms_interface import DualTransform, BasicTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLdFogcG3bGe",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "LANGS = {\n",
    "    'en': 'english',\n",
    "    'it': 'italian',\n",
    "    'fr': 'french',\n",
    "    'es': 'spanish',\n",
    "    'tr': 'turkish',\n",
    "    'ru': 'russian',\n",
    "    'pt': 'portuguese'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lcd0sZJ3bGj",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_sentences(text, lang='en'):\n",
    "    return sent_tokenize(text, LANGS.get(lang, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMHg6Xvz3bGn",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def exclude_duplicate_sentences(text, lang='en'):\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in get_sentences(text, lang):\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        if sentence not in sentences:\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EizVrxLZ3bGr"
   },
   "outputs": [],
   "source": [
    "def clean_text(text, lang='en'):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[0-9\"]', '', text)\n",
    "    text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = exclude_duplicate_sentences(text, lang)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffbjaiBH3bGu",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class NLPTransform(BasicTransform):\n",
    "    \"\"\" Transform for nlp task.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return {\"data\": self.apply}\n",
    "\n",
    "    def update_params(self, params, **kwargs):\n",
    "        if hasattr(self, \"interpolation\"):\n",
    "            params[\"interpolation\"] = self.interpolation\n",
    "\n",
    "        if hasattr(self, \"fill_value\"):\n",
    "            params[\"fill_value\"] = self.fill_value\n",
    "\n",
    "        return params\n",
    "\n",
    "    def get_sentences(self, text, lang='en'):\n",
    "        return sent_tokenize(text, LANGS.get(lang, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yi6VEAZB3bGy",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ShuffleSentencesTransform(NLPTransform):\n",
    "    \"\"\" Do shuffle by sentence \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = self.get_sentences(text, lang)\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        return ' '.join(sentences), lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3b2yIDOv3bG1",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeDuplicateSentencesTransform(NLPTransform):\n",
    "    \"\"\" Exclude equal sentences \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = []\n",
    "\n",
    "        for sentence in self.get_sentences(text, lang):\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if sentence not in sentences:\n",
    "                sentences.append(sentence)\n",
    "\n",
    "        return ' '.join(sentences), lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kyDSPoUF3bG4",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeNumbersTransform(NLPTransform):\n",
    "    \"\"\" exclude any numbers \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAbXjyV63bG8",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeHashtagsTransform(NLPTransform):\n",
    "    \"\"\" Exclude any hashtags with # \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQHHd_wo3bG_",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeUsersMentionedTransform(NLPTransform):\n",
    "    \"\"\" Exclude @users \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sv-ecgw3bHC",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeUrlsTransform(NLPTransform):\n",
    "    \"\"\" Exclude urls \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'https?\\S+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YSO5OEp3bHF"
   },
   "outputs": [],
   "source": [
    "def get_open_subtitles():\n",
    "    df_ot = get_pickled_data(\"ot.pkl\")\n",
    "\n",
    "    if df_ot is None:\n",
    "        df_ot = pd.read_csv(f'{ROOT_PATH}/input/open-subtitles-toxic-pseudo-labeling/open-subtitles-synthesic.csv', index_col='id')[['comment_text', 'toxic', 'lang']]\n",
    "        df_ot = df_ot[~df_ot['comment_text'].isna()]\n",
    "        df_ot['comment_text'] = df_ot.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "        df_ot = df_ot.drop_duplicates(subset='comment_text')\n",
    "        df_ot['toxic'] = df_ot['toxic'].round().astype(np.int)\n",
    "        get_obj_or_dump(\"ot.pkl\", default=df_ot)\n",
    "\n",
    "    return df_ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Oyryrcx3bHI",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SynthesicOpenSubtitlesTransform(NLPTransform):\n",
    "    def __init__(self, always_apply=False, supliment_toxic=None, p=0.5, mix=False):\n",
    "        super(SynthesicOpenSubtitlesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "        df = get_open_subtitles()\n",
    "        self.synthesic_toxic = df[df['toxic'] == 1].comment_text.values\n",
    "        self.synthesic_non_toxic = df[df['toxic'] == 0].comment_text.values\n",
    "\n",
    "        if supliment_toxic is not None:\n",
    "            self.synthesic_toxic = np.concatenate((self.synthesic_toxic, supliment_toxic))\n",
    "        self.mix = mix\n",
    "\n",
    "        del df\n",
    "        gc.collect();\n",
    "\n",
    "\n",
    "    def _mix_both(self, texts):\n",
    "        for i in range(random.randint(0,2)):\n",
    "            texts.append(random.choice(self.synthesic_non_toxic))\n",
    "\n",
    "        for i in range(random.randint(1,3)):\n",
    "            texts.append(random.choice(self.synthesic_toxic))\n",
    "\n",
    "    def generate_synthesic_sample(self, text, toxic):\n",
    "        texts = [text]\n",
    "\n",
    "        if toxic == 0:\n",
    "            if self.mix:\n",
    "                self._mix_both(texts)\n",
    "                toxic = 1\n",
    "            else:\n",
    "                for i in range(random.randint(1,5)):\n",
    "                    texts.append(random.choice(self.synthesic_non_toxic))\n",
    "        else:\n",
    "            self._mix_both(texts)\n",
    "        random.shuffle(texts)\n",
    "\n",
    "        return ' '.join(texts), toxic\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, toxic = data\n",
    "        text, toxic = self.generate_synthesic_sample(text, toxic)\n",
    "\n",
    "        return text, toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IY_VsMfk3bHL",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return albumentations.Compose([\n",
    "        ExcludeUsersMentionedTransform(p=0.95),\n",
    "        ExcludeUrlsTransform(p=0.95),\n",
    "        ExcludeNumbersTransform(p=0.95),\n",
    "        ExcludeHashtagsTransform(p=0.95),\n",
    "        ExcludeDuplicateSentencesTransform(p=0.95),\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4bTYrA1l3bHR",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_synthesic_transforms(supliment_toxic, p=0.5, mix=False):\n",
    "    return SynthesicOpenSubtitlesTransform(p=p, supliment_toxic=supliment_toxic, mix=mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjmhIsAK3bHU",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_toxic_comments(df):\n",
    "        df = df[~df['comment_text'].isna()]\n",
    "        df = df.drop_duplicates(subset='comment_text')\n",
    "        df['toxic'] = df['toxic'].round().astype(np.int)\n",
    "\n",
    "        return df[df['toxic'] == 1].comment_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nib4YbrO3bHX",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def onehot(size, target, aux=None):\n",
    "    if aux is not None:\n",
    "        vec = np.zeros(size+len(aux), dtype=np.float32)\n",
    "        vec[target] = 1.\n",
    "        vec[2:] = aux\n",
    "        vec = torch.tensor(vec, dtype=torch.float32)\n",
    "    else:\n",
    "        vec = torch.zeros(size, dtype=torch.float32)\n",
    "        vec[target] = 1.\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PsNlLK4G3bHZ"
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, labels_or_ids, comment_texts, langs,\n",
    "                 severe_toxic=None, obscene=None, threat=None, insult=None, identity_hate=None,\n",
    "                 use_train_transforms=False, test=False, use_aux=True, transformers=None):\n",
    "        self.test = test\n",
    "        self.labels_or_ids = labels_or_ids\n",
    "        self.comment_texts = comment_texts\n",
    "        self.langs = langs\n",
    "        self.severe_toxic = severe_toxic\n",
    "        self.obscene = obscene\n",
    "        self.threat = threat\n",
    "        self.insult = insult\n",
    "        self.identity_hate = identity_hate\n",
    "        self.use_train_transforms = use_train_transforms\n",
    "        self.aux = None\n",
    "        assert transformers is not None\n",
    "        self.transformers = transformers\n",
    "        self.vocab = vocab\n",
    "\n",
    "        if use_aux:\n",
    "            self.aux = [self.severe_toxic, self.obscene, self.threat, self.insult, self.identity_hate]\n",
    "\n",
    "    def get_tokens(self, text):\n",
    "        encoded = self.transformers['tokenizer'].encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "\n",
    "        return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.comment_texts.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.comment_texts[idx]\n",
    "        lang = self.langs[idx]\n",
    "\n",
    "        if self.severe_toxic is None:\n",
    "            aux = [0., 0., 0., 0., 0.]\n",
    "        else:\n",
    "            aux = [self.severe_toxic[idx], self.obscene[idx], self.threat[idx], self.insult[idx], self.identity_hate[idx]]\n",
    "\n",
    "\n",
    "        label = self.labels_or_ids[idx]\n",
    "\n",
    "        if self.use_train_transforms and (not self.test):\n",
    "            text, _ = self.transformers['train_transforms'](data=(text, lang))['data']\n",
    "            tokens, attention_mask = self.get_tokens(str(text))\n",
    "            token_length = sum(attention_mask)\n",
    "\n",
    "            if token_length > 0.8*MAX_LENGTH:\n",
    "                text, _ = self.transformers['shuffle_transforms'](data=(text, lang))['data']\n",
    "            elif token_length < 60:\n",
    "                text, label = self.transformers['synthesic_transforms_often'](data=(text, label))['data']\n",
    "            else: # will not need to use transforms\n",
    "                #text, label = synthesic_transforms_low(data=(text, label))['data']\n",
    "                pass\n",
    "\n",
    "        # TODO add language detection and shuffle\n",
    "        # https://pypi.org/project/langdetect/\n",
    "        # if self.use_train_transforms and self.test:\n",
    "        #    text, _ = train_transforms(data=(text, lang))['data']\n",
    "        #    tokens, attention_mask = self.get_tokens(str(text))\n",
    "        #    token_length = sum(attention_mask)\n",
    "\n",
    "        #    if token_length > 0.8*MAX_LENGTH:\n",
    "        #        text, _ = shuffle_transforms(data=(text, lang))['data']\n",
    "        # to tensors\n",
    "        tokens, attention_mask = self.get_tokens(str(text))\n",
    "        tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n",
    "\n",
    "        if self.test:  # for test, return id TODO TTA\n",
    "            return [tokens, attention_mask], self.labels_or_ids[idx]\n",
    "\n",
    "        # label might be changed\n",
    "        target = onehot(2, label, aux=aux)\n",
    "\n",
    "        return [tokens, attention_mask], target\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(np.char.add(self.labels_or_ids.astype(str), self.langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swiDMY2l3bHb"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner.kernels.fastai_kernel import FastAIKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrGZycwx3bHd"
   },
   "outputs": [],
   "source": [
    "class Shonenkov(FastAIKernel):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Shonenkov, self).__init__(**kargs)\n",
    "        self.data = None\n",
    "        self.transformers = None\n",
    "        self.setup_transformers()\n",
    "\n",
    "    def build_and_set_model(self):\n",
    "        self.model = ToxicSimpleNNModel()\n",
    "\n",
    "    def set_random_seed(self):\n",
    "        seed_everything(SEED)\n",
    "\n",
    "    def setup_transformers(self):\n",
    "        if self.transformers is None:\n",
    "            supliment_toxic = None # avoid overfit\n",
    "            train_transforms = get_train_transforms();\n",
    "            synthesic_transforms_often = get_synthesic_transforms(supliment_toxic, p=0.5)\n",
    "            synthesic_transforms_low = None\n",
    "            #tokenizer = tokenizer\n",
    "            shuffle_transforms = ShuffleSentencesTransform(always_apply=True)\n",
    "\n",
    "            self.transformers = {'train_transforms': train_transforms,\n",
    "                                 'synthesic_transforms_often': synthesic_transforms_often,\n",
    "                                 'synthesic_transforms_low': synthesic_transforms_low,\n",
    "                                 'tokenizer': tokenizer, 'shuffle_transforms':\n",
    "                                 shuffle_transforms}\n",
    "\n",
    "    def prepare_train_dev_data(self):\n",
    "        df_train = get_pickled_data(\"train.pkl\")\n",
    "\n",
    "        if df_train is None:\n",
    "            df_train = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-toxicity-train-data-with-aux/train_data.csv')\n",
    "            df_train['comment_text'] = df_train.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "            get_obj_or_dump(\"train.pkl\", default=df_train)\n",
    "\n",
    "        #supliment_toxic = get_toxic_comments(df_train)\n",
    "        self.train_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_train['toxic'].values,\n",
    "            comment_texts=df_train['comment_text'].values,\n",
    "            langs=df_train['lang'].values,\n",
    "            severe_toxic=df_train['severe_toxic'].values,\n",
    "            obscene=df_train['obscene'].values,\n",
    "            threat=df_train['threat'].values,\n",
    "            insult=df_train['insult'].values,\n",
    "            identity_hate=df_train['identity_hate'].values,\n",
    "            use_train_transforms=True,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "        df_val = get_pickled_data(\"val.pkl\")\n",
    "\n",
    "        if df_val is None:\n",
    "            df_val = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/validation.csv', index_col='id')\n",
    "            df_val['comment_text'] = df_val.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "            get_obj_or_dump(\"val.pkl\", default=df_val)\n",
    "\n",
    "        self.validation_tune_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_val['toxic'].values,\n",
    "            comment_texts=df_val['comment_text'].values,\n",
    "            langs=df_val['lang'].values,\n",
    "            use_train_transforms=True,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "        self.validation_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_val['toxic'].values,\n",
    "            comment_texts=df_val['comment_text'].values,\n",
    "            langs=df_val['lang'].values,\n",
    "            use_train_transforms=False,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "\n",
    "        del df_val\n",
    "#del df_val_unclean\n",
    "        gc.collect();\n",
    "\n",
    "        del df_train\n",
    "        gc.collect();\n",
    "\n",
    "    def prepare_test_data(self):\n",
    "        df_test = get_pickled_data(\"test.pkl\")\n",
    "\n",
    "        if df_test is None:\n",
    "            df_test = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/test.csv', index_col='id')\n",
    "            df_test['comment_text'] = df_test.parallel_apply(lambda x: clean_text(x['content'], x['lang']), axis=1)\n",
    "            get_obj_or_dump(\"test.pkl\", default=df_test)\n",
    "\n",
    "        self.test_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_test.index.values, ## here different!!!\n",
    "            comment_texts=df_test['comment_text'].values,\n",
    "            langs=df_test['lang'].values,\n",
    "            use_train_transforms=False,\n",
    "            test=True,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "\n",
    "        del df_test\n",
    "        gc.collect();\n",
    "    def after_prepare_data_hook(self):\n",
    "        \"\"\"Put to databunch here\"\"\"\n",
    "        self.data = DataBunch.create(self.train_dataset,\n",
    "                                     self.validation_dataset,\n",
    "                                     bs=TrainGlobalConfig.batch_size,\n",
    "                                     num_workers=TrainGlobalConfig.num_workers)\n",
    "\n",
    "    def peek_data(self):\n",
    "        if self.data is not None:\n",
    "            may_debug()\n",
    "            o = self.data.one_batch()\n",
    "            print(o)\n",
    "\n",
    "            return o\n",
    "        else:\n",
    "            if self.logger is not None:\n",
    "                self.logger.error(\"peek_data failed, DataBunch is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0U0v_7EA3bHg",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from kaggle_runner.metrics.metrics import matthews_correlation\n",
    "class RocAucMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = np.array([])\n",
    "        self.y_true_float = np.array([], dtype=np.float)\n",
    "        self.y_pred = np.array([])\n",
    "        self.score = 0\n",
    "        self.mc_score = 0\n",
    "        self.aux_part = 0\n",
    "\n",
    "    def update(self, y_true, y_pred, aux_part=0):\n",
    "        #y_true_ = y_true\n",
    "        y_true = y_true[:,:2].cpu().numpy().argmax(axis=1)\n",
    "        y_true_float = y_true.astype(np.float)\n",
    "        y_pred = nn.functional.softmax(y_pred[:,:2], dim=1).data.cpu().numpy()[:,1]\n",
    "        self.y_true = np.hstack((self.y_true, y_true))\n",
    "        self.y_true_float = np.hstack((self.y_true_float, y_true_float))\n",
    "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
    "        try:\n",
    "            self.score = sklearn.metrics.roc_auc_score(self.y_true, self.y_pred, labels=np.array([0, 1]))\n",
    "        except Exception:\n",
    "            self.score = 0\n",
    "        self.mc_score = matthews_correlation(self.y_true_float, self.y_pred)\n",
    "        self.aux_part = aux_part\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.score\n",
    "    @property\n",
    "    def mc_avg(self):\n",
    "        return self.mc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sj4NmKFm3bHj"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "meOpcCrs3bHl",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcatLT2U3bHn",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ToxicSimpleNNModel(nn.Module):\n",
    "    def __init__(self, use_aux=True):\n",
    "        super(ToxicSimpleNNModel, self).__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(BACKBONE_PATH)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        aux_len = 0\n",
    "\n",
    "        if use_aux:\n",
    "            aux_len = 5\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.backbone.pooler.dense.out_features*2,\n",
    "            out_features=2+aux_len,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        bs, seq_length = input_ids.shape\n",
    "        seq_x, _ = self.backbone(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        apool = torch.mean(seq_x, 1)\n",
    "        mpool, _ = torch.max(seq_x, 1)\n",
    "        x = torch.cat((apool, mpool), 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2NkI1sW3bHs"
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imzd88o33bIB"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGfbDB5z3bID"
   },
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnDl9J_d3bIF",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbxKT4Td3bII",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TPUFitter:\n",
    "\n",
    "    def __init__(self, model, device, config):\n",
    "        if not os.path.exists('node_submissions'):\n",
    "            os.makedirs('node_submissions')\n",
    "\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        self.log_path = 'log.txt'\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr*xm.xrt_world_size())\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "\n",
    "        self.criterion = config.criterion\n",
    "        xm.master_print(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(train_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
    "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=final_scores.mc_avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def run_tuning_and_inference(self, test_loader, validation_tune_loader):\n",
    "        for e in range(2):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*xm.xrt_world_size()\n",
    "            para_loader = pl.ParallelLoader(validation_tune_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "            para_loader = pl.ParallelLoader(test_loader, [self.device])\n",
    "            self.run_inference(para_loader.per_device_loader(self.device))\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(val_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(train_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    logger.info(\n",
    "                        f'Train Step {step}, bs: {batch_size}, loss: ' + \\\n",
    "                        f\"{losses.avg:.5f}, lr: {self.optimizer.param_groups[0]['lr']} final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, \" + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(self.device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "            targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs, attention_masks)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            _check_grad(self.optimizer)\n",
    "            logger.info(\"step: %d, loss: %f\", step, loss)\n",
    "\n",
    "            xm.optimizer_step(self.optimizer)\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        self.model.eval()\n",
    "        self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_inference(self, test_loader):\n",
    "        self.model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, ids) in enumerate(test_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "        result = pd.DataFrame(result)\n",
    "        node_count = len(glob('node_submissions/*.csv'))\n",
    "        result.to_csv(f'node_submissions/submission_{node_count}_{datetime.utcnow().microsecond}_{random.random()}.csv', index=False)\n",
    "\n",
    "    def save(self, path):\n",
    "        xm.save(self.model.state_dict(), path)\n",
    "\n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            xm.master_print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            xm.master_print(f'{message}', logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRdMTDq13bIN",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing = 0.1, dim=-1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.cls = 2\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            pred = x[:,:2].log_softmax(dim=self.dim)\n",
    "            aux=x[:, 2:]\n",
    "\n",
    "            toxic_target = target[:,:2]\n",
    "            aux_target = target[:, 2:]\n",
    "            with torch.no_grad():\n",
    "                # smooth_toxic = pred.data.clone()\n",
    "                smooth_toxic = self.smoothing + (1-self.smoothing*2)*toxic_target\n",
    "                # smooth_toxic.scatter_(1, toxic_target.data.unsqueeze(1), self.confidence) # only for 0 1 label, put confidence to related place\n",
    "                # for 0-1, 0 -> 0.1, 1->0.9.(if 1), if zero. 0->0.9, 1->0.1\n",
    "                smooth_aux = self.smoothing + (1-self.smoothing*2)*aux_target  # only for binary cross entropy, so for lable, it is (1-smooth)*\n",
    "\n",
    "            aux_loss = torch.nn.functional.binary_cross_entropy_with_logits(aux, smooth_aux)\n",
    "\n",
    "            return torch.mean(torch.sum(-smooth_toxic * pred, dim=self.dim)) + aux_loss/3\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x[:,:2], target[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6KQPK1tG3bIO",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    \"\"\" Global Config for this notebook \"\"\"\n",
    "    num_workers = 0  #    loaders\n",
    "    batch_size = 16  # bs\n",
    "    n_epochs = 2  #    \n",
    "    lr = 0.5 * 1e-5 #  learning rate (     TPU   - )\n",
    "    fold_number = 0  #    \n",
    "\n",
    "    # -------------------\n",
    "    verbose = True  #  \n",
    "    verbose_step = 1  #     \n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  #  scheduler.step   optimizer.step\n",
    "    validation_scheduler = True  #  scheduler.step   loss (  )\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='max',\n",
    "        factor=0.7,\n",
    "        patience=0,\n",
    "        verbose=False,\n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0,\n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )\n",
    "    # --------------------\n",
    "\n",
    "    # -------------------\n",
    "    criterion = LabelSmoothing()\n",
    "    # -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jRWjZRd3bIQ"
   },
   "outputs": [],
   "source": [
    "def test_init():\n",
    "    l = Shonenkov(loss_func=None, metrics=None)\n",
    "    assert l is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQ86CF413bIS",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/clean-pickle-for-jigsaw-toxicity/*pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROFBN4h33bIV"
   },
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7bNG49v3bIZ"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner import may_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "fYMCn2Gt3bIb",
    "outputId": "221aec22-4e09-4f8a-e04f-641d2f36a1db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-19 13:08:00,717:utils:load ot.pkl\n",
      "[DEBUG]2020-06-19 13:08:02,201:utils:None -> KernelRunningState.SAVE_SUBMISSION_DONE\n",
      "[DEBUG]2020-06-19 13:08:02,203:utils:load train.pkl\n",
      "[DEBUG]2020-06-19 13:08:06,509:utils:load val.pkl\n",
      "[DEBUG]2020-06-19 13:08:06,845:utils:state KernelRunningState.PREPARE_DATA_DONE\n",
      "[DEBUG]2020-06-19 13:08:25,743:utils:state KernelRunningState.TRAINING_DONE\n",
      "[DEBUG]2020-06-19 13:08:25,745:utils:state KernelRunningState.EVL_DEV_DONE\n",
      "[DEBUG]2020-06-19 13:08:25,746:utils:load test.pkl\n",
      "[DEBUG]2020-06-19 13:08:26,415:utils:state KernelRunningState.SAVE_SUBMISSION_DONE\n"
     ]
    }
   ],
   "source": [
    "k = Shonenkov(metrics=None, loss_func=LabelSmoothing(), opt_func=None)\n",
    "k.run(dump_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0ienNqm3bId"
   },
   "outputs": [],
   "source": [
    "def _check_grad(raw_opt):\n",
    "    pg = raw_opt.param_groups\n",
    "    pg0pl = pg[0]['params'] # pg0pl[0] is a Parameter\n",
    "    pg1pl = pg[1]['params'] # pg0pl[0] is a Parameter\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    #    #norms = torch.tensor([torch.norm(p) for p in pg0pl])\n",
    "    #    normsg = torch.tensor([torch.norm(p.grad) for p in pg0pl if p.grad is not None])\n",
    "    #    #logger.debug(\"params info pg0: norm std(%f) mean(%f)\", *torch.std_mean(norms))\n",
    "    #    logger.debug(\"grad info pg0: norm std(%f) mean(%f)\", *torch.std_mean(normsg))\n",
    "\n",
    "    #    #norms1 = torch.tensor([torch.norm(p) for p in pg1pl])\n",
    "    #    norms1g = torch.tensor([torch.norm(p.grad) for p in pg1pl if p.grad is not None])\n",
    "    #    #logger.debug(\"params info pg1: norm std(%f) mean(%f)\", *torch.std_mean(norms1))\n",
    "    #    logger.debug(\"grad info pg1: norm std(%f) mean(%f)\", *torch.std_mean(norms1g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sul01z663bIf",
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from kaggle_runner import logger\n",
    "\n",
    "def test_model_fn(device=torch.device(\"cpu\")):\n",
    "    #device = xm.xla_device(devkind='TPU')\n",
    "    #device=torch.device(\"xla\")\n",
    "    logger.debug(\"Device used: %s\", device)\n",
    "\n",
    "    #k.run(dump_flag=True) # it seems it cannot save right\n",
    "    #k.run(dump_flag=False)\n",
    "    #k.peek_data()\n",
    "\n",
    "    self = k\n",
    "    assert self.validation_dataset is not None\n",
    "    #assert self.learner is not None\n",
    "\n",
    "    net = k.model\n",
    "    assert net is not None\n",
    "    net.to(device)\n",
    "\n",
    "    param_optimizer = list(self.model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    #optimizer = AdamW(optimizer_grouped_parameters, lr=TrainGlobalConfig.lr*xm.xrt_world_size())\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=TrainGlobalConfig.lr*8)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        self.train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        shuffle=False, # sampler is set, so shuffle here should be False\n",
    "        sampler=BalanceClassSampler(labels=k.train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    may_debug()\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        self.validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        self.test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    validation_tune_loader = torch.utils.data.DataLoader(\n",
    "        self.validation_tune_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        #sampler=validation_tune_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    def validation(model, device, config, val_loader, criterion):\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(val_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "    def run_inference(model, device, config, test_loader):\n",
    "        model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(test_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def train_one_epoch(model, device, config, train_loader, criterion, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(train_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.debug(\n",
    "                        f'Train Step {step}, bs: {batch_size}, loss: ' + \\\n",
    "                        f\"{losses.avg:.5f}, lr: {optimizer.param_groups[0]['lr']} final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, \" + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            _check_grad(optimizer)\n",
    "            #optimizer.step()\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "\n",
    "        model.eval()\n",
    "        #self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_tuning_and_inference(net, device, TrainGlobalConfig, validation_loader, train_loader):\n",
    "        for e in range(1):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*8\n",
    "\n",
    "            losses, final_scores = train_one_epoch(net, device, TrainGlobalConfig, train_loader, TrainGlobalConfig.criterion, )\n",
    "            self.log(f'[RESULT]: Tune_Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
    "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
    "            self.log(f'[RESULT]: Tune_Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "\n",
    "    train_one_epoch(net, device, TrainGlobalConfig, train_loader, TrainGlobalConfig.criterion, optimizer)\n",
    "    losses, final_scores = validation(net, device, TrainGlobalConfig, validation_loader, TrainGlobalConfig.criterion)\n",
    "    logger.info(f\"Val results: losses={losses}, final_scores={final_scores}\")\n",
    "\n",
    "    results = run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "    logger.info(f\"Test done, result len %d\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDtvtHma3bIh"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner import defaults\n",
    "_DEBUG = defaults.DEBUG\n",
    "defaults.DEBUG = True\n",
    "#test_model_fn()\n",
    "defaults.DEBUG = _DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kl5LAGTl3bIi"
   },
   "outputs": [],
   "source": [
    "#k.learner\n",
    "#k.learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-54VVqb3bIn",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch\n",
    "\n",
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.core import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.vision import *\n",
    "from fastai.basic_train import *\n",
    "from kaggle_runner import logger\n",
    "\n",
    "def len_parallelloader(self):\n",
    "    return len(self._loader._loader)\n",
    "pl.PerDeviceLoader.__len__ = len_parallelloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnvcfuzd3bIp"
   },
   "outputs": [],
   "source": [
    "import pysnooper\n",
    "class CheckGrad(LearnerCallback):\n",
    "    def __init__(self, learn:Learner, skip_loss_step=False):\n",
    "        super().__init__(learn)\n",
    "        self.skip_loss_step = skip_loss_step\n",
    "        logger.debug(\"Init Callback CheckGrad with skip_loss_step: \" +str(self.skip_loss_step))\n",
    "        self.losses = None\n",
    "        self.final_scores = None\n",
    "\n",
    "    def on_train_begin(self, **kwargs:Any)->None:\n",
    "        self.losses = AverageMeter()\n",
    "        self.final_scores = RocAucMeter()\n",
    "\n",
    "    def on_backward_begin(self, **kwargs:Any)->None:\n",
    "        #print(kwargs.keys())\n",
    "        \"\"\"dict_keys(['epoch', 'iteration', 'num_batch', 'skip_validate',\n",
    "        'n_epochs', 'pbar', 'metrics', 'stop_training', 'last_input',\n",
    "        'last_target', 'train', 'stop_epoch', 'skip_step', 'skip_zero',\n",
    "        'skip_bwd', 'last_output', 'last_loss', 'smooth_loss'])\n",
    "        \"\"\"\n",
    "        pg = self.learn.opt.opt.param_groups\n",
    "        #logger.debug(\"grad info: %s\", raw_opt)\n",
    "        logger.debug(f\"on_backward_begin lr: {pg[0]['lr']}\")\n",
    "        logger.debug(\"itr: %d, num_batch: %d, last loss: %f, smooth_loss: %f\",\n",
    "                     kwargs['iteration'], kwargs['num_batch'],\n",
    "                     kwargs['last_loss'], kwargs['smooth_loss'])\n",
    "\n",
    "        self.final_scores.update(kwargs['last_target'], kwargs['last_output'])\n",
    "        self.losses.update(kwargs['last_loss'].detach().item(), TrainGlobalConfig.batch_size)\n",
    "        logger.debug(f\"loss_avg: {self.losses.avg:.5f}, lr_pg0:\"\n",
    "                     f\"{pg[0]['lr']}, lr_pg1: {pg[1]['lr']}final_score:\"\n",
    "                     f\"{self.final_scores.avg:.5f}, mc_score:\"\n",
    "                     f\"{self.final_scores.mc_avg:.5f}\")\n",
    "\n",
    "    def on_backward_end(self, **kwargs:Any)->None:\n",
    "        raw_opt = self.learn.opt.opt\n",
    "        _check_grad(raw_opt)\n",
    "\n",
    "        return {'skip_step': self.skip_loss_step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnvcfuzd3bIp"
   },
   "outputs": [],
   "source": [
    "def to_device(b:Collection[Tensor],device:torch.device)->Collection[Tensor]:\n",
    "    \"Recursively map lists of tensors in `b ` to FP16.\"\n",
    "\n",
    "    return recurse(lambda x: x.to(device), b)\n",
    "\n",
    "def batch_to_device(b:Collection[Tensor],device:torch.device)->Collection[Tensor]:\n",
    "    \"Move the input of batch `b` to TPU.\"\n",
    "\n",
    "    return [to_device(b[0],device), to_device(b[1],device)]\n",
    "\n",
    "def _change_dl(dl, shuffle):\n",
    "    old_dl = dl\n",
    "    train_sampler = DistributedSamplerWrapper(\n",
    "        sampler=BalanceClassSampler(labels=k.train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        k.train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    new_dl = train_loader\n",
    "\n",
    "    return old_dl,new_dl,train_sampler\n",
    "\n",
    "def _change_dl_val(dl, shuffle):\n",
    "    old_dl = dl\n",
    "    validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        k.validation_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        k.validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    return old_dl,validation_loader,validation_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnvcfuzd3bIp"
   },
   "outputs": [],
   "source": [
    "class SingleTPUTraining(LearnerCallback):\n",
    "  def __init__(self, learn:Learner):\n",
    "    super().__init__(learn)\n",
    "\n",
    "  def on_train_begin(self, **kwargs:Any)->None:\n",
    "    self.device = xm.xla_device(devkind='TPU')\n",
    "    self.learn.model = self.learn.model.to(self.device)\n",
    "    #self.learn.data.add_tfm(partial(batch_to_device,device=self.device))\n",
    "    self.old_sampler_train_dl,self.data.train_dl,self.train_sampler = _change_dl(self.data.train_dl, shuffle=True)\n",
    "    self.old_sampler_valid_dl,self.data.valid_dl,self.valid_sampler = _change_dl_val(self.data.valid_dl, shuffle=False)\n",
    "\n",
    "    self.learn.data.train_dl = pl.ParallelLoader(self.data.train_dl, [self.device]).per_device_loader(self.device)\n",
    "    self.learn.data.valid_dl = pl.ParallelLoader(self.data.valid_dl, [self.device]).per_device_loader(self.device)\n",
    "    self.learn.data.train_dl.dataset = None #self.old_train_dl.dataset\n",
    "    self.learn.data.valid_dl.dataset = None #self.old_train_dl.dataset\n",
    "\n",
    "  def on_backward_end(self, **kwargs:Any)->None:\n",
    "    xm.optimizer_step(self.learn.opt.opt, barrier=True)\n",
    "\n",
    "def _to_tpu(learn:Learner) -> Learner:\n",
    "    learn.callback_fns.append(SingleTPUTraining)\n",
    "\n",
    "    return learn\n",
    "\n",
    "Learner.to_tpu = _to_tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnvcfuzd3bIp"
   },
   "outputs": [],
   "source": [
    "import pysnooper\n",
    "class TPUDistributed(LearnerCallback):\n",
    "    def __init__(self, learn:Learner, debug=True):\n",
    "        super().__init__(learn)\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "        if debug:\n",
    "            self.device = xm.xla_device(devkind='TPU')\n",
    "            logger.debug(\"TPUDistributed in DEBUG mode\")\n",
    "            #self.device = xm.xla_device(devkind='CPU')\n",
    "        else:\n",
    "            self.device = xm.xla_device(devkind='TPU')\n",
    "        logger.debug(\"%s used for xla_device for TPUDistributed\" % self.device)\n",
    "\n",
    "    def on_train_begin(self, **kwargs:Any)->None:\n",
    "        self.learn.model = self.learn.model.to(self.device)\n",
    "\n",
    "        pg = self.learn.opt.opt.param_groups\n",
    "        pg0pl = pg[0]['params'] # pg0pl[0] is a Parameter\n",
    "        pg1pl = pg[1]['params'] # pg0pl[0] is a Parameter\n",
    "\n",
    "        #logger.debug(\"grad info: %s\", raw_opt)\n",
    "        logger.debug(f\"on_train_begin pg0 lr: {pg[0]['lr']}\")\n",
    "        logger.debug(f\"on_train_begin pg1 lr: {pg[1]['lr']}\")\n",
    "\n",
    "        if self.debug:\n",
    "            self.learn.opt.lr = self.learn.opt.lr*xm.xrt_world_size()\n",
    "            #pg[0]['lr'] *= xm.xrt_world_size() # will do it twice...\n",
    "            #pg[1]['lr'] *= xm.xrt_world_size()\n",
    "            logger.debug(\"opt info: %s\\n type: %s\", self.learn.opt, type(self.learn.opt))\n",
    "        else:\n",
    "            self.learn.opt.lr = self.learn.opt.lr*xm.xrt_world_size()\n",
    "\n",
    "        logger.debug(\"%s used for xla_device, to device done\" % self.device)\n",
    "\n",
    "        shuffle = self.data.train_dl.init_kwargs['shuffle'] if hasattr(self.data.train_dl, 'init_kwargs') else True\n",
    "        self.old_sampler_train_dl,self.data.train_dl,self.train_sampler = _change_dl(self.data.train_dl, shuffle)\n",
    "\n",
    "        if hasattr(self.data, 'valid_dl') and self.data.valid_dl is not None:\n",
    "            self.old_sampler_valid_dl,self.data.valid_dl,self.valid_sampler = _change_dl_val(self.data.valid_dl, shuffle)\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self,**kwargs:Any)->None:\n",
    "        logger.debug(\"Epoch begins on device %s\" % self.device)\n",
    "\n",
    "        self.old_train_dl = self.data.train_dl\n",
    "        self.learn.data.train_dl = pl.ParallelLoader(self.old_train_dl, [self.device]).per_device_loader(self.device)\n",
    "        self.learn.data.train_dl.dataset = None #self.old_train_dl.dataset\n",
    "\n",
    "        if hasattr(self.data, 'valid_dl') and self.data.valid_dl is not None:\n",
    "            self.old_valid_dl = self.learn.data.valid_dl\n",
    "            self.learn.data.valid_dl = pl.ParallelLoader(self.old_valid_dl, [self.device]).per_device_loader(self.device)\n",
    "\n",
    "            self.learn.data.valid_dl.dataset = self.old_valid_dl.dataset\n",
    "            self.learn.data.valid_dl.dl = self.learn.data.valid_dl._loader._loader\n",
    "\n",
    "    def on_backward_end(self, **kwargs:Any)->None:\n",
    "        xm.optimizer_step(self.learn.opt.opt, barrier=True) # copied from https://github.com/tmabraham/fastai_tpu/blob/8b73018cf705da1a73d9be1f105a8e886051a90c/fastai_v1/tpu_distributed_fastai.py, and needed a fix\n",
    "        #may_debug(True)\n",
    "\n",
    "        return {'skip_step': True}\n",
    "\n",
    "    def on_epoch_end(self,**kwargs:Any)->None:\n",
    "        self.learn.data.train_dl = self.old_train_dl\n",
    "        self.learn.data.valid_dl = self.old_valid_dl\n",
    "\n",
    "    def on_train_end(self,**kwargs:Any)->None:\n",
    "        self.learn.data.train_dl = self.old_sampler_train_dl\n",
    "        self.learn.data.valid_dl = self.old_sampler_valid_dl\n",
    "\n",
    "\n",
    "def _to_tpu_distributed(learn:Learner) -> Learner:\n",
    "  #Learner.fit = _fit_tpu\n",
    "    learn.callback_fns.append(TPUDistributed)\n",
    "\n",
    "    return learn\n",
    "\n",
    "\n",
    "Learner.to_tpu_distributed = _to_tpu_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7z7QKwF3bIr",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from fastai.callbacks.misc import StopAfterNBatches\n",
    "from fastai.callbacks import *\n",
    "\n",
    "import pysnooper\n",
    "\n",
    "def debug_train(use_dist_cb=True):\n",
    "    logger.debug(f'debug train with{\" \" if use_dist_cb else \"OUT\"} to_tpu_distributed')\n",
    "    from kaggle_runner import defaults\n",
    "    _DEBUG = defaults.DEBUG\n",
    "    defaults.DEBUG = True\n",
    "\n",
    "    param_optimizer = list(k.model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'lr': 0., 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'lr': 0., 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    def AdamW_with_given_p(p_to_ignore, *args, **kargs):\n",
    "        kargs['lr']=TrainGlobalConfig.lr*xm.xrt_world_size()\n",
    "\n",
    "        return AdamW(optimizer_grouped_parameters, *args, **kargs)\n",
    "\n",
    "    learn = k.create_learner(k, opt_func=AdamW_with_given_p,\n",
    "                             loss_func=LabelSmoothing(),\n",
    "                             wd=0.01,\n",
    "                             callback_fns=[partial(GradientClipping, clip=0.5),\n",
    "                                           ShowGraph,\n",
    "                                           partial(CSVLogger, append=True),\n",
    "                                           partial(CheckGrad, skip_loss_step=False)]\n",
    "                             )\n",
    "\n",
    "    if use_dist_cb:\n",
    "        learn = learn.to_tpu_distributed()\n",
    "    else:\n",
    "        learn = learn.to_tpu()\n",
    "\n",
    "    learn.callbacks.append(StopAfterNBatches(n_batches=1000))\n",
    "    #learn.callback_fns.append(CheckGrad)\n",
    "    #print('hello')\n",
    "    #learn.lr_find(start_lr=1e-7, end_lr=1e-4, num_it=200)\n",
    "    #learn.recorder.plot()\n",
    "    #learn.fit_one_cycle(1, max_lr=2e-5)\n",
    "    learn.fit(1, lr=4e-5) # original 0.5*e-5*8=4*e-5\n",
    "    defaults.DEBUG = _DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VrJUbCYd3bIu",
    "lines_to_next_cell": 2,
    "outputId": "3dda0676-7309-467a-b5fd-6d0919ddb525"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-19 13:08:27,347:utils:debug train withOUT to_tpu_distributed\n",
      "[DEBUG]2020-06-19 13:08:27,377:utils:Init Callback CheckGrad with skip_loss_step: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='398' class='' max='18548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.15% [398/18548 06:48<5:10:12 1.1496]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-19 13:09:43,557:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:43,558:utils:itr: 0, num_batch: 0, last loss: 1.187500, smooth_loss: 1.187500\n",
      "[DEBUG]2020-06-19 13:09:44,031:utils:loss_avg: 1.18750, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.38095, mc_score:-0.33333\n",
      "[DEBUG]2020-06-19 13:09:45,219:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:45,220:utils:itr: 1, num_batch: 1, last loss: 1.109375, smooth_loss: 1.148043\n",
      "[DEBUG]2020-06-19 13:09:45,458:utils:loss_avg: 1.14844, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.53333, mc_score:-0.30212\n",
      "[DEBUG]2020-06-19 13:09:46,724:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:46,725:utils:itr: 2, num_batch: 2, last loss: 1.125000, smooth_loss: 1.140206\n",
      "[DEBUG]2020-06-19 13:09:46,983:utils:loss_avg: 1.14062, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.51049, mc_score:-0.17680\n",
      "[DEBUG]2020-06-19 13:09:47,819:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:47,820:utils:itr: 3, num_batch: 3, last loss: 1.781250, smooth_loss: 1.305356\n",
      "[DEBUG]2020-06-19 13:09:48,060:utils:loss_avg: 1.30078, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.45567, mc_score:-0.24566\n",
      "[DEBUG]2020-06-19 13:09:48,831:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:48,833:utils:itr: 4, num_batch: 4, last loss: 1.203125, smooth_loss: 1.284075\n",
      "[DEBUG]2020-06-19 13:09:49,097:utils:loss_avg: 1.28125, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.44255, mc_score:-0.20101\n",
      "[DEBUG]2020-06-19 13:09:49,865:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:49,867:utils:itr: 5, num_batch: 5, last loss: 1.101562, smooth_loss: 1.252100\n",
      "[DEBUG]2020-06-19 13:09:50,106:utils:loss_avg: 1.25130, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.44261, mc_score:-0.19225\n",
      "[DEBUG]2020-06-19 13:09:50,864:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:50,865:utils:itr: 6, num_batch: 6, last loss: 1.203125, smooth_loss: 1.244672\n",
      "[DEBUG]2020-06-19 13:09:51,123:utils:loss_avg: 1.24442, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.43726, mc_score:-0.22194\n",
      "[DEBUG]2020-06-19 13:09:51,963:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:51,965:utils:itr: 7, num_batch: 7, last loss: 1.093750, smooth_loss: 1.224447\n",
      "[DEBUG]2020-06-19 13:09:52,192:utils:loss_avg: 1.22559, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.44261, mc_score:-0.16586\n",
      "[DEBUG]2020-06-19 13:09:52,949:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:52,950:utils:itr: 8, num_batch: 8, last loss: 1.187500, smooth_loss: 1.220002\n",
      "[DEBUG]2020-06-19 13:09:53,173:utils:loss_avg: 1.22135, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.46322, mc_score:-0.12996\n",
      "[DEBUG]2020-06-19 13:09:53,930:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:53,931:utils:itr: 9, num_batch: 9, last loss: 1.148438, smooth_loss: 1.212178\n",
      "[DEBUG]2020-06-19 13:09:54,157:utils:loss_avg: 1.21406, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48061, mc_score:-0.12185\n",
      "[DEBUG]2020-06-19 13:09:54,916:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:54,917:utils:itr: 10, num_batch: 10, last loss: 1.148438, smooth_loss: 1.205780\n",
      "[DEBUG]2020-06-19 13:09:55,142:utils:loss_avg: 1.20810, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49961, mc_score:-0.09746\n",
      "[DEBUG]2020-06-19 13:09:55,966:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:55,967:utils:itr: 11, num_batch: 11, last loss: 1.039062, smooth_loss: 1.190292\n",
      "[DEBUG]2020-06-19 13:09:56,195:utils:loss_avg: 1.19401, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50369, mc_score:-0.08894\n",
      "[DEBUG]2020-06-19 13:09:56,954:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:56,955:utils:itr: 12, num_batch: 12, last loss: 1.140625, smooth_loss: 1.185991\n",
      "[DEBUG]2020-06-19 13:09:57,184:utils:loss_avg: 1.18990, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50597, mc_score:-0.06383\n",
      "[DEBUG]2020-06-19 13:09:57,946:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:57,947:utils:itr: 13, num_batch: 13, last loss: 1.203125, smooth_loss: 1.187382\n",
      "[DEBUG]2020-06-19 13:09:58,177:utils:loss_avg: 1.19085, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49729, mc_score:-0.07679\n",
      "[DEBUG]2020-06-19 13:09:58,936:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:09:58,937:utils:itr: 14, num_batch: 14, last loss: 1.187500, smooth_loss: 1.187392\n",
      "[DEBUG]2020-06-19 13:09:59,181:utils:loss_avg: 1.19063, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49760, mc_score:-0.08814\n",
      "[DEBUG]2020-06-19 13:10:00,005:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:00,007:utils:itr: 15, num_batch: 15, last loss: 1.007812, smooth_loss: 1.174388\n",
      "[DEBUG]2020-06-19 13:10:00,260:utils:loss_avg: 1.17920, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49738, mc_score:-0.08961\n",
      "[DEBUG]2020-06-19 13:10:01,021:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:01,022:utils:itr: 16, num_batch: 16, last loss: 1.078125, smooth_loss: 1.167765\n",
      "[DEBUG]2020-06-19 13:10:01,251:utils:loss_avg: 1.17325, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49724, mc_score:-0.06831\n",
      "[DEBUG]2020-06-19 13:10:02,012:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:02,013:utils:itr: 17, num_batch: 17, last loss: 1.203125, smooth_loss: 1.170084\n",
      "[DEBUG]2020-06-19 13:10:02,257:utils:loss_avg: 1.17491, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50314, mc_score:-0.09062\n",
      "[DEBUG]2020-06-19 13:10:03,015:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:03,016:utils:itr: 18, num_batch: 18, last loss: 1.101562, smooth_loss: 1.165785\n",
      "[DEBUG]2020-06-19 13:10:03,256:utils:loss_avg: 1.17105, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50463, mc_score:-0.07272\n",
      "[DEBUG]2020-06-19 13:10:04,100:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:04,101:utils:itr: 19, num_batch: 19, last loss: 1.203125, smooth_loss: 1.168032\n",
      "[DEBUG]2020-06-19 13:10:04,332:utils:loss_avg: 1.17266, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50456, mc_score:-0.06200\n",
      "[DEBUG]2020-06-19 13:10:05,093:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:05,094:utils:itr: 20, num_batch: 20, last loss: 1.242188, smooth_loss: 1.172322\n",
      "[DEBUG]2020-06-19 13:10:05,322:utils:loss_avg: 1.17597, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50724, mc_score:-0.05350\n",
      "[DEBUG]2020-06-19 13:10:06,082:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:06,083:utils:itr: 21, num_batch: 21, last loss: 1.062500, smooth_loss: 1.166201\n",
      "[DEBUG]2020-06-19 13:10:06,310:utils:loss_avg: 1.17081, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.51080, mc_score:-0.05049\n",
      "[DEBUG]2020-06-19 13:10:07,074:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:07,075:utils:itr: 22, num_batch: 22, last loss: 1.078125, smooth_loss: 1.161461\n",
      "[DEBUG]2020-06-19 13:10:07,305:utils:loss_avg: 1.16678, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.51228, mc_score:-0.06498\n",
      "[DEBUG]2020-06-19 13:10:08,151:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:08,153:utils:itr: 23, num_batch: 23, last loss: 1.101562, smooth_loss: 1.158343\n",
      "[DEBUG]2020-06-19 13:10:08,384:utils:loss_avg: 1.16406, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50699, mc_score:-0.06809\n",
      "[DEBUG]2020-06-19 13:10:09,145:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:09,146:utils:itr: 24, num_batch: 24, last loss: 1.132812, smooth_loss: 1.157055\n",
      "[DEBUG]2020-06-19 13:10:09,374:utils:loss_avg: 1.16281, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50686, mc_score:-0.06589\n",
      "[DEBUG]2020-06-19 13:10:10,138:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:10,139:utils:itr: 25, num_batch: 25, last loss: 0.984375, smooth_loss: 1.148603\n",
      "[DEBUG]2020-06-19 13:10:10,370:utils:loss_avg: 1.15595, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50453, mc_score:-0.06584\n",
      "[DEBUG]2020-06-19 13:10:11,131:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:11,132:utils:itr: 26, num_batch: 26, last loss: 1.062500, smooth_loss: 1.144507\n",
      "[DEBUG]2020-06-19 13:10:11,361:utils:loss_avg: 1.15249, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.50242, mc_score:-0.06517\n",
      "[DEBUG]2020-06-19 13:10:12,175:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:12,177:utils:itr: 27, num_batch: 27, last loss: 1.289062, smooth_loss: 1.151199\n",
      "[DEBUG]2020-06-19 13:10:12,421:utils:loss_avg: 1.15737, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49483, mc_score:-0.06821\n",
      "[DEBUG]2020-06-19 13:10:13,185:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:13,186:utils:itr: 28, num_batch: 28, last loss: 1.062500, smooth_loss: 1.147198\n",
      "[DEBUG]2020-06-19 13:10:13,413:utils:loss_avg: 1.15409, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49590, mc_score:-0.05573\n",
      "[DEBUG]2020-06-19 13:10:14,173:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:14,174:utils:itr: 29, num_batch: 29, last loss: 1.281250, smooth_loss: 1.153097\n",
      "[DEBUG]2020-06-19 13:10:14,400:utils:loss_avg: 1.15833, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49062, mc_score:-0.05391\n",
      "[DEBUG]2020-06-19 13:10:15,161:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:15,162:utils:itr: 30, num_batch: 30, last loss: 1.062500, smooth_loss: 1.149204\n",
      "[DEBUG]2020-06-19 13:10:15,389:utils:loss_avg: 1.15524, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49332, mc_score:-0.04689\n",
      "[DEBUG]2020-06-19 13:10:16,224:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:16,225:utils:itr: 31, num_batch: 31, last loss: 1.171875, smooth_loss: 1.150156\n",
      "[DEBUG]2020-06-19 13:10:16,456:utils:loss_avg: 1.15576, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49400, mc_score:-0.04113\n",
      "[DEBUG]2020-06-19 13:10:17,215:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:17,216:utils:itr: 32, num_batch: 32, last loss: 1.085938, smooth_loss: 1.147517\n",
      "[DEBUG]2020-06-19 13:10:17,446:utils:loss_avg: 1.15365, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49045, mc_score:-0.03311\n",
      "[DEBUG]2020-06-19 13:10:18,208:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:18,209:utils:itr: 33, num_batch: 33, last loss: 1.164062, smooth_loss: 1.148183\n",
      "[DEBUG]2020-06-19 13:10:18,436:utils:loss_avg: 1.15395, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48845, mc_score:-0.03775\n",
      "[DEBUG]2020-06-19 13:10:19,201:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:19,203:utils:itr: 34, num_batch: 34, last loss: 0.867188, smooth_loss: 1.137097\n",
      "[DEBUG]2020-06-19 13:10:19,431:utils:loss_avg: 1.14576, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48890, mc_score:-0.03160\n",
      "[DEBUG]2020-06-19 13:10:20,264:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:20,266:utils:itr: 35, num_batch: 35, last loss: 1.171875, smooth_loss: 1.138443\n",
      "[DEBUG]2020-06-19 13:10:20,495:utils:loss_avg: 1.14648, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49114, mc_score:-0.02713\n",
      "[DEBUG]2020-06-19 13:10:21,256:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:21,257:utils:itr: 36, num_batch: 36, last loss: 1.148438, smooth_loss: 1.138822\n",
      "[DEBUG]2020-06-19 13:10:21,484:utils:loss_avg: 1.14654, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49562, mc_score:-0.02250\n",
      "[DEBUG]2020-06-19 13:10:22,247:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:22,248:utils:itr: 37, num_batch: 37, last loss: 1.218750, smooth_loss: 1.141805\n",
      "[DEBUG]2020-06-19 13:10:22,477:utils:loss_avg: 1.14844, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49363, mc_score:-0.01715\n",
      "[DEBUG]2020-06-19 13:10:23,241:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:23,242:utils:itr: 38, num_batch: 38, last loss: 0.925781, smooth_loss: 1.133881\n",
      "[DEBUG]2020-06-19 13:10:23,467:utils:loss_avg: 1.14273, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49055, mc_score:-0.02098\n",
      "[DEBUG]2020-06-19 13:10:24,300:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:24,301:utils:itr: 39, num_batch: 39, last loss: 1.468750, smooth_loss: 1.145963\n",
      "[DEBUG]2020-06-19 13:10:24,523:utils:loss_avg: 1.15088, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49008, mc_score:-0.03014\n",
      "[DEBUG]2020-06-19 13:10:25,287:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:25,289:utils:itr: 40, num_batch: 40, last loss: 1.203125, smooth_loss: 1.147993\n",
      "[DEBUG]2020-06-19 13:10:25,515:utils:loss_avg: 1.15215, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48401, mc_score:-0.03850\n",
      "[DEBUG]2020-06-19 13:10:26,275:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:26,277:utils:itr: 41, num_batch: 41, last loss: 1.453125, smooth_loss: 1.158663\n",
      "[DEBUG]2020-06-19 13:10:26,504:utils:loss_avg: 1.15932, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47884, mc_score:-0.04876\n",
      "[DEBUG]2020-06-19 13:10:27,262:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:27,264:utils:itr: 42, num_batch: 42, last loss: 1.640625, smooth_loss: 1.175268\n",
      "[DEBUG]2020-06-19 13:10:27,490:utils:loss_avg: 1.17051, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47155, mc_score:-0.05668\n",
      "[DEBUG]2020-06-19 13:10:28,330:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:28,331:utils:itr: 43, num_batch: 43, last loss: 1.414062, smooth_loss: 1.183378\n",
      "[DEBUG]2020-06-19 13:10:28,557:utils:loss_avg: 1.17605, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.46864, mc_score:-0.06718\n",
      "[DEBUG]2020-06-19 13:10:29,318:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:29,320:utils:itr: 44, num_batch: 44, last loss: 1.414062, smooth_loss: 1.191104\n",
      "[DEBUG]2020-06-19 13:10:29,549:utils:loss_avg: 1.18134, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47035, mc_score:-0.06779\n",
      "[DEBUG]2020-06-19 13:10:30,311:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:30,313:utils:itr: 45, num_batch: 45, last loss: 1.023438, smooth_loss: 1.185563\n",
      "[DEBUG]2020-06-19 13:10:30,543:utils:loss_avg: 1.17790, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47758, mc_score:-0.06136\n",
      "[DEBUG]2020-06-19 13:10:31,307:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:31,309:utils:itr: 46, num_batch: 46, last loss: 1.257812, smooth_loss: 1.187920\n",
      "[DEBUG]2020-06-19 13:10:31,536:utils:loss_avg: 1.17960, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47763, mc_score:-0.06136\n",
      "[DEBUG]2020-06-19 13:10:32,372:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:32,374:utils:itr: 47, num_batch: 47, last loss: 1.109375, smooth_loss: 1.185390\n",
      "[DEBUG]2020-06-19 13:10:32,599:utils:loss_avg: 1.17814, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48420, mc_score:-0.05216\n",
      "[DEBUG]2020-06-19 13:10:33,358:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:33,360:utils:itr: 48, num_batch: 48, last loss: 1.109375, smooth_loss: 1.182971\n",
      "[DEBUG]2020-06-19 13:10:33,587:utils:loss_avg: 1.17674, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48626, mc_score:-0.04438\n",
      "[DEBUG]2020-06-19 13:10:34,351:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:34,353:utils:itr: 49, num_batch: 49, last loss: 1.023438, smooth_loss: 1.177952\n",
      "[DEBUG]2020-06-19 13:10:34,579:utils:loss_avg: 1.17367, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48987, mc_score:-0.03319\n",
      "[DEBUG]2020-06-19 13:10:35,341:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:35,342:utils:itr: 50, num_batch: 50, last loss: 1.429688, smooth_loss: 1.185781\n",
      "[DEBUG]2020-06-19 13:10:35,569:utils:loss_avg: 1.17869, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48567, mc_score:-0.03347\n",
      "[DEBUG]2020-06-19 13:10:36,435:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:36,437:utils:itr: 51, num_batch: 51, last loss: 1.257812, smooth_loss: 1.187997\n",
      "[DEBUG]2020-06-19 13:10:36,664:utils:loss_avg: 1.18021, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48357, mc_score:-0.03549\n",
      "[DEBUG]2020-06-19 13:10:37,428:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:37,429:utils:itr: 52, num_batch: 52, last loss: 1.210938, smooth_loss: 1.188695\n",
      "[DEBUG]2020-06-19 13:10:37,657:utils:loss_avg: 1.18079, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48485, mc_score:-0.04637\n",
      "[DEBUG]2020-06-19 13:10:38,422:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:38,424:utils:itr: 53, num_batch: 53, last loss: 0.933594, smooth_loss: 1.181012\n",
      "[DEBUG]2020-06-19 13:10:38,650:utils:loss_avg: 1.17622, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49377, mc_score:-0.04115\n",
      "[DEBUG]2020-06-19 13:10:39,412:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:39,413:utils:itr: 54, num_batch: 54, last loss: 1.039062, smooth_loss: 1.176780\n",
      "[DEBUG]2020-06-19 13:10:39,642:utils:loss_avg: 1.17372, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49200, mc_score:-0.03870\n",
      "[DEBUG]2020-06-19 13:10:40,502:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:40,503:utils:itr: 55, num_batch: 55, last loss: 1.132812, smooth_loss: 1.175482\n",
      "[DEBUG]2020-06-19 13:10:40,730:utils:loss_avg: 1.17299, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48875, mc_score:-0.03555\n",
      "[DEBUG]2020-06-19 13:10:41,539:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:41,541:utils:itr: 56, num_batch: 56, last loss: 1.265625, smooth_loss: 1.178118\n",
      "[DEBUG]2020-06-19 13:10:41,769:utils:loss_avg: 1.17462, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48628, mc_score:-0.03832\n",
      "[DEBUG]2020-06-19 13:10:42,528:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:42,529:utils:itr: 57, num_batch: 57, last loss: 1.484375, smooth_loss: 1.186993\n",
      "[DEBUG]2020-06-19 13:10:42,759:utils:loss_avg: 1.17996, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48543, mc_score:-0.04172\n",
      "[DEBUG]2020-06-19 13:10:43,524:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:43,526:utils:itr: 58, num_batch: 58, last loss: 1.031250, smooth_loss: 1.182520\n",
      "[DEBUG]2020-06-19 13:10:43,753:utils:loss_avg: 1.17744, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48659, mc_score:-0.04200\n",
      "[DEBUG]2020-06-19 13:10:44,587:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:44,588:utils:itr: 59, num_batch: 59, last loss: 0.957031, smooth_loss: 1.176100\n",
      "[DEBUG]2020-06-19 13:10:44,812:utils:loss_avg: 1.17376, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48392, mc_score:-0.04529\n",
      "[DEBUG]2020-06-19 13:10:45,573:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:45,575:utils:itr: 60, num_batch: 60, last loss: 1.078125, smooth_loss: 1.173334\n",
      "[DEBUG]2020-06-19 13:10:45,800:utils:loss_avg: 1.17220, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48279, mc_score:-0.05010\n",
      "[DEBUG]2020-06-19 13:10:46,565:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:46,566:utils:itr: 61, num_batch: 61, last loss: 1.101562, smooth_loss: 1.171324\n",
      "[DEBUG]2020-06-19 13:10:46,790:utils:loss_avg: 1.17106, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48221, mc_score:-0.05250\n",
      "[DEBUG]2020-06-19 13:10:47,549:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:47,551:utils:itr: 62, num_batch: 62, last loss: 1.117188, smooth_loss: 1.169820\n",
      "[DEBUG]2020-06-19 13:10:47,779:utils:loss_avg: 1.17020, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48258, mc_score:-0.05239\n",
      "[DEBUG]2020-06-19 13:10:48,628:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:48,630:utils:itr: 63, num_batch: 63, last loss: 1.078125, smooth_loss: 1.167293\n",
      "[DEBUG]2020-06-19 13:10:48,856:utils:loss_avg: 1.16876, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48492, mc_score:-0.04845\n",
      "[DEBUG]2020-06-19 13:10:49,619:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:49,621:utils:itr: 64, num_batch: 64, last loss: 1.390625, smooth_loss: 1.173403\n",
      "[DEBUG]2020-06-19 13:10:49,846:utils:loss_avg: 1.17218, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48361, mc_score:-0.05218\n",
      "[DEBUG]2020-06-19 13:10:50,605:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:50,606:utils:itr: 65, num_batch: 65, last loss: 1.328125, smooth_loss: 1.177605\n",
      "[DEBUG]2020-06-19 13:10:50,834:utils:loss_avg: 1.17454, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48239, mc_score:-0.05510\n",
      "[DEBUG]2020-06-19 13:10:51,595:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:51,596:utils:itr: 66, num_batch: 66, last loss: 1.078125, smooth_loss: 1.174922\n",
      "[DEBUG]2020-06-19 13:10:51,827:utils:loss_avg: 1.17310, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48238, mc_score:-0.05358\n",
      "[DEBUG]2020-06-19 13:10:52,671:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:52,672:utils:itr: 67, num_batch: 67, last loss: 1.054688, smooth_loss: 1.171703\n",
      "[DEBUG]2020-06-19 13:10:52,899:utils:loss_avg: 1.17136, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48678, mc_score:-0.05982\n",
      "[DEBUG]2020-06-19 13:10:53,661:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:53,662:utils:itr: 68, num_batch: 68, last loss: 1.031250, smooth_loss: 1.167967\n",
      "[DEBUG]2020-06-19 13:10:53,886:utils:loss_avg: 1.16933, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48728, mc_score:-0.06321\n",
      "[DEBUG]2020-06-19 13:10:54,648:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:54,650:utils:itr: 69, num_batch: 69, last loss: 1.390625, smooth_loss: 1.173850\n",
      "[DEBUG]2020-06-19 13:10:54,880:utils:loss_avg: 1.17249, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48526, mc_score:-0.06031\n",
      "[DEBUG]2020-06-19 13:10:55,646:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:55,648:utils:itr: 70, num_batch: 70, last loss: 1.039062, smooth_loss: 1.170311\n",
      "[DEBUG]2020-06-19 13:10:55,875:utils:loss_avg: 1.17061, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48829, mc_score:-0.05334\n",
      "[DEBUG]2020-06-19 13:10:56,704:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:56,706:utils:itr: 71, num_batch: 71, last loss: 1.117188, smooth_loss: 1.168925\n",
      "[DEBUG]2020-06-19 13:10:56,936:utils:loss_avg: 1.16987, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48802, mc_score:-0.05475\n",
      "[DEBUG]2020-06-19 13:10:57,703:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:57,704:utils:itr: 72, num_batch: 72, last loss: 1.250000, smooth_loss: 1.171028\n",
      "[DEBUG]2020-06-19 13:10:57,936:utils:loss_avg: 1.17097, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49007, mc_score:-0.05054\n",
      "[DEBUG]2020-06-19 13:10:58,705:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:58,706:utils:itr: 73, num_batch: 73, last loss: 1.179688, smooth_loss: 1.171251\n",
      "[DEBUG]2020-06-19 13:10:58,940:utils:loss_avg: 1.17108, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48714, mc_score:-0.04861\n",
      "[DEBUG]2020-06-19 13:10:59,705:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:10:59,706:utils:itr: 74, num_batch: 74, last loss: 1.140625, smooth_loss: 1.170466\n",
      "[DEBUG]2020-06-19 13:10:59,934:utils:loss_avg: 1.17068, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48899, mc_score:-0.04265\n",
      "[DEBUG]2020-06-19 13:11:00,760:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:00,761:utils:itr: 75, num_batch: 75, last loss: 1.210938, smooth_loss: 1.171498\n",
      "[DEBUG]2020-06-19 13:11:00,985:utils:loss_avg: 1.17121, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48844, mc_score:-0.04160\n",
      "[DEBUG]2020-06-19 13:11:01,750:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:01,752:utils:itr: 76, num_batch: 76, last loss: 1.296875, smooth_loss: 1.174676\n",
      "[DEBUG]2020-06-19 13:11:01,980:utils:loss_avg: 1.17284, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48756, mc_score:-0.04410\n",
      "[DEBUG]2020-06-19 13:11:02,742:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:02,743:utils:itr: 77, num_batch: 77, last loss: 1.187500, smooth_loss: 1.174999\n",
      "[DEBUG]2020-06-19 13:11:02,966:utils:loss_avg: 1.17303, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48647, mc_score:-0.04790\n",
      "[DEBUG]2020-06-19 13:11:03,726:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:03,728:utils:itr: 78, num_batch: 78, last loss: 1.171875, smooth_loss: 1.174921\n",
      "[DEBUG]2020-06-19 13:11:03,953:utils:loss_avg: 1.17301, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48637, mc_score:-0.04728\n",
      "[DEBUG]2020-06-19 13:11:04,779:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:04,780:utils:itr: 79, num_batch: 79, last loss: 0.902344, smooth_loss: 1.168118\n",
      "[DEBUG]2020-06-19 13:11:05,005:utils:loss_avg: 1.16963, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48851, mc_score:-0.04262\n",
      "[DEBUG]2020-06-19 13:11:05,768:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:05,769:utils:itr: 80, num_batch: 80, last loss: 1.046875, smooth_loss: 1.165107\n",
      "[DEBUG]2020-06-19 13:11:05,996:utils:loss_avg: 1.16811, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49029, mc_score:-0.04136\n",
      "[DEBUG]2020-06-19 13:11:06,759:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:06,761:utils:itr: 81, num_batch: 81, last loss: 1.078125, smooth_loss: 1.162957\n",
      "[DEBUG]2020-06-19 13:11:06,986:utils:loss_avg: 1.16702, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49006, mc_score:-0.04216\n",
      "[DEBUG]2020-06-19 13:11:07,750:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:07,752:utils:itr: 82, num_batch: 82, last loss: 1.164062, smooth_loss: 1.162985\n",
      "[DEBUG]2020-06-19 13:11:07,975:utils:loss_avg: 1.16698, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48947, mc_score:-0.04127\n",
      "[DEBUG]2020-06-19 13:11:08,816:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:08,817:utils:itr: 83, num_batch: 83, last loss: 0.820312, smooth_loss: 1.154594\n",
      "[DEBUG]2020-06-19 13:11:09,044:utils:loss_avg: 1.16285, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49287, mc_score:-0.04100\n",
      "[DEBUG]2020-06-19 13:11:09,806:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:09,807:utils:itr: 84, num_batch: 84, last loss: 1.421875, smooth_loss: 1.161109\n",
      "[DEBUG]2020-06-19 13:11:10,033:utils:loss_avg: 1.16590, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49078, mc_score:-0.04397\n",
      "[DEBUG]2020-06-19 13:11:10,796:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:10,797:utils:itr: 85, num_batch: 85, last loss: 1.203125, smooth_loss: 1.162129\n",
      "[DEBUG]2020-06-19 13:11:11,027:utils:loss_avg: 1.16633, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48923, mc_score:-0.04313\n",
      "[DEBUG]2020-06-19 13:11:11,789:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:11,790:utils:itr: 86, num_batch: 86, last loss: 1.281250, smooth_loss: 1.165008\n",
      "[DEBUG]2020-06-19 13:11:12,017:utils:loss_avg: 1.16765, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48949, mc_score:-0.04601\n",
      "[DEBUG]2020-06-19 13:11:12,863:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:12,864:utils:itr: 87, num_batch: 87, last loss: 0.910156, smooth_loss: 1.158874\n",
      "[DEBUG]2020-06-19 13:11:13,092:utils:loss_avg: 1.16473, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49058, mc_score:-0.04445\n",
      "[DEBUG]2020-06-19 13:11:13,855:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:13,856:utils:itr: 88, num_batch: 88, last loss: 0.972656, smooth_loss: 1.154411\n",
      "[DEBUG]2020-06-19 13:11:14,082:utils:loss_avg: 1.16257, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49350, mc_score:-0.04552\n",
      "[DEBUG]2020-06-19 13:11:14,843:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:14,845:utils:itr: 89, num_batch: 89, last loss: 1.281250, smooth_loss: 1.157439\n",
      "[DEBUG]2020-06-19 13:11:15,071:utils:loss_avg: 1.16389, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49451, mc_score:-0.04392\n",
      "[DEBUG]2020-06-19 13:11:15,830:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:15,831:utils:itr: 90, num_batch: 90, last loss: 1.148438, smooth_loss: 1.157225\n",
      "[DEBUG]2020-06-19 13:11:16,054:utils:loss_avg: 1.16372, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49504, mc_score:-0.04411\n",
      "[DEBUG]2020-06-19 13:11:16,884:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:16,885:utils:itr: 91, num_batch: 91, last loss: 0.800781, smooth_loss: 1.148780\n",
      "[DEBUG]2020-06-19 13:11:17,115:utils:loss_avg: 1.15977, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49607, mc_score:-0.04547\n",
      "[DEBUG]2020-06-19 13:11:17,875:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:17,876:utils:itr: 92, num_batch: 92, last loss: 1.257812, smooth_loss: 1.151354\n",
      "[DEBUG]2020-06-19 13:11:18,099:utils:loss_avg: 1.16083, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49561, mc_score:-0.04793\n",
      "[DEBUG]2020-06-19 13:11:18,857:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:18,858:utils:itr: 93, num_batch: 93, last loss: 1.015625, smooth_loss: 1.148161\n",
      "[DEBUG]2020-06-19 13:11:19,085:utils:loss_avg: 1.15928, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49485, mc_score:-0.04667\n",
      "[DEBUG]2020-06-19 13:11:19,851:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:19,852:utils:itr: 94, num_batch: 94, last loss: 1.164062, smooth_loss: 1.148534\n",
      "[DEBUG]2020-06-19 13:11:20,079:utils:loss_avg: 1.15933, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49495, mc_score:-0.04419\n",
      "[DEBUG]2020-06-19 13:11:20,902:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:20,903:utils:itr: 95, num_batch: 95, last loss: 1.289062, smooth_loss: 1.151816\n",
      "[DEBUG]2020-06-19 13:11:21,129:utils:loss_avg: 1.16069, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49457, mc_score:-0.04588\n",
      "[DEBUG]2020-06-19 13:11:21,891:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:21,892:utils:itr: 96, num_batch: 96, last loss: 1.257812, smooth_loss: 1.154284\n",
      "[DEBUG]2020-06-19 13:11:22,117:utils:loss_avg: 1.16169, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49355, mc_score:-0.04800\n",
      "[DEBUG]2020-06-19 13:11:22,879:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:22,880:utils:itr: 97, num_batch: 97, last loss: 1.484375, smooth_loss: 1.161944\n",
      "[DEBUG]2020-06-19 13:11:23,109:utils:loss_avg: 1.16498, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49181, mc_score:-0.04907\n",
      "[DEBUG]2020-06-19 13:11:23,871:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:23,872:utils:itr: 98, num_batch: 98, last loss: 1.187500, smooth_loss: 1.162535\n",
      "[DEBUG]2020-06-19 13:11:24,094:utils:loss_avg: 1.16521, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48986, mc_score:-0.05000\n",
      "[DEBUG]2020-06-19 13:11:24,987:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:24,988:utils:itr: 99, num_batch: 99, last loss: 0.859375, smooth_loss: 1.155545\n",
      "[DEBUG]2020-06-19 13:11:25,215:utils:loss_avg: 1.16215, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48914, mc_score:-0.04794\n",
      "[DEBUG]2020-06-19 13:11:25,977:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:25,978:utils:itr: 100, num_batch: 100, last loss: 1.156250, smooth_loss: 1.155561\n",
      "[DEBUG]2020-06-19 13:11:26,206:utils:loss_avg: 1.16209, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48910, mc_score:-0.04677\n",
      "[DEBUG]2020-06-19 13:11:26,967:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:26,969:utils:itr: 101, num_batch: 101, last loss: 1.171875, smooth_loss: 1.155935\n",
      "[DEBUG]2020-06-19 13:11:27,197:utils:loss_avg: 1.16219, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48788, mc_score:-0.04882\n",
      "[DEBUG]2020-06-19 13:11:27,959:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:27,961:utils:itr: 102, num_batch: 102, last loss: 1.085938, smooth_loss: 1.154335\n",
      "[DEBUG]2020-06-19 13:11:28,192:utils:loss_avg: 1.16145, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48957, mc_score:-0.04630\n",
      "[DEBUG]2020-06-19 13:11:29,062:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:29,064:utils:itr: 103, num_batch: 103, last loss: 1.062500, smooth_loss: 1.152243\n",
      "[DEBUG]2020-06-19 13:11:29,292:utils:loss_avg: 1.16049, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48991, mc_score:-0.04520\n",
      "[DEBUG]2020-06-19 13:11:30,053:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:30,054:utils:itr: 104, num_batch: 104, last loss: 1.046875, smooth_loss: 1.149848\n",
      "[DEBUG]2020-06-19 13:11:30,282:utils:loss_avg: 1.15941, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48923, mc_score:-0.04765\n",
      "[DEBUG]2020-06-19 13:11:31,041:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:31,043:utils:itr: 105, num_batch: 105, last loss: 1.312500, smooth_loss: 1.153534\n",
      "[DEBUG]2020-06-19 13:11:31,270:utils:loss_avg: 1.16086, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48857, mc_score:-0.04826\n",
      "[DEBUG]2020-06-19 13:11:32,033:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:32,034:utils:itr: 106, num_batch: 106, last loss: 1.187500, smooth_loss: 1.154302\n",
      "[DEBUG]2020-06-19 13:11:32,262:utils:loss_avg: 1.16111, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48735, mc_score:-0.04720\n",
      "[DEBUG]2020-06-19 13:11:33,105:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:33,107:utils:itr: 107, num_batch: 107, last loss: 1.289062, smooth_loss: 1.157340\n",
      "[DEBUG]2020-06-19 13:11:33,335:utils:loss_avg: 1.16229, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48744, mc_score:-0.04714\n",
      "[DEBUG]2020-06-19 13:11:34,098:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:34,099:utils:itr: 108, num_batch: 108, last loss: 1.148438, smooth_loss: 1.157140\n",
      "[DEBUG]2020-06-19 13:11:34,324:utils:loss_avg: 1.16216, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48665, mc_score:-0.04628\n",
      "[DEBUG]2020-06-19 13:11:35,088:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:35,089:utils:itr: 109, num_batch: 109, last loss: 1.078125, smooth_loss: 1.155367\n",
      "[DEBUG]2020-06-19 13:11:35,315:utils:loss_avg: 1.16140, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48520, mc_score:-0.04695\n",
      "[DEBUG]2020-06-19 13:11:36,072:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:36,074:utils:itr: 110, num_batch: 110, last loss: 1.117188, smooth_loss: 1.154513\n",
      "[DEBUG]2020-06-19 13:11:36,302:utils:loss_avg: 1.16100, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48555, mc_score:-0.04842\n",
      "[DEBUG]2020-06-19 13:11:37,130:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:37,132:utils:itr: 111, num_batch: 111, last loss: 1.265625, smooth_loss: 1.156994\n",
      "[DEBUG]2020-06-19 13:11:37,362:utils:loss_avg: 1.16193, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48618, mc_score:-0.04537\n",
      "[DEBUG]2020-06-19 13:11:38,127:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:38,128:utils:itr: 112, num_batch: 112, last loss: 0.921875, smooth_loss: 1.151757\n",
      "[DEBUG]2020-06-19 13:11:38,351:utils:loss_avg: 1.15981, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48816, mc_score:-0.04416\n",
      "[DEBUG]2020-06-19 13:11:39,110:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:39,111:utils:itr: 113, num_batch: 113, last loss: 1.117188, smooth_loss: 1.150989\n",
      "[DEBUG]2020-06-19 13:11:39,341:utils:loss_avg: 1.15944, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48800, mc_score:-0.04375\n",
      "[DEBUG]2020-06-19 13:11:40,105:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:40,106:utils:itr: 114, num_batch: 114, last loss: 1.015625, smooth_loss: 1.147988\n",
      "[DEBUG]2020-06-19 13:11:40,332:utils:loss_avg: 1.15819, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48841, mc_score:-0.04113\n",
      "[DEBUG]2020-06-19 13:11:41,160:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:41,161:utils:itr: 115, num_batch: 115, last loss: 1.093750, smooth_loss: 1.146788\n",
      "[DEBUG]2020-06-19 13:11:41,391:utils:loss_avg: 1.15763, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48717, mc_score:-0.04339\n",
      "[DEBUG]2020-06-19 13:11:42,153:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:42,154:utils:itr: 116, num_batch: 116, last loss: 1.390625, smooth_loss: 1.152171\n",
      "[DEBUG]2020-06-19 13:11:42,383:utils:loss_avg: 1.15962, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48809, mc_score:-0.04370\n",
      "[DEBUG]2020-06-19 13:11:43,144:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:43,145:utils:itr: 117, num_batch: 117, last loss: 1.054688, smooth_loss: 1.150024\n",
      "[DEBUG]2020-06-19 13:11:43,372:utils:loss_avg: 1.15873, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48857, mc_score:-0.04451\n",
      "[DEBUG]2020-06-19 13:11:44,137:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:44,139:utils:itr: 118, num_batch: 118, last loss: 0.988281, smooth_loss: 1.146467\n",
      "[DEBUG]2020-06-19 13:11:44,364:utils:loss_avg: 1.15730, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48956, mc_score:-0.04513\n",
      "[DEBUG]2020-06-19 13:11:45,192:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:45,193:utils:itr: 119, num_batch: 119, last loss: 1.257812, smooth_loss: 1.148911\n",
      "[DEBUG]2020-06-19 13:11:45,420:utils:loss_avg: 1.15814, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49097, mc_score:-0.04274\n",
      "[DEBUG]2020-06-19 13:11:46,180:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:46,182:utils:itr: 120, num_batch: 120, last loss: 1.390625, smooth_loss: 1.154204\n",
      "[DEBUG]2020-06-19 13:11:46,408:utils:loss_avg: 1.16006, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49127, mc_score:-0.04440\n",
      "[DEBUG]2020-06-19 13:11:47,175:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:47,176:utils:itr: 121, num_batch: 121, last loss: 1.500000, smooth_loss: 1.161763\n",
      "[DEBUG]2020-06-19 13:11:47,401:utils:loss_avg: 1.16285, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48935, mc_score:-0.04799\n",
      "[DEBUG]2020-06-19 13:11:48,164:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:48,166:utils:itr: 122, num_batch: 122, last loss: 1.062500, smooth_loss: 1.159597\n",
      "[DEBUG]2020-06-19 13:11:48,397:utils:loss_avg: 1.16203, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48836, mc_score:-0.04685\n",
      "[DEBUG]2020-06-19 13:11:49,234:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:49,235:utils:itr: 123, num_batch: 123, last loss: 1.562500, smooth_loss: 1.168372\n",
      "[DEBUG]2020-06-19 13:11:49,465:utils:loss_avg: 1.16526, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48602, mc_score:-0.04754\n",
      "[DEBUG]2020-06-19 13:11:50,237:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:50,238:utils:itr: 124, num_batch: 124, last loss: 1.054688, smooth_loss: 1.165900\n",
      "[DEBUG]2020-06-19 13:11:50,463:utils:loss_avg: 1.16437, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48745, mc_score:-0.04774\n",
      "[DEBUG]2020-06-19 13:11:51,225:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:51,227:utils:itr: 125, num_batch: 125, last loss: 1.031250, smooth_loss: 1.162978\n",
      "[DEBUG]2020-06-19 13:11:51,454:utils:loss_avg: 1.16332, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48685, mc_score:-0.04462\n",
      "[DEBUG]2020-06-19 13:11:52,215:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:52,217:utils:itr: 126, num_batch: 126, last loss: 1.265625, smooth_loss: 1.165202\n",
      "[DEBUG]2020-06-19 13:11:52,444:utils:loss_avg: 1.16412, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48656, mc_score:-0.04333\n",
      "[DEBUG]2020-06-19 13:11:53,284:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:53,285:utils:itr: 127, num_batch: 127, last loss: 1.437500, smooth_loss: 1.171092\n",
      "[DEBUG]2020-06-19 13:11:53,512:utils:loss_avg: 1.16626, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48613, mc_score:-0.04564\n",
      "[DEBUG]2020-06-19 13:11:54,270:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:54,272:utils:itr: 128, num_batch: 128, last loss: 1.132812, smooth_loss: 1.170265\n",
      "[DEBUG]2020-06-19 13:11:54,503:utils:loss_avg: 1.16600, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48687, mc_score:-0.04723\n",
      "[DEBUG]2020-06-19 13:11:55,266:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:55,268:utils:itr: 129, num_batch: 129, last loss: 1.343750, smooth_loss: 1.174005\n",
      "[DEBUG]2020-06-19 13:11:55,493:utils:loss_avg: 1.16737, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48678, mc_score:-0.04914\n",
      "[DEBUG]2020-06-19 13:11:56,257:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:56,259:utils:itr: 130, num_batch: 130, last loss: 0.953125, smooth_loss: 1.169251\n",
      "[DEBUG]2020-06-19 13:11:56,489:utils:loss_avg: 1.16573, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48784, mc_score:-0.04807\n",
      "[DEBUG]2020-06-19 13:11:57,319:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:57,320:utils:itr: 131, num_batch: 131, last loss: 1.187500, smooth_loss: 1.169643\n",
      "[DEBUG]2020-06-19 13:11:57,550:utils:loss_avg: 1.16590, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48684, mc_score:-0.04751\n",
      "[DEBUG]2020-06-19 13:11:58,311:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:58,313:utils:itr: 132, num_batch: 132, last loss: 1.195312, smooth_loss: 1.170194\n",
      "[DEBUG]2020-06-19 13:11:58,543:utils:loss_avg: 1.16612, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48648, mc_score:-0.04714\n",
      "[DEBUG]2020-06-19 13:11:59,306:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:11:59,308:utils:itr: 133, num_batch: 133, last loss: 1.062500, smooth_loss: 1.167886\n",
      "[DEBUG]2020-06-19 13:11:59,536:utils:loss_avg: 1.16535, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48843, mc_score:-0.04400\n",
      "[DEBUG]2020-06-19 13:12:00,298:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:00,300:utils:itr: 134, num_batch: 134, last loss: 1.226562, smooth_loss: 1.169142\n",
      "[DEBUG]2020-06-19 13:12:00,530:utils:loss_avg: 1.16580, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48741, mc_score:-0.04763\n",
      "[DEBUG]2020-06-19 13:12:01,379:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:01,380:utils:itr: 135, num_batch: 135, last loss: 1.257812, smooth_loss: 1.171037\n",
      "[DEBUG]2020-06-19 13:12:01,607:utils:loss_avg: 1.16648, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48739, mc_score:-0.04775\n",
      "[DEBUG]2020-06-19 13:12:02,370:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:02,371:utils:itr: 136, num_batch: 136, last loss: 1.265625, smooth_loss: 1.173055\n",
      "[DEBUG]2020-06-19 13:12:02,596:utils:loss_avg: 1.16720, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48804, mc_score:-0.04704\n",
      "[DEBUG]2020-06-19 13:12:03,355:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:03,356:utils:itr: 137, num_batch: 137, last loss: 1.406250, smooth_loss: 1.178025\n",
      "[DEBUG]2020-06-19 13:12:03,585:utils:loss_avg: 1.16893, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48752, mc_score:-0.04598\n",
      "[DEBUG]2020-06-19 13:12:04,347:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:04,349:utils:itr: 138, num_batch: 138, last loss: 1.046875, smooth_loss: 1.175234\n",
      "[DEBUG]2020-06-19 13:12:04,576:utils:loss_avg: 1.16805, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48828, mc_score:-0.04281\n",
      "[DEBUG]2020-06-19 13:12:05,411:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:05,413:utils:itr: 139, num_batch: 139, last loss: 1.132812, smooth_loss: 1.174332\n",
      "[DEBUG]2020-06-19 13:12:05,636:utils:loss_avg: 1.16780, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48844, mc_score:-0.04314\n",
      "[DEBUG]2020-06-19 13:12:06,395:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:06,397:utils:itr: 140, num_batch: 140, last loss: 1.210938, smooth_loss: 1.175109\n",
      "[DEBUG]2020-06-19 13:12:06,626:utils:loss_avg: 1.16811, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48758, mc_score:-0.04694\n",
      "[DEBUG]2020-06-19 13:12:07,388:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:07,389:utils:itr: 141, num_batch: 141, last loss: 1.132812, smooth_loss: 1.174212\n",
      "[DEBUG]2020-06-19 13:12:07,616:utils:loss_avg: 1.16786, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48724, mc_score:-0.05003\n",
      "[DEBUG]2020-06-19 13:12:08,379:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:08,380:utils:itr: 142, num_batch: 142, last loss: 1.179688, smooth_loss: 1.174328\n",
      "[DEBUG]2020-06-19 13:12:08,608:utils:loss_avg: 1.16794, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48719, mc_score:-0.05402\n",
      "[DEBUG]2020-06-19 13:12:09,429:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:09,430:utils:itr: 143, num_batch: 143, last loss: 0.984375, smooth_loss: 1.170310\n",
      "[DEBUG]2020-06-19 13:12:09,658:utils:loss_avg: 1.16667, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48727, mc_score:-0.05412\n",
      "[DEBUG]2020-06-19 13:12:10,420:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:10,422:utils:itr: 144, num_batch: 144, last loss: 1.289062, smooth_loss: 1.172819\n",
      "[DEBUG]2020-06-19 13:12:10,652:utils:loss_avg: 1.16751, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48593, mc_score:-0.05374\n",
      "[DEBUG]2020-06-19 13:12:11,417:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:11,419:utils:itr: 145, num_batch: 145, last loss: 1.039062, smooth_loss: 1.169996\n",
      "[DEBUG]2020-06-19 13:12:11,645:utils:loss_avg: 1.16663, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48709, mc_score:-0.05178\n",
      "[DEBUG]2020-06-19 13:12:12,407:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:12,408:utils:itr: 146, num_batch: 146, last loss: 0.898438, smooth_loss: 1.164271\n",
      "[DEBUG]2020-06-19 13:12:12,631:utils:loss_avg: 1.16481, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48653, mc_score:-0.05255\n",
      "[DEBUG]2020-06-19 13:12:13,457:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:13,458:utils:itr: 147, num_batch: 147, last loss: 1.335938, smooth_loss: 1.167886\n",
      "[DEBUG]2020-06-19 13:12:13,685:utils:loss_avg: 1.16596, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48632, mc_score:-0.05170\n",
      "[DEBUG]2020-06-19 13:12:14,450:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:14,452:utils:itr: 148, num_batch: 148, last loss: 1.328125, smooth_loss: 1.171257\n",
      "[DEBUG]2020-06-19 13:12:14,673:utils:loss_avg: 1.16705, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48473, mc_score:-0.05396\n",
      "[DEBUG]2020-06-19 13:12:15,433:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:15,434:utils:itr: 149, num_batch: 149, last loss: 1.101562, smooth_loss: 1.169793\n",
      "[DEBUG]2020-06-19 13:12:15,662:utils:loss_avg: 1.16661, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48482, mc_score:-0.05344\n",
      "[DEBUG]2020-06-19 13:12:16,424:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:16,426:utils:itr: 150, num_batch: 150, last loss: 0.988281, smooth_loss: 1.165982\n",
      "[DEBUG]2020-06-19 13:12:16,649:utils:loss_avg: 1.16543, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48442, mc_score:-0.05400\n",
      "[DEBUG]2020-06-19 13:12:17,472:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:17,474:utils:itr: 151, num_batch: 151, last loss: 1.171875, smooth_loss: 1.166106\n",
      "[DEBUG]2020-06-19 13:12:17,700:utils:loss_avg: 1.16548, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48409, mc_score:-0.05197\n",
      "[DEBUG]2020-06-19 13:12:18,463:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:18,464:utils:itr: 152, num_batch: 152, last loss: 1.250000, smooth_loss: 1.167864\n",
      "[DEBUG]2020-06-19 13:12:18,691:utils:loss_avg: 1.16603, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48278, mc_score:-0.05520\n",
      "[DEBUG]2020-06-19 13:12:19,452:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:19,454:utils:itr: 153, num_batch: 153, last loss: 0.941406, smooth_loss: 1.163123\n",
      "[DEBUG]2020-06-19 13:12:19,682:utils:loss_avg: 1.16457, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48289, mc_score:-0.05411\n",
      "[DEBUG]2020-06-19 13:12:20,448:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:20,449:utils:itr: 154, num_batch: 154, last loss: 0.996094, smooth_loss: 1.159630\n",
      "[DEBUG]2020-06-19 13:12:20,674:utils:loss_avg: 1.16348, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48353, mc_score:-0.05421\n",
      "[DEBUG]2020-06-19 13:12:21,525:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:21,527:utils:itr: 155, num_batch: 155, last loss: 1.031250, smooth_loss: 1.156948\n",
      "[DEBUG]2020-06-19 13:12:21,755:utils:loss_avg: 1.16264, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48367, mc_score:-0.05455\n",
      "[DEBUG]2020-06-19 13:12:22,513:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:22,515:utils:itr: 156, num_batch: 156, last loss: 0.980469, smooth_loss: 1.153264\n",
      "[DEBUG]2020-06-19 13:12:22,742:utils:loss_avg: 1.16147, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48295, mc_score:-0.05550\n",
      "[DEBUG]2020-06-19 13:12:23,504:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:23,505:utils:itr: 157, num_batch: 157, last loss: 1.007812, smooth_loss: 1.150230\n",
      "[DEBUG]2020-06-19 13:12:23,732:utils:loss_avg: 1.16050, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48240, mc_score:-0.05581\n",
      "[DEBUG]2020-06-19 13:12:24,492:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:24,494:utils:itr: 158, num_batch: 158, last loss: 1.148438, smooth_loss: 1.150193\n",
      "[DEBUG]2020-06-19 13:12:24,722:utils:loss_avg: 1.16043, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48186, mc_score:-0.05477\n",
      "[DEBUG]2020-06-19 13:12:25,706:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:25,708:utils:itr: 159, num_batch: 159, last loss: 1.156250, smooth_loss: 1.150319\n",
      "[DEBUG]2020-06-19 13:12:25,943:utils:loss_avg: 1.16040, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48115, mc_score:-0.05627\n",
      "[DEBUG]2020-06-19 13:12:26,771:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:26,773:utils:itr: 160, num_batch: 160, last loss: 1.296875, smooth_loss: 1.153368\n",
      "[DEBUG]2020-06-19 13:12:26,999:utils:loss_avg: 1.16125, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47965, mc_score:-0.05784\n",
      "[DEBUG]2020-06-19 13:12:27,764:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:27,765:utils:itr: 161, num_batch: 161, last loss: 1.148438, smooth_loss: 1.153266\n",
      "[DEBUG]2020-06-19 13:12:27,995:utils:loss_avg: 1.16117, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47948, mc_score:-0.05508\n",
      "[DEBUG]2020-06-19 13:12:28,759:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:28,761:utils:itr: 162, num_batch: 162, last loss: 1.179688, smooth_loss: 1.153815\n",
      "[DEBUG]2020-06-19 13:12:28,988:utils:loss_avg: 1.16128, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48012, mc_score:-0.05248\n",
      "[DEBUG]2020-06-19 13:12:29,830:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:29,831:utils:itr: 163, num_batch: 163, last loss: 1.484375, smooth_loss: 1.160676\n",
      "[DEBUG]2020-06-19 13:12:30,061:utils:loss_avg: 1.16325, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47892, mc_score:-0.05327\n",
      "[DEBUG]2020-06-19 13:12:30,825:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:30,827:utils:itr: 164, num_batch: 164, last loss: 1.218750, smooth_loss: 1.161880\n",
      "[DEBUG]2020-06-19 13:12:31,055:utils:loss_avg: 1.16359, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47826, mc_score:-0.05430\n",
      "[DEBUG]2020-06-19 13:12:31,817:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:31,819:utils:itr: 165, num_batch: 165, last loss: 1.242188, smooth_loss: 1.163544\n",
      "[DEBUG]2020-06-19 13:12:32,044:utils:loss_avg: 1.16406, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47851, mc_score:-0.05441\n",
      "[DEBUG]2020-06-19 13:12:32,812:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:32,814:utils:itr: 166, num_batch: 166, last loss: 0.882812, smooth_loss: 1.157731\n",
      "[DEBUG]2020-06-19 13:12:33,039:utils:loss_avg: 1.16238, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47922, mc_score:-0.05106\n",
      "[DEBUG]2020-06-19 13:12:33,906:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:33,907:utils:itr: 167, num_batch: 167, last loss: 1.179688, smooth_loss: 1.158185\n",
      "[DEBUG]2020-06-19 13:12:34,138:utils:loss_avg: 1.16248, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47990, mc_score:-0.05252\n",
      "[DEBUG]2020-06-19 13:12:34,900:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:34,901:utils:itr: 168, num_batch: 168, last loss: 1.078125, smooth_loss: 1.156529\n",
      "[DEBUG]2020-06-19 13:12:35,130:utils:loss_avg: 1.16198, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47923, mc_score:-0.05394\n",
      "[DEBUG]2020-06-19 13:12:35,895:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:35,897:utils:itr: 169, num_batch: 169, last loss: 1.210938, smooth_loss: 1.157654\n",
      "[DEBUG]2020-06-19 13:12:36,130:utils:loss_avg: 1.16227, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47976, mc_score:-0.05216\n",
      "[DEBUG]2020-06-19 13:12:36,890:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:36,891:utils:itr: 170, num_batch: 170, last loss: 1.367188, smooth_loss: 1.161981\n",
      "[DEBUG]2020-06-19 13:12:37,117:utils:loss_avg: 1.16347, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47765, mc_score:-0.05499\n",
      "[DEBUG]2020-06-19 13:12:37,943:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:37,945:utils:itr: 171, num_batch: 171, last loss: 1.023438, smooth_loss: 1.159122\n",
      "[DEBUG]2020-06-19 13:12:38,171:utils:loss_avg: 1.16265, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47761, mc_score:-0.05636\n",
      "[DEBUG]2020-06-19 13:12:38,932:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:38,934:utils:itr: 172, num_batch: 172, last loss: 0.917969, smooth_loss: 1.154148\n",
      "[DEBUG]2020-06-19 13:12:39,161:utils:loss_avg: 1.16124, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47791, mc_score:-0.05754\n",
      "[DEBUG]2020-06-19 13:12:39,920:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:39,921:utils:itr: 173, num_batch: 173, last loss: 0.933594, smooth_loss: 1.149602\n",
      "[DEBUG]2020-06-19 13:12:40,151:utils:loss_avg: 1.15993, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47859, mc_score:-0.05667\n",
      "[DEBUG]2020-06-19 13:12:40,911:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:40,913:utils:itr: 174, num_batch: 174, last loss: 1.164062, smooth_loss: 1.149899\n",
      "[DEBUG]2020-06-19 13:12:41,140:utils:loss_avg: 1.15996, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47808, mc_score:-0.05651\n",
      "[DEBUG]2020-06-19 13:12:41,966:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:41,967:utils:itr: 175, num_batch: 175, last loss: 1.226562, smooth_loss: 1.151478\n",
      "[DEBUG]2020-06-19 13:12:42,195:utils:loss_avg: 1.16033, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47795, mc_score:-0.05570\n",
      "[DEBUG]2020-06-19 13:12:42,954:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:42,956:utils:itr: 176, num_batch: 176, last loss: 1.187500, smooth_loss: 1.152219\n",
      "[DEBUG]2020-06-19 13:12:43,183:utils:loss_avg: 1.16049, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47868, mc_score:-0.05564\n",
      "[DEBUG]2020-06-19 13:12:43,943:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:43,945:utils:itr: 177, num_batch: 177, last loss: 1.585938, smooth_loss: 1.161138\n",
      "[DEBUG]2020-06-19 13:12:44,173:utils:loss_avg: 1.16288, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47820, mc_score:-0.05562\n",
      "[DEBUG]2020-06-19 13:12:44,941:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:44,942:utils:itr: 178, num_batch: 178, last loss: 0.832031, smooth_loss: 1.154374\n",
      "[DEBUG]2020-06-19 13:12:45,167:utils:loss_avg: 1.16103, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47934, mc_score:-0.05468\n",
      "[DEBUG]2020-06-19 13:12:46,011:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:46,013:utils:itr: 179, num_batch: 179, last loss: 1.023438, smooth_loss: 1.151685\n",
      "[DEBUG]2020-06-19 13:12:46,240:utils:loss_avg: 1.16026, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48100, mc_score:-0.05249\n",
      "[DEBUG]2020-06-19 13:12:47,000:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:47,002:utils:itr: 180, num_batch: 180, last loss: 0.972656, smooth_loss: 1.148009\n",
      "[DEBUG]2020-06-19 13:12:47,230:utils:loss_avg: 1.15923, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48205, mc_score:-0.05168\n",
      "[DEBUG]2020-06-19 13:12:47,992:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:47,993:utils:itr: 181, num_batch: 181, last loss: 0.960938, smooth_loss: 1.144171\n",
      "[DEBUG]2020-06-19 13:12:48,223:utils:loss_avg: 1.15814, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48237, mc_score:-0.05202\n",
      "[DEBUG]2020-06-19 13:12:48,983:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:48,985:utils:itr: 182, num_batch: 182, last loss: 1.070312, smooth_loss: 1.142656\n",
      "[DEBUG]2020-06-19 13:12:49,211:utils:loss_avg: 1.15766, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48277, mc_score:-0.05147\n",
      "[DEBUG]2020-06-19 13:12:50,104:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:50,105:utils:itr: 183, num_batch: 183, last loss: 1.046875, smooth_loss: 1.140693\n",
      "[DEBUG]2020-06-19 13:12:50,332:utils:loss_avg: 1.15706, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48206, mc_score:-0.05238\n",
      "[DEBUG]2020-06-19 13:12:51,094:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:51,096:utils:itr: 184, num_batch: 184, last loss: 1.171875, smooth_loss: 1.141332\n",
      "[DEBUG]2020-06-19 13:12:51,324:utils:loss_avg: 1.15714, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48190, mc_score:-0.05392\n",
      "[DEBUG]2020-06-19 13:12:52,084:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:52,086:utils:itr: 185, num_batch: 185, last loss: 1.085938, smooth_loss: 1.140197\n",
      "[DEBUG]2020-06-19 13:12:52,313:utils:loss_avg: 1.15675, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48205, mc_score:-0.05338\n",
      "[DEBUG]2020-06-19 13:12:53,073:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:53,075:utils:itr: 186, num_batch: 186, last loss: 1.078125, smooth_loss: 1.138927\n",
      "[DEBUG]2020-06-19 13:12:53,304:utils:loss_avg: 1.15633, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48307, mc_score:-0.05103\n",
      "[DEBUG]2020-06-19 13:12:54,259:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:54,260:utils:itr: 187, num_batch: 187, last loss: 1.210938, smooth_loss: 1.140400\n",
      "[DEBUG]2020-06-19 13:12:54,492:utils:loss_avg: 1.15662, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48326, mc_score:-0.05100\n",
      "[DEBUG]2020-06-19 13:12:55,251:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:55,253:utils:itr: 188, num_batch: 188, last loss: 1.445312, smooth_loss: 1.146635\n",
      "[DEBUG]2020-06-19 13:12:55,484:utils:loss_avg: 1.15815, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48272, mc_score:-0.05051\n",
      "[DEBUG]2020-06-19 13:12:56,244:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:56,245:utils:itr: 189, num_batch: 189, last loss: 1.171875, smooth_loss: 1.147151\n",
      "[DEBUG]2020-06-19 13:12:56,471:utils:loss_avg: 1.15822, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48239, mc_score:-0.05118\n",
      "[DEBUG]2020-06-19 13:12:57,230:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:57,232:utils:itr: 190, num_batch: 190, last loss: 1.062500, smooth_loss: 1.145422\n",
      "[DEBUG]2020-06-19 13:12:57,460:utils:loss_avg: 1.15772, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48158, mc_score:-0.05219\n",
      "[DEBUG]2020-06-19 13:12:58,306:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:58,308:utils:itr: 191, num_batch: 191, last loss: 1.132812, smooth_loss: 1.145164\n",
      "[DEBUG]2020-06-19 13:12:58,531:utils:loss_avg: 1.15759, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48232, mc_score:-0.05299\n",
      "[DEBUG]2020-06-19 13:12:59,291:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:12:59,292:utils:itr: 192, num_batch: 192, last loss: 1.187500, smooth_loss: 1.146028\n",
      "[DEBUG]2020-06-19 13:12:59,518:utils:loss_avg: 1.15775, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48240, mc_score:-0.05178\n",
      "[DEBUG]2020-06-19 13:13:00,280:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:00,281:utils:itr: 193, num_batch: 193, last loss: 1.093750, smooth_loss: 1.144961\n",
      "[DEBUG]2020-06-19 13:13:00,508:utils:loss_avg: 1.15742, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48272, mc_score:-0.05161\n",
      "[DEBUG]2020-06-19 13:13:01,269:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:01,271:utils:itr: 194, num_batch: 194, last loss: 1.500000, smooth_loss: 1.152203\n",
      "[DEBUG]2020-06-19 13:13:01,498:utils:loss_avg: 1.15917, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48154, mc_score:-0.05378\n",
      "[DEBUG]2020-06-19 13:13:02,348:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:02,349:utils:itr: 195, num_batch: 195, last loss: 1.117188, smooth_loss: 1.151489\n",
      "[DEBUG]2020-06-19 13:13:02,573:utils:loss_avg: 1.15896, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48226, mc_score:-0.05178\n",
      "[DEBUG]2020-06-19 13:13:03,329:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:03,331:utils:itr: 196, num_batch: 196, last loss: 1.265625, smooth_loss: 1.153816\n",
      "[DEBUG]2020-06-19 13:13:03,558:utils:loss_avg: 1.15950, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48162, mc_score:-0.05369\n",
      "[DEBUG]2020-06-19 13:13:04,320:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:04,321:utils:itr: 197, num_batch: 197, last loss: 1.273438, smooth_loss: 1.156253\n",
      "[DEBUG]2020-06-19 13:13:04,552:utils:loss_avg: 1.16008, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48185, mc_score:-0.05308\n",
      "[DEBUG]2020-06-19 13:13:05,313:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:05,314:utils:itr: 198, num_batch: 198, last loss: 0.968750, smooth_loss: 1.152434\n",
      "[DEBUG]2020-06-19 13:13:05,542:utils:loss_avg: 1.15912, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48267, mc_score:-0.05235\n",
      "[DEBUG]2020-06-19 13:13:06,388:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:06,389:utils:itr: 199, num_batch: 199, last loss: 1.031250, smooth_loss: 1.149967\n",
      "[DEBUG]2020-06-19 13:13:06,619:utils:loss_avg: 1.15848, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48155, mc_score:-0.05317\n",
      "[DEBUG]2020-06-19 13:13:07,383:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:07,384:utils:itr: 200, num_batch: 200, last loss: 1.046875, smooth_loss: 1.147869\n",
      "[DEBUG]2020-06-19 13:13:07,615:utils:loss_avg: 1.15792, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48179, mc_score:-0.05087\n",
      "[DEBUG]2020-06-19 13:13:08,376:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:08,378:utils:itr: 201, num_batch: 201, last loss: 1.007812, smooth_loss: 1.145020\n",
      "[DEBUG]2020-06-19 13:13:08,608:utils:loss_avg: 1.15718, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48207, mc_score:-0.05116\n",
      "[DEBUG]2020-06-19 13:13:09,365:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:09,366:utils:itr: 202, num_batch: 202, last loss: 1.203125, smooth_loss: 1.146201\n",
      "[DEBUG]2020-06-19 13:13:09,592:utils:loss_avg: 1.15740, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48198, mc_score:-0.05165\n",
      "[DEBUG]2020-06-19 13:13:10,454:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:10,455:utils:itr: 203, num_batch: 203, last loss: 1.046875, smooth_loss: 1.144182\n",
      "[DEBUG]2020-06-19 13:13:10,685:utils:loss_avg: 1.15686, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48274, mc_score:-0.05140\n",
      "[DEBUG]2020-06-19 13:13:11,451:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:11,452:utils:itr: 204, num_batch: 204, last loss: 1.187500, smooth_loss: 1.145062\n",
      "[DEBUG]2020-06-19 13:13:11,684:utils:loss_avg: 1.15701, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48279, mc_score:-0.05028\n",
      "[DEBUG]2020-06-19 13:13:12,446:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:12,447:utils:itr: 205, num_batch: 205, last loss: 1.195312, smooth_loss: 1.146083\n",
      "[DEBUG]2020-06-19 13:13:12,675:utils:loss_avg: 1.15720, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48223, mc_score:-0.05003\n",
      "[DEBUG]2020-06-19 13:13:13,441:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:13,442:utils:itr: 206, num_batch: 206, last loss: 1.226562, smooth_loss: 1.147718\n",
      "[DEBUG]2020-06-19 13:13:13,674:utils:loss_avg: 1.15753, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48160, mc_score:-0.05163\n",
      "[DEBUG]2020-06-19 13:13:14,550:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:14,552:utils:itr: 207, num_batch: 207, last loss: 1.195312, smooth_loss: 1.148684\n",
      "[DEBUG]2020-06-19 13:13:14,784:utils:loss_avg: 1.15771, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48161, mc_score:-0.05148\n",
      "[DEBUG]2020-06-19 13:13:15,545:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:15,546:utils:itr: 208, num_batch: 208, last loss: 1.281250, smooth_loss: 1.151375\n",
      "[DEBUG]2020-06-19 13:13:15,775:utils:loss_avg: 1.15831, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48098, mc_score:-0.05158\n",
      "[DEBUG]2020-06-19 13:13:16,538:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:16,540:utils:itr: 209, num_batch: 209, last loss: 1.265625, smooth_loss: 1.153693\n",
      "[DEBUG]2020-06-19 13:13:16,763:utils:loss_avg: 1.15882, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48039, mc_score:-0.05252\n",
      "[DEBUG]2020-06-19 13:13:17,522:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:17,523:utils:itr: 210, num_batch: 210, last loss: 1.070312, smooth_loss: 1.152002\n",
      "[DEBUG]2020-06-19 13:13:17,747:utils:loss_avg: 1.15840, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48088, mc_score:-0.05143\n",
      "[DEBUG]2020-06-19 13:13:18,606:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:18,607:utils:itr: 211, num_batch: 211, last loss: 1.234375, smooth_loss: 1.153673\n",
      "[DEBUG]2020-06-19 13:13:18,833:utils:loss_avg: 1.15876, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48068, mc_score:-0.05056\n",
      "[DEBUG]2020-06-19 13:13:19,593:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:19,594:utils:itr: 212, num_batch: 212, last loss: 0.960938, smooth_loss: 1.149765\n",
      "[DEBUG]2020-06-19 13:13:19,822:utils:loss_avg: 1.15783, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.47985, mc_score:-0.05207\n",
      "[DEBUG]2020-06-19 13:13:20,583:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:20,585:utils:itr: 213, num_batch: 213, last loss: 1.109375, smooth_loss: 1.148946\n",
      "[DEBUG]2020-06-19 13:13:20,814:utils:loss_avg: 1.15760, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48060, mc_score:-0.05109\n",
      "[DEBUG]2020-06-19 13:13:21,578:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:21,580:utils:itr: 214, num_batch: 214, last loss: 0.890625, smooth_loss: 1.143712\n",
      "[DEBUG]2020-06-19 13:13:21,819:utils:loss_avg: 1.15636, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48135, mc_score:-0.04886\n",
      "[DEBUG]2020-06-19 13:13:22,665:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:22,666:utils:itr: 215, num_batch: 215, last loss: 0.976562, smooth_loss: 1.140326\n",
      "[DEBUG]2020-06-19 13:13:22,904:utils:loss_avg: 1.15553, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48226, mc_score:-0.04759\n",
      "[DEBUG]2020-06-19 13:13:23,667:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:23,668:utils:itr: 216, num_batch: 216, last loss: 1.273438, smooth_loss: 1.143022\n",
      "[DEBUG]2020-06-19 13:13:23,905:utils:loss_avg: 1.15607, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48192, mc_score:-0.04841\n",
      "[DEBUG]2020-06-19 13:13:24,671:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:24,672:utils:itr: 217, num_batch: 217, last loss: 1.007812, smooth_loss: 1.140284\n",
      "[DEBUG]2020-06-19 13:13:24,910:utils:loss_avg: 1.15539, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48210, mc_score:-0.04847\n",
      "[DEBUG]2020-06-19 13:13:25,675:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:25,677:utils:itr: 218, num_batch: 218, last loss: 1.101562, smooth_loss: 1.139500\n",
      "[DEBUG]2020-06-19 13:13:25,914:utils:loss_avg: 1.15514, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48241, mc_score:-0.04661\n",
      "[DEBUG]2020-06-19 13:13:26,770:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:26,772:utils:itr: 219, num_batch: 219, last loss: 1.390625, smooth_loss: 1.144583\n",
      "[DEBUG]2020-06-19 13:13:27,014:utils:loss_avg: 1.15621, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48148, mc_score:-0.04915\n",
      "[DEBUG]2020-06-19 13:13:27,776:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:27,777:utils:itr: 220, num_batch: 220, last loss: 1.093750, smooth_loss: 1.143554\n",
      "[DEBUG]2020-06-19 13:13:28,015:utils:loss_avg: 1.15593, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48207, mc_score:-0.04904\n",
      "[DEBUG]2020-06-19 13:13:28,782:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:28,783:utils:itr: 221, num_batch: 221, last loss: 0.960938, smooth_loss: 1.139860\n",
      "[DEBUG]2020-06-19 13:13:29,022:utils:loss_avg: 1.15505, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48291, mc_score:-0.04978\n",
      "[DEBUG]2020-06-19 13:13:29,788:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:29,789:utils:itr: 222, num_batch: 222, last loss: 1.164062, smooth_loss: 1.140350\n",
      "[DEBUG]2020-06-19 13:13:30,031:utils:loss_avg: 1.15509, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48410, mc_score:-0.04848\n",
      "[DEBUG]2020-06-19 13:13:30,858:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:30,859:utils:itr: 223, num_batch: 223, last loss: 0.949219, smooth_loss: 1.136485\n",
      "[DEBUG]2020-06-19 13:13:31,100:utils:loss_avg: 1.15417, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48448, mc_score:-0.05033\n",
      "[DEBUG]2020-06-19 13:13:31,868:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:31,869:utils:itr: 224, num_batch: 224, last loss: 1.195312, smooth_loss: 1.137674\n",
      "[DEBUG]2020-06-19 13:13:32,102:utils:loss_avg: 1.15436, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48410, mc_score:-0.05080\n",
      "[DEBUG]2020-06-19 13:13:32,863:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:32,864:utils:itr: 225, num_batch: 225, last loss: 1.343750, smooth_loss: 1.141839\n",
      "[DEBUG]2020-06-19 13:13:33,093:utils:loss_avg: 1.15520, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48439, mc_score:-0.05131\n",
      "[DEBUG]2020-06-19 13:13:33,854:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:33,855:utils:itr: 226, num_batch: 226, last loss: 1.281250, smooth_loss: 1.144656\n",
      "[DEBUG]2020-06-19 13:13:34,083:utils:loss_avg: 1.15575, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48407, mc_score:-0.05141\n",
      "[DEBUG]2020-06-19 13:13:34,922:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:34,924:utils:itr: 227, num_batch: 227, last loss: 1.109375, smooth_loss: 1.143943\n",
      "[DEBUG]2020-06-19 13:13:35,149:utils:loss_avg: 1.15555, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48473, mc_score:-0.04930\n",
      "[DEBUG]2020-06-19 13:13:35,912:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:35,914:utils:itr: 228, num_batch: 228, last loss: 1.210938, smooth_loss: 1.145297\n",
      "[DEBUG]2020-06-19 13:13:36,144:utils:loss_avg: 1.15579, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48405, mc_score:-0.05075\n",
      "[DEBUG]2020-06-19 13:13:36,903:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:36,904:utils:itr: 229, num_batch: 229, last loss: 1.023438, smooth_loss: 1.142836\n",
      "[DEBUG]2020-06-19 13:13:37,134:utils:loss_avg: 1.15521, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48442, mc_score:-0.05101\n",
      "[DEBUG]2020-06-19 13:13:37,901:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:37,903:utils:itr: 230, num_batch: 230, last loss: 1.156250, smooth_loss: 1.143107\n",
      "[DEBUG]2020-06-19 13:13:38,131:utils:loss_avg: 1.15522, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48449, mc_score:-0.05001\n",
      "[DEBUG]2020-06-19 13:13:38,981:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:38,983:utils:itr: 231, num_batch: 231, last loss: 1.132812, smooth_loss: 1.142899\n",
      "[DEBUG]2020-06-19 13:13:39,216:utils:loss_avg: 1.15512, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48502, mc_score:-0.04999\n",
      "[DEBUG]2020-06-19 13:13:39,975:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:39,977:utils:itr: 232, num_batch: 232, last loss: 1.023438, smooth_loss: 1.140488\n",
      "[DEBUG]2020-06-19 13:13:40,205:utils:loss_avg: 1.15456, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48461, mc_score:-0.05102\n",
      "[DEBUG]2020-06-19 13:13:40,971:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:40,973:utils:itr: 233, num_batch: 233, last loss: 1.281250, smooth_loss: 1.143328\n",
      "[DEBUG]2020-06-19 13:13:41,204:utils:loss_avg: 1.15510, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48471, mc_score:-0.05041\n",
      "[DEBUG]2020-06-19 13:13:41,966:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:41,967:utils:itr: 234, num_batch: 234, last loss: 1.656250, smooth_loss: 1.153677\n",
      "[DEBUG]2020-06-19 13:13:42,197:utils:loss_avg: 1.15723, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48302, mc_score:-0.05175\n",
      "[DEBUG]2020-06-19 13:13:43,038:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:43,039:utils:itr: 235, num_batch: 235, last loss: 1.273438, smooth_loss: 1.156092\n",
      "[DEBUG]2020-06-19 13:13:43,271:utils:loss_avg: 1.15772, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48202, mc_score:-0.05239\n",
      "[DEBUG]2020-06-19 13:13:44,033:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:44,034:utils:itr: 236, num_batch: 236, last loss: 0.792969, smooth_loss: 1.148769\n",
      "[DEBUG]2020-06-19 13:13:44,262:utils:loss_avg: 1.15618, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48358, mc_score:-0.04911\n",
      "[DEBUG]2020-06-19 13:13:45,023:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:45,025:utils:itr: 237, num_batch: 237, last loss: 1.078125, smooth_loss: 1.147344\n",
      "[DEBUG]2020-06-19 13:13:45,251:utils:loss_avg: 1.15586, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48471, mc_score:-0.04749\n",
      "[DEBUG]2020-06-19 13:13:46,010:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:46,012:utils:itr: 238, num_batch: 238, last loss: 1.156250, smooth_loss: 1.147524\n",
      "[DEBUG]2020-06-19 13:13:46,242:utils:loss_avg: 1.15586, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48482, mc_score:-0.04654\n",
      "[DEBUG]2020-06-19 13:13:47,116:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:47,117:utils:itr: 239, num_batch: 239, last loss: 1.046875, smooth_loss: 1.145495\n",
      "[DEBUG]2020-06-19 13:13:47,344:utils:loss_avg: 1.15540, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48585, mc_score:-0.04570\n",
      "[DEBUG]2020-06-19 13:13:48,105:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:48,106:utils:itr: 240, num_batch: 240, last loss: 1.406250, smooth_loss: 1.150751\n",
      "[DEBUG]2020-06-19 13:13:48,334:utils:loss_avg: 1.15644, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48491, mc_score:-0.04570\n",
      "[DEBUG]2020-06-19 13:13:49,093:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:49,094:utils:itr: 241, num_batch: 241, last loss: 1.203125, smooth_loss: 1.151806\n",
      "[DEBUG]2020-06-19 13:13:49,326:utils:loss_avg: 1.15664, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48453, mc_score:-0.04551\n",
      "[DEBUG]2020-06-19 13:13:50,090:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:50,092:utils:itr: 242, num_batch: 242, last loss: 1.007812, smooth_loss: 1.148905\n",
      "[DEBUG]2020-06-19 13:13:50,322:utils:loss_avg: 1.15602, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48375, mc_score:-0.04465\n",
      "[DEBUG]2020-06-19 13:13:51,159:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:51,160:utils:itr: 243, num_batch: 243, last loss: 1.062500, smooth_loss: 1.147164\n",
      "[DEBUG]2020-06-19 13:13:51,394:utils:loss_avg: 1.15564, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48437, mc_score:-0.04373\n",
      "[DEBUG]2020-06-19 13:13:52,155:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:52,156:utils:itr: 244, num_batch: 244, last loss: 1.179688, smooth_loss: 1.147819\n",
      "[DEBUG]2020-06-19 13:13:52,385:utils:loss_avg: 1.15574, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48444, mc_score:-0.04437\n",
      "[DEBUG]2020-06-19 13:13:53,151:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:53,152:utils:itr: 245, num_batch: 245, last loss: 1.140625, smooth_loss: 1.147675\n",
      "[DEBUG]2020-06-19 13:13:53,382:utils:loss_avg: 1.15568, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48472, mc_score:-0.04492\n",
      "[DEBUG]2020-06-19 13:13:54,145:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:54,146:utils:itr: 246, num_batch: 246, last loss: 1.015625, smooth_loss: 1.145015\n",
      "[DEBUG]2020-06-19 13:13:54,383:utils:loss_avg: 1.15511, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48535, mc_score:-0.04401\n",
      "[DEBUG]2020-06-19 13:13:55,343:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:55,344:utils:itr: 247, num_batch: 247, last loss: 1.171875, smooth_loss: 1.145556\n",
      "[DEBUG]2020-06-19 13:13:55,576:utils:loss_avg: 1.15518, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48484, mc_score:-0.04455\n",
      "[DEBUG]2020-06-19 13:13:56,339:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:56,340:utils:itr: 248, num_batch: 248, last loss: 1.101562, smooth_loss: 1.144671\n",
      "[DEBUG]2020-06-19 13:13:56,566:utils:loss_avg: 1.15496, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48549, mc_score:-0.04519\n",
      "[DEBUG]2020-06-19 13:13:57,328:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:57,330:utils:itr: 249, num_batch: 249, last loss: 1.000000, smooth_loss: 1.141759\n",
      "[DEBUG]2020-06-19 13:13:57,555:utils:loss_avg: 1.15434, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48616, mc_score:-0.04563\n",
      "[DEBUG]2020-06-19 13:13:58,315:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:58,316:utils:itr: 250, num_batch: 250, last loss: 1.281250, smooth_loss: 1.144566\n",
      "[DEBUG]2020-06-19 13:13:58,546:utils:loss_avg: 1.15485, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48618, mc_score:-0.04544\n",
      "[DEBUG]2020-06-19 13:13:59,401:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:13:59,402:utils:itr: 251, num_batch: 251, last loss: 0.925781, smooth_loss: 1.140163\n",
      "[DEBUG]2020-06-19 13:13:59,635:utils:loss_avg: 1.15394, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48690, mc_score:-0.04418\n",
      "[DEBUG]2020-06-19 13:14:00,606:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:00,607:utils:itr: 252, num_batch: 252, last loss: 1.054688, smooth_loss: 1.138443\n",
      "[DEBUG]2020-06-19 13:14:00,839:utils:loss_avg: 1.15355, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48722, mc_score:-0.04481\n",
      "[DEBUG]2020-06-19 13:14:01,601:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:01,603:utils:itr: 253, num_batch: 253, last loss: 1.140625, smooth_loss: 1.138487\n",
      "[DEBUG]2020-06-19 13:14:01,831:utils:loss_avg: 1.15350, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48678, mc_score:-0.04506\n",
      "[DEBUG]2020-06-19 13:14:02,593:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:02,595:utils:itr: 254, num_batch: 254, last loss: 1.085938, smooth_loss: 1.137430\n",
      "[DEBUG]2020-06-19 13:14:02,822:utils:loss_avg: 1.15323, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48697, mc_score:-0.04470\n",
      "[DEBUG]2020-06-19 13:14:03,660:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:03,661:utils:itr: 255, num_batch: 255, last loss: 1.242188, smooth_loss: 1.139537\n",
      "[DEBUG]2020-06-19 13:14:03,893:utils:loss_avg: 1.15358, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48656, mc_score:-0.04690\n",
      "[DEBUG]2020-06-19 13:14:04,655:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:04,657:utils:itr: 256, num_batch: 256, last loss: 1.304688, smooth_loss: 1.142859\n",
      "[DEBUG]2020-06-19 13:14:04,886:utils:loss_avg: 1.15417, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48626, mc_score:-0.04700\n",
      "[DEBUG]2020-06-19 13:14:05,651:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:05,652:utils:itr: 257, num_batch: 257, last loss: 1.046875, smooth_loss: 1.140928\n",
      "[DEBUG]2020-06-19 13:14:05,883:utils:loss_avg: 1.15375, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48700, mc_score:-0.04681\n",
      "[DEBUG]2020-06-19 13:14:06,645:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:06,647:utils:itr: 258, num_batch: 258, last loss: 1.250000, smooth_loss: 1.143121\n",
      "[DEBUG]2020-06-19 13:14:06,879:utils:loss_avg: 1.15412, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48679, mc_score:-0.04759\n",
      "[DEBUG]2020-06-19 13:14:07,746:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:07,748:utils:itr: 259, num_batch: 259, last loss: 1.218750, smooth_loss: 1.144642\n",
      "[DEBUG]2020-06-19 13:14:07,975:utils:loss_avg: 1.15437, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48613, mc_score:-0.04810\n",
      "[DEBUG]2020-06-19 13:14:08,741:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:08,743:utils:itr: 260, num_batch: 260, last loss: 1.179688, smooth_loss: 1.145347\n",
      "[DEBUG]2020-06-19 13:14:08,974:utils:loss_avg: 1.15447, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48653, mc_score:-0.04679\n",
      "[DEBUG]2020-06-19 13:14:09,737:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:09,738:utils:itr: 261, num_batch: 261, last loss: 1.203125, smooth_loss: 1.146508\n",
      "[DEBUG]2020-06-19 13:14:09,972:utils:loss_avg: 1.15465, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48637, mc_score:-0.04730\n",
      "[DEBUG]2020-06-19 13:14:10,738:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:10,740:utils:itr: 262, num_batch: 262, last loss: 1.078125, smooth_loss: 1.145133\n",
      "[DEBUG]2020-06-19 13:14:10,972:utils:loss_avg: 1.15436, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48642, mc_score:-0.04634\n",
      "[DEBUG]2020-06-19 13:14:11,838:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:11,840:utils:itr: 263, num_batch: 263, last loss: 1.242188, smooth_loss: 1.147084\n",
      "[DEBUG]2020-06-19 13:14:12,072:utils:loss_avg: 1.15470, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48651, mc_score:-0.04676\n",
      "[DEBUG]2020-06-19 13:14:12,836:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:12,837:utils:itr: 264, num_batch: 264, last loss: 1.015625, smooth_loss: 1.144442\n",
      "[DEBUG]2020-06-19 13:14:13,070:utils:loss_avg: 1.15417, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48650, mc_score:-0.04673\n",
      "[DEBUG]2020-06-19 13:14:13,834:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:13,835:utils:itr: 265, num_batch: 265, last loss: 1.000000, smooth_loss: 1.141540\n",
      "[DEBUG]2020-06-19 13:14:14,062:utils:loss_avg: 1.15359, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48677, mc_score:-0.04435\n",
      "[DEBUG]2020-06-19 13:14:14,830:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:14,831:utils:itr: 266, num_batch: 266, last loss: 1.039062, smooth_loss: 1.139481\n",
      "[DEBUG]2020-06-19 13:14:15,067:utils:loss_avg: 1.15316, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48674, mc_score:-0.04500\n",
      "[DEBUG]2020-06-19 13:14:15,916:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:15,917:utils:itr: 267, num_batch: 267, last loss: 1.187500, smooth_loss: 1.140446\n",
      "[DEBUG]2020-06-19 13:14:16,151:utils:loss_avg: 1.15329, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48714, mc_score:-0.04274\n",
      "[DEBUG]2020-06-19 13:14:16,916:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:16,918:utils:itr: 268, num_batch: 268, last loss: 1.132812, smooth_loss: 1.140292\n",
      "[DEBUG]2020-06-19 13:14:17,152:utils:loss_avg: 1.15322, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48754, mc_score:-0.04058\n",
      "[DEBUG]2020-06-19 13:14:17,919:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:17,920:utils:itr: 269, num_batch: 269, last loss: 1.203125, smooth_loss: 1.141555\n",
      "[DEBUG]2020-06-19 13:14:18,153:utils:loss_avg: 1.15340, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48825, mc_score:-0.04028\n",
      "[DEBUG]2020-06-19 13:14:18,918:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:18,919:utils:itr: 270, num_batch: 270, last loss: 1.234375, smooth_loss: 1.143419\n",
      "[DEBUG]2020-06-19 13:14:19,147:utils:loss_avg: 1.15370, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48871, mc_score:-0.03890\n",
      "[DEBUG]2020-06-19 13:14:20,014:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:20,016:utils:itr: 271, num_batch: 271, last loss: 1.359375, smooth_loss: 1.147756\n",
      "[DEBUG]2020-06-19 13:14:20,249:utils:loss_avg: 1.15445, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48843, mc_score:-0.03967\n",
      "[DEBUG]2020-06-19 13:14:21,014:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:21,015:utils:itr: 272, num_batch: 272, last loss: 1.132812, smooth_loss: 1.147456\n",
      "[DEBUG]2020-06-19 13:14:21,246:utils:loss_avg: 1.15438, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48810, mc_score:-0.03942\n",
      "[DEBUG]2020-06-19 13:14:22,012:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:22,014:utils:itr: 273, num_batch: 273, last loss: 1.093750, smooth_loss: 1.146377\n",
      "[DEBUG]2020-06-19 13:14:22,245:utils:loss_avg: 1.15415, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48818, mc_score:-0.03788\n",
      "[DEBUG]2020-06-19 13:14:23,012:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:23,014:utils:itr: 274, num_batch: 274, last loss: 1.328125, smooth_loss: 1.150026\n",
      "[DEBUG]2020-06-19 13:14:23,244:utils:loss_avg: 1.15479, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48806, mc_score:-0.03957\n",
      "[DEBUG]2020-06-19 13:14:24,101:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:24,103:utils:itr: 275, num_batch: 275, last loss: 1.046875, smooth_loss: 1.147956\n",
      "[DEBUG]2020-06-19 13:14:24,333:utils:loss_avg: 1.15440, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48861, mc_score:-0.03942\n",
      "[DEBUG]2020-06-19 13:14:25,094:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:25,096:utils:itr: 276, num_batch: 276, last loss: 1.140625, smooth_loss: 1.147808\n",
      "[DEBUG]2020-06-19 13:14:25,324:utils:loss_avg: 1.15435, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48914, mc_score:-0.04066\n",
      "[DEBUG]2020-06-19 13:14:26,092:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:26,094:utils:itr: 277, num_batch: 277, last loss: 1.437500, smooth_loss: 1.153623\n",
      "[DEBUG]2020-06-19 13:14:26,327:utils:loss_avg: 1.15536, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48819, mc_score:-0.04133\n",
      "[DEBUG]2020-06-19 13:14:27,095:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:27,096:utils:itr: 278, num_batch: 278, last loss: 1.289062, smooth_loss: 1.156342\n",
      "[DEBUG]2020-06-19 13:14:27,327:utils:loss_avg: 1.15584, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48796, mc_score:-0.04190\n",
      "[DEBUG]2020-06-19 13:14:28,175:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:28,177:utils:itr: 279, num_batch: 279, last loss: 0.921875, smooth_loss: 1.151636\n",
      "[DEBUG]2020-06-19 13:14:28,402:utils:loss_avg: 1.15501, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48828, mc_score:-0.04132\n",
      "[DEBUG]2020-06-19 13:14:29,170:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:29,172:utils:itr: 280, num_batch: 280, last loss: 1.039062, smooth_loss: 1.149377\n",
      "[DEBUG]2020-06-19 13:14:29,404:utils:loss_avg: 1.15460, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48770, mc_score:-0.04075\n",
      "[DEBUG]2020-06-19 13:14:30,176:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:30,178:utils:itr: 281, num_batch: 281, last loss: 1.023438, smooth_loss: 1.146850\n",
      "[DEBUG]2020-06-19 13:14:30,410:utils:loss_avg: 1.15413, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48794, mc_score:-0.03965\n",
      "[DEBUG]2020-06-19 13:14:31,181:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:31,183:utils:itr: 282, num_batch: 282, last loss: 1.242188, smooth_loss: 1.148763\n",
      "[DEBUG]2020-06-19 13:14:31,413:utils:loss_avg: 1.15444, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48789, mc_score:-0.03958\n",
      "[DEBUG]2020-06-19 13:14:32,271:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:32,273:utils:itr: 283, num_batch: 283, last loss: 1.179688, smooth_loss: 1.149383\n",
      "[DEBUG]2020-06-19 13:14:32,505:utils:loss_avg: 1.15453, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48805, mc_score:-0.03913\n",
      "[DEBUG]2020-06-19 13:14:33,274:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:33,276:utils:itr: 284, num_batch: 284, last loss: 1.296875, smooth_loss: 1.152343\n",
      "[DEBUG]2020-06-19 13:14:33,506:utils:loss_avg: 1.15503, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48753, mc_score:-0.03939\n",
      "[DEBUG]2020-06-19 13:14:34,276:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:34,277:utils:itr: 285, num_batch: 285, last loss: 1.000000, smooth_loss: 1.149286\n",
      "[DEBUG]2020-06-19 13:14:34,501:utils:loss_avg: 1.15449, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48791, mc_score:-0.03963\n",
      "[DEBUG]2020-06-19 13:14:35,271:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:35,273:utils:itr: 286, num_batch: 286, last loss: 1.257812, smooth_loss: 1.151464\n",
      "[DEBUG]2020-06-19 13:14:35,503:utils:loss_avg: 1.15485, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48830, mc_score:-0.03873\n",
      "[DEBUG]2020-06-19 13:14:36,376:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:36,378:utils:itr: 287, num_batch: 287, last loss: 1.187500, smooth_loss: 1.152186\n",
      "[DEBUG]2020-06-19 13:14:36,609:utils:loss_avg: 1.15496, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48813, mc_score:-0.03743\n",
      "[DEBUG]2020-06-19 13:14:37,378:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:37,379:utils:itr: 288, num_batch: 288, last loss: 1.250000, smooth_loss: 1.154148\n",
      "[DEBUG]2020-06-19 13:14:37,610:utils:loss_avg: 1.15529, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48783, mc_score:-0.03749\n",
      "[DEBUG]2020-06-19 13:14:38,375:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:38,378:utils:itr: 289, num_batch: 289, last loss: 1.257812, smooth_loss: 1.156228\n",
      "[DEBUG]2020-06-19 13:14:38,608:utils:loss_avg: 1.15564, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48755, mc_score:-0.03742\n",
      "[DEBUG]2020-06-19 13:14:39,375:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:39,376:utils:itr: 290, num_batch: 290, last loss: 1.382812, smooth_loss: 1.160772\n",
      "[DEBUG]2020-06-19 13:14:39,610:utils:loss_avg: 1.15642, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48678, mc_score:-0.03814\n",
      "[DEBUG]2020-06-19 13:14:40,459:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:40,460:utils:itr: 291, num_batch: 291, last loss: 1.203125, smooth_loss: 1.161621\n",
      "[DEBUG]2020-06-19 13:14:40,692:utils:loss_avg: 1.15658, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48612, mc_score:-0.03838\n",
      "[DEBUG]2020-06-19 13:14:41,459:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:41,461:utils:itr: 292, num_batch: 292, last loss: 1.312500, smooth_loss: 1.164647\n",
      "[DEBUG]2020-06-19 13:14:41,692:utils:loss_avg: 1.15712, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48590, mc_score:-0.03810\n",
      "[DEBUG]2020-06-19 13:14:42,459:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:42,461:utils:itr: 293, num_batch: 293, last loss: 1.226562, smooth_loss: 1.165889\n",
      "[DEBUG]2020-06-19 13:14:42,697:utils:loss_avg: 1.15735, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48541, mc_score:-0.03989\n",
      "[DEBUG]2020-06-19 13:14:43,468:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:43,469:utils:itr: 294, num_batch: 294, last loss: 1.195312, smooth_loss: 1.166479\n",
      "[DEBUG]2020-06-19 13:14:43,703:utils:loss_avg: 1.15748, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48550, mc_score:-0.03966\n",
      "[DEBUG]2020-06-19 13:14:44,549:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:44,551:utils:itr: 295, num_batch: 295, last loss: 0.738281, smooth_loss: 1.157893\n",
      "[DEBUG]2020-06-19 13:14:44,785:utils:loss_avg: 1.15607, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48622, mc_score:-0.03815\n",
      "[DEBUG]2020-06-19 13:14:45,557:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:45,558:utils:itr: 296, num_batch: 296, last loss: 0.957031, smooth_loss: 1.153866\n",
      "[DEBUG]2020-06-19 13:14:45,793:utils:loss_avg: 1.15540, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48562, mc_score:-0.03810\n",
      "[DEBUG]2020-06-19 13:14:46,566:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:46,567:utils:itr: 297, num_batch: 297, last loss: 0.984375, smooth_loss: 1.150468\n",
      "[DEBUG]2020-06-19 13:14:46,799:utils:loss_avg: 1.15482, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48621, mc_score:-0.03595\n",
      "[DEBUG]2020-06-19 13:14:47,570:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:47,572:utils:itr: 298, num_batch: 298, last loss: 1.031250, smooth_loss: 1.148078\n",
      "[DEBUG]2020-06-19 13:14:47,809:utils:loss_avg: 1.15441, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48647, mc_score:-0.03567\n",
      "[DEBUG]2020-06-19 13:14:48,674:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:48,675:utils:itr: 299, num_batch: 299, last loss: 1.093750, smooth_loss: 1.146989\n",
      "[DEBUG]2020-06-19 13:14:48,905:utils:loss_avg: 1.15421, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48667, mc_score:-0.03556\n",
      "[DEBUG]2020-06-19 13:14:49,671:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:49,673:utils:itr: 300, num_batch: 300, last loss: 0.894531, smooth_loss: 1.141928\n",
      "[DEBUG]2020-06-19 13:14:49,904:utils:loss_avg: 1.15334, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48784, mc_score:-0.03417\n",
      "[DEBUG]2020-06-19 13:14:50,675:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:50,677:utils:itr: 301, num_batch: 301, last loss: 0.964844, smooth_loss: 1.138378\n",
      "[DEBUG]2020-06-19 13:14:50,906:utils:loss_avg: 1.15272, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48784, mc_score:-0.03501\n",
      "[DEBUG]2020-06-19 13:14:51,677:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:51,679:utils:itr: 302, num_batch: 302, last loss: 1.000000, smooth_loss: 1.135605\n",
      "[DEBUG]2020-06-19 13:14:51,908:utils:loss_avg: 1.15221, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48784, mc_score:-0.03399\n",
      "[DEBUG]2020-06-19 13:14:52,772:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:52,774:utils:itr: 303, num_batch: 303, last loss: 1.078125, smooth_loss: 1.134453\n",
      "[DEBUG]2020-06-19 13:14:53,004:utils:loss_avg: 1.15197, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48760, mc_score:-0.03312\n",
      "[DEBUG]2020-06-19 13:14:53,770:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:53,771:utils:itr: 304, num_batch: 304, last loss: 1.281250, smooth_loss: 1.137395\n",
      "[DEBUG]2020-06-19 13:14:53,996:utils:loss_avg: 1.15239, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48783, mc_score:-0.03407\n",
      "[DEBUG]2020-06-19 13:14:54,766:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:54,767:utils:itr: 305, num_batch: 305, last loss: 0.843750, smooth_loss: 1.131510\n",
      "[DEBUG]2020-06-19 13:14:54,994:utils:loss_avg: 1.15139, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48894, mc_score:-0.03365\n",
      "[DEBUG]2020-06-19 13:14:55,763:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:55,765:utils:itr: 306, num_batch: 306, last loss: 1.031250, smooth_loss: 1.129501\n",
      "[DEBUG]2020-06-19 13:14:55,991:utils:loss_avg: 1.15100, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48870, mc_score:-0.03358\n",
      "[DEBUG]2020-06-19 13:14:56,969:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:56,971:utils:itr: 307, num_batch: 307, last loss: 1.273438, smooth_loss: 1.132385\n",
      "[DEBUG]2020-06-19 13:14:57,201:utils:loss_avg: 1.15139, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48887, mc_score:-0.03436\n",
      "[DEBUG]2020-06-19 13:14:57,969:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:57,971:utils:itr: 308, num_batch: 308, last loss: 0.980469, smooth_loss: 1.129341\n",
      "[DEBUG]2020-06-19 13:14:58,197:utils:loss_avg: 1.15084, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48904, mc_score:-0.03386\n",
      "[DEBUG]2020-06-19 13:14:58,966:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:58,967:utils:itr: 309, num_batch: 309, last loss: 1.062500, smooth_loss: 1.128001\n",
      "[DEBUG]2020-06-19 13:14:59,196:utils:loss_avg: 1.15055, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48948, mc_score:-0.03252\n",
      "[DEBUG]2020-06-19 13:14:59,965:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:14:59,966:utils:itr: 310, num_batch: 310, last loss: 1.375000, smooth_loss: 1.132951\n",
      "[DEBUG]2020-06-19 13:15:00,194:utils:loss_avg: 1.15128, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48902, mc_score:-0.03213\n",
      "[DEBUG]2020-06-19 13:15:01,041:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:01,042:utils:itr: 311, num_batch: 311, last loss: 1.125000, smooth_loss: 1.132792\n",
      "[DEBUG]2020-06-19 13:15:01,272:utils:loss_avg: 1.15119, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48891, mc_score:-0.03237\n",
      "[DEBUG]2020-06-19 13:15:02,037:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:02,038:utils:itr: 312, num_batch: 312, last loss: 1.281250, smooth_loss: 1.135766\n",
      "[DEBUG]2020-06-19 13:15:02,268:utils:loss_avg: 1.15161, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48815, mc_score:-0.03232\n",
      "[DEBUG]2020-06-19 13:15:03,040:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:03,042:utils:itr: 313, num_batch: 313, last loss: 1.187500, smooth_loss: 1.136803\n",
      "[DEBUG]2020-06-19 13:15:03,273:utils:loss_avg: 1.15172, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48827, mc_score:-0.03158\n",
      "[DEBUG]2020-06-19 13:15:04,046:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:04,048:utils:itr: 314, num_batch: 314, last loss: 0.957031, smooth_loss: 1.133201\n",
      "[DEBUG]2020-06-19 13:15:04,277:utils:loss_avg: 1.15110, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48883, mc_score:-0.03064\n",
      "[DEBUG]2020-06-19 13:15:05,194:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:05,196:utils:itr: 315, num_batch: 315, last loss: 1.078125, smooth_loss: 1.132098\n",
      "[DEBUG]2020-06-19 13:15:05,423:utils:loss_avg: 1.15087, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48931, mc_score:-0.02948\n",
      "[DEBUG]2020-06-19 13:15:06,190:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:06,192:utils:itr: 316, num_batch: 316, last loss: 1.031250, smooth_loss: 1.130077\n",
      "[DEBUG]2020-06-19 13:15:06,422:utils:loss_avg: 1.15050, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49021, mc_score:-0.02756\n",
      "[DEBUG]2020-06-19 13:15:07,192:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:07,194:utils:itr: 317, num_batch: 317, last loss: 1.367188, smooth_loss: 1.134827\n",
      "[DEBUG]2020-06-19 13:15:07,424:utils:loss_avg: 1.15118, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48992, mc_score:-0.02743\n",
      "[DEBUG]2020-06-19 13:15:08,196:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:08,197:utils:itr: 318, num_batch: 318, last loss: 1.140625, smooth_loss: 1.134943\n",
      "[DEBUG]2020-06-19 13:15:08,427:utils:loss_avg: 1.15114, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49052, mc_score:-0.02692\n",
      "[DEBUG]2020-06-19 13:15:09,282:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:09,284:utils:itr: 319, num_batch: 319, last loss: 1.171875, smooth_loss: 1.135683\n",
      "[DEBUG]2020-06-19 13:15:09,517:utils:loss_avg: 1.15121, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49076, mc_score:-0.02565\n",
      "[DEBUG]2020-06-19 13:15:10,289:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:10,290:utils:itr: 320, num_batch: 320, last loss: 1.156250, smooth_loss: 1.136095\n",
      "[DEBUG]2020-06-19 13:15:10,523:utils:loss_avg: 1.15122, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49064, mc_score:-0.02438\n",
      "[DEBUG]2020-06-19 13:15:11,292:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:11,294:utils:itr: 321, num_batch: 321, last loss: 1.210938, smooth_loss: 1.137594\n",
      "[DEBUG]2020-06-19 13:15:11,530:utils:loss_avg: 1.15141, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49027, mc_score:-0.02430\n",
      "[DEBUG]2020-06-19 13:15:12,301:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:12,303:utils:itr: 322, num_batch: 322, last loss: 1.234375, smooth_loss: 1.139533\n",
      "[DEBUG]2020-06-19 13:15:12,533:utils:loss_avg: 1.15167, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48946, mc_score:-0.02710\n",
      "[DEBUG]2020-06-19 13:15:13,387:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:13,389:utils:itr: 323, num_batch: 323, last loss: 1.070312, smooth_loss: 1.138147\n",
      "[DEBUG]2020-06-19 13:15:13,620:utils:loss_avg: 1.15142, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48992, mc_score:-0.02748\n",
      "[DEBUG]2020-06-19 13:15:14,389:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:14,391:utils:itr: 324, num_batch: 324, last loss: 1.171875, smooth_loss: 1.138822\n",
      "[DEBUG]2020-06-19 13:15:14,620:utils:loss_avg: 1.15148, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49033, mc_score:-0.02698\n",
      "[DEBUG]2020-06-19 13:15:15,391:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:15,393:utils:itr: 325, num_batch: 325, last loss: 1.304688, smooth_loss: 1.142144\n",
      "[DEBUG]2020-06-19 13:15:15,623:utils:loss_avg: 1.15195, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49016, mc_score:-0.02821\n",
      "[DEBUG]2020-06-19 13:15:16,394:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:16,396:utils:itr: 326, num_batch: 326, last loss: 1.031250, smooth_loss: 1.139923\n",
      "[DEBUG]2020-06-19 13:15:16,627:utils:loss_avg: 1.15158, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49025, mc_score:-0.02900\n",
      "[DEBUG]2020-06-19 13:15:17,482:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:17,484:utils:itr: 327, num_batch: 327, last loss: 1.484375, smooth_loss: 1.146821\n",
      "[DEBUG]2020-06-19 13:15:17,715:utils:loss_avg: 1.15259, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48922, mc_score:-0.02896\n",
      "[DEBUG]2020-06-19 13:15:18,482:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:18,483:utils:itr: 328, num_batch: 328, last loss: 1.335938, smooth_loss: 1.150608\n",
      "[DEBUG]2020-06-19 13:15:18,716:utils:loss_avg: 1.15315, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48878, mc_score:-0.03017\n",
      "[DEBUG]2020-06-19 13:15:19,487:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:19,489:utils:itr: 329, num_batch: 329, last loss: 1.406250, smooth_loss: 1.155728\n",
      "[DEBUG]2020-06-19 13:15:19,722:utils:loss_avg: 1.15392, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48863, mc_score:-0.02921\n",
      "[DEBUG]2020-06-19 13:15:20,492:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:20,494:utils:itr: 330, num_batch: 330, last loss: 1.117188, smooth_loss: 1.154956\n",
      "[DEBUG]2020-06-19 13:15:20,726:utils:loss_avg: 1.15381, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48802, mc_score:-0.03054\n",
      "[DEBUG]2020-06-19 13:15:21,579:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:21,581:utils:itr: 331, num_batch: 331, last loss: 1.078125, smooth_loss: 1.153418\n",
      "[DEBUG]2020-06-19 13:15:21,814:utils:loss_avg: 1.15358, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48771, mc_score:-0.03117\n",
      "[DEBUG]2020-06-19 13:15:22,588:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:22,590:utils:itr: 332, num_batch: 332, last loss: 1.015625, smooth_loss: 1.150658\n",
      "[DEBUG]2020-06-19 13:15:22,818:utils:loss_avg: 1.15316, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48858, mc_score:-0.03053\n",
      "[DEBUG]2020-06-19 13:15:23,589:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:23,591:utils:itr: 333, num_batch: 333, last loss: 1.226562, smooth_loss: 1.152178\n",
      "[DEBUG]2020-06-19 13:15:23,822:utils:loss_avg: 1.15338, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48885, mc_score:-0.03077\n",
      "[DEBUG]2020-06-19 13:15:24,591:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:24,592:utils:itr: 334, num_batch: 334, last loss: 1.234375, smooth_loss: 1.153824\n",
      "[DEBUG]2020-06-19 13:15:24,822:utils:loss_avg: 1.15363, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48912, mc_score:-0.02967\n",
      "[DEBUG]2020-06-19 13:15:25,669:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:25,671:utils:itr: 335, num_batch: 335, last loss: 1.148438, smooth_loss: 1.153716\n",
      "[DEBUG]2020-06-19 13:15:25,906:utils:loss_avg: 1.15361, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48967, mc_score:-0.02873\n",
      "[DEBUG]2020-06-19 13:15:26,677:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:26,679:utils:itr: 336, num_batch: 336, last loss: 1.023438, smooth_loss: 1.151108\n",
      "[DEBUG]2020-06-19 13:15:26,908:utils:loss_avg: 1.15322, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48989, mc_score:-0.02841\n",
      "[DEBUG]2020-06-19 13:15:27,682:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:27,684:utils:itr: 337, num_batch: 337, last loss: 1.085938, smooth_loss: 1.149803\n",
      "[DEBUG]2020-06-19 13:15:27,915:utils:loss_avg: 1.15303, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.48960, mc_score:-0.02814\n",
      "[DEBUG]2020-06-19 13:15:28,688:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:28,690:utils:itr: 338, num_batch: 338, last loss: 0.972656, smooth_loss: 1.146257\n",
      "[DEBUG]2020-06-19 13:15:28,920:utils:loss_avg: 1.15249, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49007, mc_score:-0.02724\n",
      "[DEBUG]2020-06-19 13:15:29,787:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:29,789:utils:itr: 339, num_batch: 339, last loss: 1.054688, smooth_loss: 1.144423\n",
      "[DEBUG]2020-06-19 13:15:30,021:utils:loss_avg: 1.15221, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49096, mc_score:-0.02658\n",
      "[DEBUG]2020-06-19 13:15:30,789:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:30,791:utils:itr: 340, num_batch: 340, last loss: 1.312500, smooth_loss: 1.147788\n",
      "[DEBUG]2020-06-19 13:15:31,025:utils:loss_avg: 1.15268, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49113, mc_score:-0.02735\n",
      "[DEBUG]2020-06-19 13:15:31,802:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:31,804:utils:itr: 341, num_batch: 341, last loss: 1.484375, smooth_loss: 1.154527\n",
      "[DEBUG]2020-06-19 13:15:32,040:utils:loss_avg: 1.15365, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49041, mc_score:-0.02822\n",
      "[DEBUG]2020-06-19 13:15:32,815:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:32,816:utils:itr: 342, num_batch: 342, last loss: 1.164062, smooth_loss: 1.154718\n",
      "[DEBUG]2020-06-19 13:15:33,051:utils:loss_avg: 1.15368, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49065, mc_score:-0.02721\n",
      "[DEBUG]2020-06-19 13:15:33,918:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:33,920:utils:itr: 343, num_batch: 343, last loss: 1.234375, smooth_loss: 1.156312\n",
      "[DEBUG]2020-06-19 13:15:34,159:utils:loss_avg: 1.15391, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49069, mc_score:-0.02669\n",
      "[DEBUG]2020-06-19 13:15:34,932:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:34,934:utils:itr: 344, num_batch: 344, last loss: 1.218750, smooth_loss: 1.157562\n",
      "[DEBUG]2020-06-19 13:15:35,166:utils:loss_avg: 1.15410, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49076, mc_score:-0.02632\n",
      "[DEBUG]2020-06-19 13:15:35,939:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:35,940:utils:itr: 345, num_batch: 345, last loss: 1.125000, smooth_loss: 1.156910\n",
      "[DEBUG]2020-06-19 13:15:36,173:utils:loss_avg: 1.15401, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49072, mc_score:-0.02716\n",
      "[DEBUG]2020-06-19 13:15:36,945:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:36,947:utils:itr: 346, num_batch: 346, last loss: 0.960938, smooth_loss: 1.152987\n",
      "[DEBUG]2020-06-19 13:15:37,180:utils:loss_avg: 1.15346, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49149, mc_score:-0.02669\n",
      "[DEBUG]2020-06-19 13:15:38,071:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:38,073:utils:itr: 347, num_batch: 347, last loss: 1.085938, smooth_loss: 1.151645\n",
      "[DEBUG]2020-06-19 13:15:38,306:utils:loss_avg: 1.15326, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49204, mc_score:-0.02579\n",
      "[DEBUG]2020-06-19 13:15:39,077:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:39,078:utils:itr: 348, num_batch: 348, last loss: 1.218750, smooth_loss: 1.152989\n",
      "[DEBUG]2020-06-19 13:15:39,312:utils:loss_avg: 1.15345, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49253, mc_score:-0.02602\n",
      "[DEBUG]2020-06-19 13:15:40,085:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:40,087:utils:itr: 349, num_batch: 349, last loss: 0.941406, smooth_loss: 1.148753\n",
      "[DEBUG]2020-06-19 13:15:40,323:utils:loss_avg: 1.15285, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49302, mc_score:-0.02559\n",
      "[DEBUG]2020-06-19 13:15:41,101:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:41,102:utils:itr: 350, num_batch: 350, last loss: 1.000000, smooth_loss: 1.145776\n",
      "[DEBUG]2020-06-19 13:15:41,332:utils:loss_avg: 1.15241, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49360, mc_score:-0.02542\n",
      "[DEBUG]2020-06-19 13:15:42,201:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:42,202:utils:itr: 351, num_batch: 351, last loss: 1.062500, smooth_loss: 1.144109\n",
      "[DEBUG]2020-06-19 13:15:42,433:utils:loss_avg: 1.15216, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49382, mc_score:-0.02482\n",
      "[DEBUG]2020-06-19 13:15:43,199:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:43,201:utils:itr: 352, num_batch: 352, last loss: 1.203125, smooth_loss: 1.145290\n",
      "[DEBUG]2020-06-19 13:15:43,434:utils:loss_avg: 1.15230, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49405, mc_score:-0.02479\n",
      "[DEBUG]2020-06-19 13:15:44,204:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:44,206:utils:itr: 353, num_batch: 353, last loss: 1.304688, smooth_loss: 1.148481\n",
      "[DEBUG]2020-06-19 13:15:44,436:utils:loss_avg: 1.15273, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49394, mc_score:-0.02377\n",
      "[DEBUG]2020-06-19 13:15:45,205:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:45,207:utils:itr: 354, num_batch: 354, last loss: 1.031250, smooth_loss: 1.146134\n",
      "[DEBUG]2020-06-19 13:15:45,437:utils:loss_avg: 1.15239, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49388, mc_score:-0.02438\n",
      "[DEBUG]2020-06-19 13:15:46,284:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:46,285:utils:itr: 355, num_batch: 355, last loss: 1.015625, smooth_loss: 1.143522\n",
      "[DEBUG]2020-06-19 13:15:46,518:utils:loss_avg: 1.15200, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49408, mc_score:-0.02282\n",
      "[DEBUG]2020-06-19 13:15:47,289:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:47,290:utils:itr: 356, num_batch: 356, last loss: 1.023438, smooth_loss: 1.141119\n",
      "[DEBUG]2020-06-19 13:15:47,521:utils:loss_avg: 1.15164, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49403, mc_score:-0.02262\n",
      "[DEBUG]2020-06-19 13:15:48,293:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:48,295:utils:itr: 357, num_batch: 357, last loss: 1.093750, smooth_loss: 1.140171\n",
      "[DEBUG]2020-06-19 13:15:48,528:utils:loss_avg: 1.15148, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49417, mc_score:-0.02350\n",
      "[DEBUG]2020-06-19 13:15:49,301:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:49,302:utils:itr: 358, num_batch: 358, last loss: 0.964844, smooth_loss: 1.136662\n",
      "[DEBUG]2020-06-19 13:15:49,538:utils:loss_avg: 1.15096, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49488, mc_score:-0.02292\n",
      "[DEBUG]2020-06-19 13:15:50,409:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:50,411:utils:itr: 359, num_batch: 359, last loss: 1.187500, smooth_loss: 1.137679\n",
      "[DEBUG]2020-06-19 13:15:50,646:utils:loss_avg: 1.15106, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49529, mc_score:-0.02273\n",
      "[DEBUG]2020-06-19 13:15:51,413:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:51,415:utils:itr: 360, num_batch: 360, last loss: 1.250000, smooth_loss: 1.139927\n",
      "[DEBUG]2020-06-19 13:15:51,645:utils:loss_avg: 1.15134, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49539, mc_score:-0.02292\n",
      "[DEBUG]2020-06-19 13:15:52,413:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:52,414:utils:itr: 361, num_batch: 361, last loss: 1.109375, smooth_loss: 1.139315\n",
      "[DEBUG]2020-06-19 13:15:52,645:utils:loss_avg: 1.15122, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49462, mc_score:-0.02391\n",
      "[DEBUG]2020-06-19 13:15:53,419:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:53,421:utils:itr: 362, num_batch: 362, last loss: 1.289062, smooth_loss: 1.142312\n",
      "[DEBUG]2020-06-19 13:15:53,646:utils:loss_avg: 1.15160, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49429, mc_score:-0.02502\n",
      "[DEBUG]2020-06-19 13:15:54,503:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:54,505:utils:itr: 363, num_batch: 363, last loss: 1.187500, smooth_loss: 1.143217\n",
      "[DEBUG]2020-06-19 13:15:54,734:utils:loss_avg: 1.15170, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49377, mc_score:-0.02570\n",
      "[DEBUG]2020-06-19 13:15:55,503:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:55,505:utils:itr: 364, num_batch: 364, last loss: 0.929688, smooth_loss: 1.138944\n",
      "[DEBUG]2020-06-19 13:15:55,737:utils:loss_avg: 1.15109, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49407, mc_score:-0.02487\n",
      "[DEBUG]2020-06-19 13:15:56,509:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:56,511:utils:itr: 365, num_batch: 365, last loss: 1.195312, smooth_loss: 1.140072\n",
      "[DEBUG]2020-06-19 13:15:56,741:utils:loss_avg: 1.15121, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49369, mc_score:-0.02635\n",
      "[DEBUG]2020-06-19 13:15:57,509:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:57,510:utils:itr: 366, num_batch: 366, last loss: 1.109375, smooth_loss: 1.139457\n",
      "[DEBUG]2020-06-19 13:15:57,741:utils:loss_avg: 1.15110, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49321, mc_score:-0.02743\n",
      "[DEBUG]2020-06-19 13:15:58,590:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:58,592:utils:itr: 367, num_batch: 367, last loss: 1.250000, smooth_loss: 1.141670\n",
      "[DEBUG]2020-06-19 13:15:58,820:utils:loss_avg: 1.15137, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49293, mc_score:-0.02761\n",
      "[DEBUG]2020-06-19 13:15:59,584:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:15:59,585:utils:itr: 368, num_batch: 368, last loss: 1.015625, smooth_loss: 1.139147\n",
      "[DEBUG]2020-06-19 13:15:59,814:utils:loss_avg: 1.15100, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49388, mc_score:-0.02569\n",
      "[DEBUG]2020-06-19 13:16:00,580:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:00,581:utils:itr: 369, num_batch: 369, last loss: 1.265625, smooth_loss: 1.141678\n",
      "[DEBUG]2020-06-19 13:16:00,811:utils:loss_avg: 1.15131, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49432, mc_score:-0.02563\n",
      "[DEBUG]2020-06-19 13:16:01,581:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:01,583:utils:itr: 370, num_batch: 370, last loss: 1.140625, smooth_loss: 1.141657\n",
      "[DEBUG]2020-06-19 13:16:01,816:utils:loss_avg: 1.15128, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49434, mc_score:-0.02556\n",
      "[DEBUG]2020-06-19 13:16:02,690:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:02,692:utils:itr: 371, num_batch: 371, last loss: 0.917969, smooth_loss: 1.137181\n",
      "[DEBUG]2020-06-19 13:16:02,925:utils:loss_avg: 1.15065, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49493, mc_score:-0.02446\n",
      "[DEBUG]2020-06-19 13:16:03,690:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:03,692:utils:itr: 372, num_batch: 372, last loss: 1.140625, smooth_loss: 1.137250\n",
      "[DEBUG]2020-06-19 13:16:03,924:utils:loss_avg: 1.15063, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49436, mc_score:-0.02464\n",
      "[DEBUG]2020-06-19 13:16:04,695:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:04,697:utils:itr: 373, num_batch: 373, last loss: 1.179688, smooth_loss: 1.138099\n",
      "[DEBUG]2020-06-19 13:16:04,931:utils:loss_avg: 1.15070, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49432, mc_score:-0.02470\n",
      "[DEBUG]2020-06-19 13:16:05,706:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:05,707:utils:itr: 374, num_batch: 374, last loss: 1.140625, smooth_loss: 1.138150\n",
      "[DEBUG]2020-06-19 13:16:05,939:utils:loss_avg: 1.15068, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49421, mc_score:-0.02349\n",
      "[DEBUG]2020-06-19 13:16:06,801:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:06,802:utils:itr: 375, num_batch: 375, last loss: 1.085938, smooth_loss: 1.137105\n",
      "[DEBUG]2020-06-19 13:16:07,032:utils:loss_avg: 1.15050, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49458, mc_score:-0.02383\n",
      "[DEBUG]2020-06-19 13:16:07,800:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:07,801:utils:itr: 376, num_batch: 376, last loss: 1.390625, smooth_loss: 1.142178\n",
      "[DEBUG]2020-06-19 13:16:08,031:utils:loss_avg: 1.15114, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49456, mc_score:-0.02426\n",
      "[DEBUG]2020-06-19 13:16:08,801:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:08,803:utils:itr: 377, num_batch: 377, last loss: 1.335938, smooth_loss: 1.146055\n",
      "[DEBUG]2020-06-19 13:16:09,026:utils:loss_avg: 1.15163, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49398, mc_score:-0.02490\n",
      "[DEBUG]2020-06-19 13:16:09,793:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:09,795:utils:itr: 378, num_batch: 378, last loss: 1.093750, smooth_loss: 1.145008\n",
      "[DEBUG]2020-06-19 13:16:10,023:utils:loss_avg: 1.15148, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49416, mc_score:-0.02435\n",
      "[DEBUG]2020-06-19 13:16:10,868:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:10,870:utils:itr: 379, num_batch: 379, last loss: 1.453125, smooth_loss: 1.151174\n",
      "[DEBUG]2020-06-19 13:16:11,103:utils:loss_avg: 1.15227, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49347, mc_score:-0.02462\n",
      "[DEBUG]2020-06-19 13:16:11,876:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:11,877:utils:itr: 380, num_batch: 380, last loss: 1.085938, smooth_loss: 1.149868\n",
      "[DEBUG]2020-06-19 13:16:12,108:utils:loss_avg: 1.15210, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49330, mc_score:-0.02470\n",
      "[DEBUG]2020-06-19 13:16:12,876:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:12,877:utils:itr: 381, num_batch: 381, last loss: 1.171875, smooth_loss: 1.150309\n",
      "[DEBUG]2020-06-19 13:16:13,110:utils:loss_avg: 1.15215, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49332, mc_score:-0.02479\n",
      "[DEBUG]2020-06-19 13:16:13,874:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:13,876:utils:itr: 382, num_batch: 382, last loss: 1.523438, smooth_loss: 1.157775\n",
      "[DEBUG]2020-06-19 13:16:14,107:utils:loss_avg: 1.15312, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49296, mc_score:-0.02512\n",
      "[DEBUG]2020-06-19 13:16:14,973:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:14,974:utils:itr: 383, num_batch: 383, last loss: 1.234375, smooth_loss: 1.159307\n",
      "[DEBUG]2020-06-19 13:16:15,205:utils:loss_avg: 1.15333, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49298, mc_score:-0.02629\n",
      "[DEBUG]2020-06-19 13:16:15,969:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:15,971:utils:itr: 384, num_batch: 384, last loss: 1.070312, smooth_loss: 1.157527\n",
      "[DEBUG]2020-06-19 13:16:16,202:utils:loss_avg: 1.15311, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49322, mc_score:-0.02601\n",
      "[DEBUG]2020-06-19 13:16:16,974:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:16,976:utils:itr: 385, num_batch: 385, last loss: 1.093750, smooth_loss: 1.156251\n",
      "[DEBUG]2020-06-19 13:16:17,206:utils:loss_avg: 1.15296, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49328, mc_score:-0.02642\n",
      "[DEBUG]2020-06-19 13:16:17,974:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:17,976:utils:itr: 386, num_batch: 386, last loss: 1.156250, smooth_loss: 1.156251\n",
      "[DEBUG]2020-06-19 13:16:18,205:utils:loss_avg: 1.15297, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49334, mc_score:-0.02588\n",
      "[DEBUG]2020-06-19 13:16:19,066:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:19,067:utils:itr: 387, num_batch: 387, last loss: 1.203125, smooth_loss: 1.157189\n",
      "[DEBUG]2020-06-19 13:16:19,298:utils:loss_avg: 1.15310, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49304, mc_score:-0.02581\n",
      "[DEBUG]2020-06-19 13:16:20,067:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:20,069:utils:itr: 388, num_batch: 388, last loss: 1.203125, smooth_loss: 1.158108\n",
      "[DEBUG]2020-06-19 13:16:20,301:utils:loss_avg: 1.15323, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49318, mc_score:-0.02684\n",
      "[DEBUG]2020-06-19 13:16:21,074:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:21,076:utils:itr: 389, num_batch: 389, last loss: 1.015625, smooth_loss: 1.155257\n",
      "[DEBUG]2020-06-19 13:16:21,309:utils:loss_avg: 1.15287, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49330, mc_score:-0.02798\n",
      "[DEBUG]2020-06-19 13:16:22,080:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:22,082:utils:itr: 390, num_batch: 390, last loss: 1.054688, smooth_loss: 1.153245\n",
      "[DEBUG]2020-06-19 13:16:22,314:utils:loss_avg: 1.15262, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49339, mc_score:-0.02814\n",
      "[DEBUG]2020-06-19 13:16:23,201:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:23,203:utils:itr: 391, num_batch: 391, last loss: 0.960938, smooth_loss: 1.149398\n",
      "[DEBUG]2020-06-19 13:16:23,437:utils:loss_avg: 1.15213, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49342, mc_score:-0.02767\n",
      "[DEBUG]2020-06-19 13:16:24,209:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:24,210:utils:itr: 392, num_batch: 392, last loss: 1.125000, smooth_loss: 1.148910\n",
      "[DEBUG]2020-06-19 13:16:24,441:utils:loss_avg: 1.15207, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49333, mc_score:-0.02748\n",
      "[DEBUG]2020-06-19 13:16:25,215:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:25,217:utils:itr: 393, num_batch: 393, last loss: 1.289062, smooth_loss: 1.151714\n",
      "[DEBUG]2020-06-19 13:16:25,450:utils:loss_avg: 1.15241, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49301, mc_score:-0.02862\n",
      "[DEBUG]2020-06-19 13:16:26,221:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:26,223:utils:itr: 394, num_batch: 394, last loss: 1.015625, smooth_loss: 1.148991\n",
      "[DEBUG]2020-06-19 13:16:26,454:utils:loss_avg: 1.15207, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49311, mc_score:-0.02889\n",
      "[DEBUG]2020-06-19 13:16:27,315:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:27,317:utils:itr: 395, num_batch: 395, last loss: 1.093750, smooth_loss: 1.147886\n",
      "[DEBUG]2020-06-19 13:16:27,547:utils:loss_avg: 1.15192, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49313, mc_score:-0.02919\n",
      "[DEBUG]2020-06-19 13:16:28,313:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:28,314:utils:itr: 396, num_batch: 396, last loss: 1.132812, smooth_loss: 1.147584\n",
      "[DEBUG]2020-06-19 13:16:28,546:utils:loss_avg: 1.15187, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49295, mc_score:-0.02892\n",
      "[DEBUG]2020-06-19 13:16:29,317:utils:on_backward_begin lr: 4e-05\n",
      "[DEBUG]2020-06-19 13:16:29,318:utils:itr: 397, num_batch: 397, last loss: 1.250000, smooth_loss: 1.149633\n",
      "[DEBUG]2020-06-19 13:16:29,548:utils:loss_avg: 1.15212, lr_pg0:4e-05, lr_pg1: 4e-05final_score:0.49283, mc_score:-0.02900\n",
      "[DEBUG]2020-06-19 13:16:30,324:utils:on_backward_begin lr: 4e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-05a5bea78e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'debug_train(use_dist_cb=False)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-8e5383cf01dc>\u001b[0m in \u001b[0;36mdebug_train\u001b[0;34m(use_dist_cb)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#learn.recorder.plot()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m#learn.fit_one_cycle(1, max_lr=2e-5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4e-5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# original 0.5*e-5*8=4*e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DEBUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_backward_begin\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'smooth_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'skip_bwd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, cb_name, call_mets, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36m_call_and_update\u001b[0;34m(self, cb, cb_name, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"Call `cb_name` on `cb` and update the inner state.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-a734405a1d8b>\u001b[0m in \u001b[0;36mon_backward_begin\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         logger.debug(\"itr: %d, num_batch: %d, last loss: %f, smooth_loss: %f\",\n\u001b[1;32m     25\u001b[0m                      \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iteration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_batch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                      kwargs['last_loss'], kwargs['smooth_loss'])\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mdebug\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \"\"\"\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEnabledFor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36m_log\u001b[0;34m(self, level, msg, args, exc_info, extra, stack_info)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         record = self.makeRecord(self.name, level, fn, lno, msg, args,\n\u001b[1;32m   1443\u001b[0m                                  exc_info, func, extra, sinfo)\n\u001b[0;32m-> 1444\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \"\"\"\n\u001b[1;32m   1453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallHandlers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mcallHandlers\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevelno\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m                     \u001b[0mhdlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m    \u001b[0;31m#break out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/python_logging_rabbitmq/handlers.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mformatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_exc_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mformatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.channel.basic_publish(\n",
      "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_defaultFormatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/kaggle_runner/utils/utils.py\u001b[0m in \u001b[0;36mlog_format\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     data.update(\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgethostname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/kaggle_runner/utils/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     data.update(\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgethostname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnonzero_finite_max\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnonzero_finite_min\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1000.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                         \u001b[0;32mor\u001b[0m \u001b[0mnonzero_finite_max\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.e8\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                         \u001b[0;32mor\u001b[0m \u001b[0mnonzero_finite_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.e-4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msci_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "debug_train(use_dist_cb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4MbjVEVm3bIw",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pysnooper\n",
    "\n",
    "@pysnooper.snoop()\n",
    "def train_loop(index, *args):\n",
    "  #data = (ImageList.from_df(df=train_df, path=path/'images', cols=1)\n",
    "  #        .random_split_by_pct(0.2)\n",
    "  #        .label_from_df(cols=0)\n",
    "  #        .transform(get_transforms(), size=224)\n",
    "  #        .databunch(bs=32, num_workers=0)\n",
    "  #        .normalize(imagenet_stat))\n",
    "  #learn = cnn_learner(data, models.resnet152, metrics=accuracy).to_tpu_distributed()\n",
    "    logger.debug(\"rank: %d entered train_loop\", index)\n",
    "\n",
    "    param_optimizer = list(k.model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'lr': 4e-5, 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'lr': 4e-5, 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    def AdamW_with_given_p(p_to_ignore, *args, **kargs):\n",
    "        kargs['lr']=TrainGlobalConfig.lr*xm.xrt_world_size()\n",
    "\n",
    "        return AdamW(optimizer_grouped_parameters, *args, **kargs)\n",
    "\n",
    "    if index == 0:\n",
    "        time.sleep(1)\n",
    "    learn = k.create_learner(k, opt_func=AdamW_with_given_p,\n",
    "                             loss_func=LabelSmoothing(),\n",
    "                             wd=0.01,\n",
    "                             callback_fns=[partial(GradientClipping, clip=0.5),\n",
    "                                           ShowGraph,\n",
    "                                           partial(CSVLogger, append=True),\n",
    "                                           partial(CheckGrad, skip_loss_step=False)]\n",
    "                             ).to_tpu_distributed()\n",
    "    learn.lr_find(start_lr=1e-7, end_lr=1e-5, num_it=200)\n",
    "    learn.recorder.plot()\n",
    "    #learn.fit_one_cycle(3, max_lr=5e-6, wd=0.001)\n",
    "    learn.fit(1, lr=5e-6, wd=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "EQDJ4gsP3bIx",
    "lines_to_next_cell": 2,
    "outputId": "69212456-cd22-45b1-a507-d638a39ae9bc"
   },
   "outputs": [],
   "source": [
    "FLAGS={}\n",
    "xmp.spawn(train_loop, args=(FLAGS,),  nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-zDM9QL3bIz"
   },
   "outputs": [],
   "source": [
    "import pysnooper\n",
    "\n",
    "@pysnooper.snoop()\n",
    "def _mp_fn(rank, flags, k=k):\n",
    "    device = xm.xla_device(devkind='TPU')\n",
    "    logger.debug(\"%s used for xla_device\" % device)\n",
    "    net = k.model\n",
    "    net.to(device)\n",
    "    logger.debug(\"%s used for xla_device, to device done\" % device)\n",
    "\n",
    "    train_sampler = DistributedSamplerWrapper(\n",
    "        sampler=BalanceClassSampler(labels=k.train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        k.train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        k.validation_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        k.validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    validation_tune_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        k.validation_tune_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "    validation_tune_loader = torch.utils.data.DataLoader(\n",
    "        k.validation_tune_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=validation_tune_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        k.test_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        k.test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    logger.debug(\"rank: %d\", rank)\n",
    "\n",
    "    if rank == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "    fitter = TPUFitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    fitter.fit(train_loader, validation_loader)\n",
    "    fitter.run_tuning_and_inference(test_loader, validation_tune_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhQxQcSA3bI3",
    "outputId": "b3b2239f-1b6b-4dd4-d9cc-d698016403a5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpLwWDel3bI7",
    "outputId": "b7ff8ab4-7852-46d5-b553-d7c712043eb1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FLAGS={}\n",
    "    xmp.spawn(_mp_fn, args=(FLAGS,),  nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTEdrF6n3bJA"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "output_model_file='XLMRobertaModel_tpu_trained.bin'\n",
    "torch.save(k.model.state_dict(), f\"{today}_{output_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wu0VhhZAFuYs"
   },
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.read_csv(path) for path in glob('node_submissions/*.csv')]).groupby('id').mean()\n",
    "submission['toxic'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRr-yzJ_yVTW"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(f'{ROOT_PATH}/submission.csv')\n",
    "\n",
    "#!cp log.txt '/content/drive/My Drive/jigsaw2020-kaggle-public-baseline/'\n",
    "!make -C kaggle_runner push_dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "name": "shonenkov_training_pipeline.ipynb",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "id,colab_type,colab,language,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
