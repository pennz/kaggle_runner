{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3cDPaT3AGY9X",
    "outputId": "03971ff3-fc6d-4ad6-d4ee-cdc657d6dd95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Ggsc5F1HweQ",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# git clone https://github.com/pennz/kaggle_runner\n",
    "# python3 -m pip install -e kaggle_runner\n",
    "# export PATH=$PWD/kaggle_runner/bin:$PATH\n",
    "# entry.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# python3 -c 'import torch_xla' || (curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null;\n",
    "#                                   python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev;\n",
    "#                                   python3 -m pip install transformers==2.5.1 > /dev/null;\n",
    "#                                   python3 -m pip install pandarallel > /dev/null;\n",
    "#                                   python3 -m pip install catalyst==20.4.2 > /dev/null;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIa8b1OozujF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PD_ugjLyz2q2",
    "outputId": "99bf52b2-367b-4776-cd35-16adce936e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbin\u001b[0m/\n",
      "\u001b[01;34mc\u001b[0m/\n",
      "callbacks.py\n",
      "data_providers.py\n",
      "\u001b[01;34mdatas\u001b[0m/\n",
      "\u001b[01;34mdatasets\u001b[0m/\n",
      "defaults.py\n",
      "\u001b[01;34mdotfiles\u001b[0m/\n",
      "entry.sh\n",
      "gdrive_setup\n",
      "\u001b[01;34mhub\u001b[0m/\n",
      "__init__.py\n",
      "\u001b[01;34mkaggle_runner\u001b[0m/\n",
      "\u001b[01;34mkaggle_runner.egg-info\u001b[0m/\n",
      "\u001b[01;34mkernels\u001b[0m/\n",
      "last-checkpoint.bin\n",
      "LICENSE\n",
      "logs.py\n",
      "log.txt\n",
      "losses.py\n",
      "lstm.py\n",
      "__main__.py\n",
      "\u001b[01;32mmain.py\u001b[0m*\n",
      "Makefile\n",
      "\u001b[01;34mmetrics\u001b[0m/\n",
      "\u001b[01;34mmodels\u001b[0m/\n",
      "\u001b[01;34mmodules\u001b[0m/\n",
      "ms_connect_log\n",
      "\u001b[01;34mnode_submissions\u001b[0m/\n",
      "notes.md\n",
      "optimizers.py\n",
      "ot.pkl\n",
      "plots.py\n",
      "post_processers.py\n",
      "predictors.py\n",
      "pytest.ini\n",
      "pytorch-xla-env-setup.py\n",
      "README.md\n",
      "rpt\n",
      "runner_log\n",
      "\u001b[01;34mrunners\u001b[0m/\n",
      "runner.sh\n",
      "\u001b[01;34mrunner_template\u001b[0m/\n",
      "run_state_KernelRunningState.PREPARE_DATA_DONE.pkl\n",
      "rvs.sh\n",
      "\u001b[01;34msample_data\u001b[0m/\n",
      "setup.py\n",
      "submission.csv\n",
      "test.pkl\n",
      "\u001b[01;34mtests\u001b[0m/\n",
      "torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
      "torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
      "torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
      "tpu_session.log\n",
      "train.pkl\n",
      "unet-with-se-resnext50-32x4d-encoder-for-stage-2.py\n",
      "\u001b[01;34mutils\u001b[0m/\n",
      "val.pkl\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqPk4EXhzujR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_USE_BF16'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGhHnJ6lzujc"
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMyrLanSzujl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErRgx7fXzujs"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NojJZlcCzujy"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from fastai.text.transform import Vocab\n",
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jdmPtg7zuj5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "XANY9mV6zuj-",
    "outputId": "1abf2caf-2d9d-42a5-e878-f2d7f1177fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !python3 -m pip install nltk > /dev/null\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anym2P3hzukC"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IDQaF8dzukM"
   },
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "kw_3WCbIzukQ",
    "outputId": "f9f540b7-73cb-4961-d432-9f874481f77d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(nb_workers=4, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypSC-XiCzukW",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from fastai.basic_data import DataBunch\n",
    "from kaggle_runner.kernels.fastai_kernel import FastAIKernel\n",
    "from kaggle_runner import may_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kCJSEXmlzukb"
   },
   "outputs": [],
   "source": [
    "SEED = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NL0d5s9Zzukh"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 224\n",
    "BACKBONE_PATH = 'xlm-roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cildaEWIzukl"
   },
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(BACKBONE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QFzu9h0zukr"
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = f'/kaggle' # for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltHaZDY4zukv"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner.utils.kernel_utils import get_obj_or_dump\n",
    "def get_pickled_data(file_path):\n",
    "    obj = get_obj_or_dump(file_path)\n",
    "\n",
    "    if obj is None:\n",
    "        return get_obj_or_dump(f\"{ROOT_PATH}/input/clean-pickle-for-jigsaw-toxicity/{file_path}\")\n",
    "\n",
    "    return obj\n",
    "vocab = get_pickled_data(\"vocab.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Il678B8zukz",
    "lines_to_next_cell": 2
   },
   "source": [
    "if vocab is None: # vocab file read~~\n",
    "   vocab = [tokenizer.convert_ids_to_tokens(i) for i in range(tokenizer.vocab_size)]\n",
    "   get_obj_or_dump(\"vocab.pkl\", default=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jV6hUe8zuk1",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZqDBNw9zuk6",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from random import shuffle\n",
    "import random\n",
    "import albumentations\n",
    "from albumentations.core.transforms_interface import DualTransform, BasicTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yukCgASQzuk_",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "LANGS = {\n",
    "    'en': 'english',\n",
    "    'it': 'italian',\n",
    "    'fr': 'french',\n",
    "    'es': 'spanish',\n",
    "    'tr': 'turkish',\n",
    "    'ru': 'russian',\n",
    "    'pt': 'portuguese'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkkcElV5zulC",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_sentences(text, lang='en'):\n",
    "    return sent_tokenize(text, LANGS.get(lang, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-aowMXzfzulI",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def exclude_duplicate_sentences(text, lang='en'):\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in get_sentences(text, lang):\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        if sentence not in sentences:\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "622apOEIzulN"
   },
   "outputs": [],
   "source": [
    "def clean_text(text, lang='en'):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[0-9\"]', '', text)\n",
    "    text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = exclude_duplicate_sentences(text, lang)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVhH5vj9zulQ",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class NLPTransform(BasicTransform):\n",
    "    \"\"\" Transform for nlp task.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return {\"data\": self.apply}\n",
    "\n",
    "    def update_params(self, params, **kwargs):\n",
    "        if hasattr(self, \"interpolation\"):\n",
    "            params[\"interpolation\"] = self.interpolation\n",
    "\n",
    "        if hasattr(self, \"fill_value\"):\n",
    "            params[\"fill_value\"] = self.fill_value\n",
    "\n",
    "        return params\n",
    "\n",
    "    def get_sentences(self, text, lang='en'):\n",
    "        return sent_tokenize(text, LANGS.get(lang, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-sAepyANzulU",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ShuffleSentencesTransform(NLPTransform):\n",
    "    \"\"\" Do shuffle by sentence \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = self.get_sentences(text, lang)\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        return ' '.join(sentences), lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "prSLBA3HzulZ",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeDuplicateSentencesTransform(NLPTransform):\n",
    "    \"\"\" Exclude equal sentences \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = []\n",
    "\n",
    "        for sentence in self.get_sentences(text, lang):\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if sentence not in sentences:\n",
    "                sentences.append(sentence)\n",
    "\n",
    "        return ' '.join(sentences), lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HKb_xCfzule",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeNumbersTransform(NLPTransform):\n",
    "    \"\"\" exclude any numbers \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WV_MyGbrzulh",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeHashtagsTransform(NLPTransform):\n",
    "    \"\"\" Exclude any hashtags with # \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4yeoFfwzulk",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeUsersMentionedTransform(NLPTransform):\n",
    "    \"\"\" Exclude @users \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGG5pYnlzulo"
   },
   "outputs": [],
   "source": [
    "class ExcludeUrlsTransform(NLPTransform):\n",
    "    \"\"\" Exclude urls \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'https?\\S+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBJka4tizulu",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_open_subtitles():\n",
    "    df_ot = get_pickled_data(\"ot.pkl\")\n",
    "\n",
    "    if df_ot is None:\n",
    "        df_ot = pd.read_csv(f'{ROOT_PATH}/input/open-subtitles-toxic-pseudo-labeling/open-subtitles-synthesic.csv', index_col='id')[['comment_text', 'toxic', 'lang']]\n",
    "        df_ot = df_ot[~df_ot['comment_text'].isna()]\n",
    "        df_ot['comment_text'] = df_ot.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "        df_ot = df_ot.drop_duplicates(subset='comment_text')\n",
    "        df_ot['toxic'] = df_ot['toxic'].round().astype(np.int)\n",
    "        get_obj_or_dump(\"ot.pkl\", default=df_ot)\n",
    "\n",
    "    return df_ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpWnceNyzulz",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SynthesicOpenSubtitlesTransform(NLPTransform):\n",
    "    def __init__(self, always_apply=False, supliment_toxic=None, p=0.5, mix=False):\n",
    "        super(SynthesicOpenSubtitlesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "        df = get_open_subtitles()\n",
    "        self.synthesic_toxic = df[df['toxic'] == 1].comment_text.values\n",
    "        self.synthesic_non_toxic = df[df['toxic'] == 0].comment_text.values\n",
    "\n",
    "        if supliment_toxic is not None:\n",
    "            self.synthesic_toxic = np.concatenate((self.synthesic_toxic, supliment_toxic))\n",
    "        self.mix = mix\n",
    "\n",
    "        del df\n",
    "        gc.collect();\n",
    "\n",
    "\n",
    "    def _mix_both(self, texts):\n",
    "        for i in range(random.randint(0,2)):\n",
    "            texts.append(random.choice(self.synthesic_non_toxic))\n",
    "\n",
    "        for i in range(random.randint(1,3)):\n",
    "            texts.append(random.choice(self.synthesic_toxic))\n",
    "\n",
    "    def generate_synthesic_sample(self, text, toxic):\n",
    "        texts = [text]\n",
    "\n",
    "        if toxic == 0:\n",
    "            if self.mix:\n",
    "                self._mix_both(texts)\n",
    "                toxic = 1\n",
    "            else:\n",
    "                for i in range(random.randint(1,5)):\n",
    "                    texts.append(random.choice(self.synthesic_non_toxic))\n",
    "        else:\n",
    "            self._mix_both(texts)\n",
    "        random.shuffle(texts)\n",
    "\n",
    "        return ' '.join(texts), toxic\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, toxic = data\n",
    "        text, toxic = self.generate_synthesic_sample(text, toxic)\n",
    "\n",
    "        return text, toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vxmloXn6zul1",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return albumentations.Compose([\n",
    "        ExcludeUsersMentionedTransform(p=0.95),\n",
    "        ExcludeUrlsTransform(p=0.95),\n",
    "        ExcludeNumbersTransform(p=0.95),\n",
    "        ExcludeHashtagsTransform(p=0.95),\n",
    "        ExcludeDuplicateSentencesTransform(p=0.95),\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryRCvNsBzul5",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_synthesic_transforms(supliment_toxic, p=0.5, mix=False):\n",
    "    return SynthesicOpenSubtitlesTransform(p=p, supliment_toxic=supliment_toxic, mix=mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qi_3uRzXzul8",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_toxic_comments(df):\n",
    "        df = df[~df['comment_text'].isna()]\n",
    "        df = df.drop_duplicates(subset='comment_text')\n",
    "        df['toxic'] = df['toxic'].round().astype(np.int)\n",
    "\n",
    "        return df[df['toxic'] == 1].comment_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PW6R9fE9zumB",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def onehot(size, target, aux=None):\n",
    "    if aux is not None:\n",
    "        vec = np.zeros(size+len(aux), dtype=np.float32)\n",
    "        vec[target] = 1.\n",
    "        vec[2:] = aux\n",
    "        vec = torch.tensor(vec, dtype=torch.float32)\n",
    "    else:\n",
    "        vec = torch.zeros(size, dtype=torch.float32)\n",
    "        vec[target] = 1.\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8O0rAN5NzumE"
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, labels_or_ids, comment_texts, langs,\n",
    "                 severe_toxic=None, obscene=None, threat=None, insult=None, identity_hate=None,\n",
    "                 use_train_transforms=False, test=False, use_aux=True, transformers=None):\n",
    "        self.test = test\n",
    "        self.labels_or_ids = labels_or_ids\n",
    "        self.comment_texts = comment_texts\n",
    "        self.langs = langs\n",
    "        self.severe_toxic = severe_toxic\n",
    "        self.obscene = obscene\n",
    "        self.threat = threat\n",
    "        self.insult = insult\n",
    "        self.identity_hate = identity_hate\n",
    "        self.use_train_transforms = use_train_transforms\n",
    "        self.aux = None\n",
    "        assert transformers is not None\n",
    "        self.transformers = transformers\n",
    "        self.vocab = vocab\n",
    "\n",
    "        if use_aux:\n",
    "            self.aux = [self.severe_toxic, self.obscene, self.threat, self.insult, self.identity_hate]\n",
    "\n",
    "    def get_tokens(self, text):\n",
    "        encoded = self.transformers['tokenizer'].encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "\n",
    "        return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.comment_texts.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.comment_texts[idx]\n",
    "        lang = self.langs[idx]\n",
    "\n",
    "        if self.severe_toxic is None:\n",
    "            aux = [0., 0., 0., 0., 0.]\n",
    "        else:\n",
    "            aux = [self.severe_toxic[idx], self.obscene[idx], self.threat[idx], self.insult[idx], self.identity_hate[idx]]\n",
    "\n",
    "\n",
    "        label = self.labels_or_ids[idx]\n",
    "\n",
    "        if self.use_train_transforms and (not self.test):\n",
    "            text, _ = self.transformers['train_transforms'](data=(text, lang))['data']\n",
    "            tokens, attention_mask = self.get_tokens(str(text))\n",
    "            token_length = sum(attention_mask)\n",
    "\n",
    "            if token_length > 0.8*MAX_LENGTH:\n",
    "                text, _ = self.transformers['shuffle_transforms'](data=(text, lang))['data']\n",
    "            elif token_length < 60:\n",
    "                text, label = self.transformers['synthesic_transforms_often'](data=(text, label))['data']\n",
    "            else: # will not need to use transforms\n",
    "                #text, label = synthesic_transforms_low(data=(text, label))['data']\n",
    "                pass\n",
    "\n",
    "        # TODO add language detection and shuffle\n",
    "        # https://pypi.org/project/langdetect/\n",
    "        # if self.use_train_transforms and self.test:\n",
    "        #    text, _ = train_transforms(data=(text, lang))['data']\n",
    "        #    tokens, attention_mask = self.get_tokens(str(text))\n",
    "        #    token_length = sum(attention_mask)\n",
    "\n",
    "        #    if token_length > 0.8*MAX_LENGTH:\n",
    "        #        text, _ = shuffle_transforms(data=(text, lang))['data']\n",
    "        # to tensors\n",
    "        tokens, attention_mask = self.get_tokens(str(text))\n",
    "        tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n",
    "\n",
    "        if self.test:  # for test, return id TODO TTA\n",
    "            return [tokens, attention_mask], self.labels_or_ids[idx]\n",
    "\n",
    "        # label might be changed\n",
    "        target = onehot(2, label, aux=aux)\n",
    "\n",
    "        return [tokens, attention_mask], target\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(np.char.add(self.labels_or_ids.astype(str), self.langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOd-6TmA0Ioy"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner.kernels.fastai_kernel import FastAIKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28CGSbIMzumK",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Shonenkov(FastAIKernel):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Shonenkov, self).__init__(**kargs)\n",
    "        self.data = None\n",
    "        self.transformers = None\n",
    "        self.setup_transformers()\n",
    "\n",
    "    def build_and_set_model(self):\n",
    "        self.model = ToxicSimpleNNModel()\n",
    "        self.setup_learner()\n",
    "\n",
    "    def set_random_seed(self):\n",
    "        seed_everything(SEED)\n",
    "\n",
    "    def setup_transformers(self):\n",
    "        if self.transformers is None:\n",
    "            supliment_toxic = None # avoid overfit\n",
    "            train_transforms = get_train_transforms();\n",
    "            synthesic_transforms_often = get_synthesic_transforms(supliment_toxic, p=0.5)\n",
    "            synthesic_transforms_low = get_synthesic_transforms(supliment_toxic, p=0.3)\n",
    "            #tokenizer = tokenizer\n",
    "            shuffle_transforms = ShuffleSentencesTransform(always_apply=True)\n",
    "\n",
    "            self.transformers = {'train_transforms': train_transforms,\n",
    "                                 'synthesic_transforms_often': synthesic_transforms_often,\n",
    "                                 'synthesic_transforms_low': synthesic_transforms_low,\n",
    "                                 'tokenizer': tokenizer, 'shuffle_transforms':\n",
    "                                 shuffle_transforms}\n",
    "\n",
    "    def prepare_train_dev_data(self):\n",
    "        df_train = get_pickled_data(\"train.pkl\")\n",
    "\n",
    "        if df_train is None:\n",
    "            df_train = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-toxicity-train-data-with-aux/train_data.csv')\n",
    "            df_train['comment_text'] = df_train.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "            get_obj_or_dump(\"train.pkl\", default=df_train)\n",
    "\n",
    "        #supliment_toxic = get_toxic_comments(df_train)\n",
    "        self.train_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_train['toxic'].values,\n",
    "            comment_texts=df_train['comment_text'].values,\n",
    "            langs=df_train['lang'].values,\n",
    "            severe_toxic=df_train['severe_toxic'].values,\n",
    "            obscene=df_train['obscene'].values,\n",
    "            threat=df_train['threat'].values,\n",
    "            insult=df_train['insult'].values,\n",
    "            identity_hate=df_train['identity_hate'].values,\n",
    "            use_train_transforms=True,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "        df_val = get_pickled_data(\"val.pkl\")\n",
    "\n",
    "        if df_val is None:\n",
    "            df_val = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/validation.csv', index_col='id')\n",
    "            df_val['comment_text'] = df_val.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "            get_obj_or_dump(\"val.pkl\", default=df_val)\n",
    "\n",
    "        self.validation_tune_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_val['toxic'].values,\n",
    "            comment_texts=df_val['comment_text'].values,\n",
    "            langs=df_val['lang'].values,\n",
    "            use_train_transforms=True,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "        self.validation_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_val['toxic'].values,\n",
    "            comment_texts=df_val['comment_text'].values,\n",
    "            langs=df_val['lang'].values,\n",
    "            use_train_transforms=False,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "\n",
    "        del df_val\n",
    "#del df_val_unclean\n",
    "        gc.collect();\n",
    "\n",
    "        del df_train\n",
    "        gc.collect();\n",
    "\n",
    "    def prepare_test_data(self):\n",
    "        df_test = get_pickled_data(\"test.pkl\")\n",
    "\n",
    "        if df_test is None:\n",
    "            df_test = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/test.csv', index_col='id')\n",
    "            df_test['comment_text'] = df_test.parallel_apply(lambda x: clean_text(x['content'], x['lang']), axis=1)\n",
    "            get_obj_or_dump(\"test.pkl\", default=df_test)\n",
    "\n",
    "        self.test_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_test.index.values, ## here different!!!\n",
    "            comment_texts=df_test['comment_text'].values,\n",
    "            langs=df_test['lang'].values,\n",
    "            use_train_transforms=False,\n",
    "            test=True,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "\n",
    "        del df_test\n",
    "        gc.collect();\n",
    "    def after_prepare_data_hook(self):\n",
    "        \"\"\"Put to databunch here\"\"\"\n",
    "        self.data = DataBunch.create(self.train_dataset,\n",
    "                                     self.validation_dataset,\n",
    "                                     bs=TrainGlobalConfig.batch_size,\n",
    "                                     num_workers=TrainGlobalConfig.num_workers)\n",
    "\n",
    "    def peek_data(self):\n",
    "        if self.data is not None:\n",
    "            may_debug(True)\n",
    "            o = self.data.one_batch()\n",
    "            print(o)\n",
    "\n",
    "            return o\n",
    "        else:\n",
    "            if self.logger is not None:\n",
    "                self.logger.error(\"peek_data failed, DataBunch is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KQNWmSkbzumN",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from kaggle_runner.metrics.metrics import matthews_correlation\n",
    "class RocAucMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = np.array([])\n",
    "        self.y_true_float = np.array([], dtype=np.float)\n",
    "        self.y_pred = np.array([])\n",
    "        self.score = 0\n",
    "        self.mc_score = 0\n",
    "        self.aux_part = 0\n",
    "\n",
    "    def update(self, y_true, y_pred, aux_part=0):\n",
    "        #y_true_ = y_true\n",
    "        y_true = y_true[:,:2].cpu().numpy().argmax(axis=1)\n",
    "        y_true_float = y_true.astype(np.float)\n",
    "        y_pred = nn.functional.softmax(y_pred[:,:2], dim=1).data.cpu().numpy()[:,1]\n",
    "        self.y_true = np.hstack((self.y_true, y_true))\n",
    "        self.y_true_float = np.hstack((self.y_true_float, y_true_float))\n",
    "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
    "        try:\n",
    "            self.score = sklearn.metrics.roc_auc_score(self.y_true, self.y_pred, labels=np.array([0, 1]))\n",
    "        except Exception:\n",
    "            self.score = 0\n",
    "        self.mc_score = matthews_correlation(self.y_true_float, self.y_pred)\n",
    "        self.aux_part = aux_part\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.score\n",
    "    @property\n",
    "    def mc_avg(self):\n",
    "        return self.mc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e1RpHFNwzumO"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wp5fIJOXzumQ",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrUYLy6FzumT",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ToxicSimpleNNModel(nn.Module):\n",
    "    def __init__(self, use_aux=True):\n",
    "        super(ToxicSimpleNNModel, self).__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(BACKBONE_PATH)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        aux_len = 0\n",
    "\n",
    "        if use_aux:\n",
    "            aux_len = 5\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.backbone.pooler.dense.out_features*2,\n",
    "            out_features=2+aux_len,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        bs, seq_length = input_ids.shape\n",
    "        seq_x, _ = self.backbone(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        apool = torch.mean(seq_x, 1)\n",
    "        mpool, _ = torch.max(seq_x, 1)\n",
    "        x = torch.cat((apool, mpool), 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flNrv6n_zumW"
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "egu_mb1zzumX"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZhuszUbzumb"
   },
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IawhLAz6zumd",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQ2Dzhxuzumg",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TPUFitter:\n",
    "\n",
    "    def __init__(self, model, device, config):\n",
    "        if not os.path.exists('node_submissions'):\n",
    "            os.makedirs('node_submissions')\n",
    "\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        self.log_path = 'log.txt'\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr*xm.xrt_world_size())\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "\n",
    "        self.criterion = config.criterion\n",
    "        xm.master_print(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(train_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
    "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=final_scores.mc_avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def run_tuning_and_inference(self, test_loader, validation_tune_loader):\n",
    "        for e in range(2):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*xm.xrt_world_size()\n",
    "            para_loader = pl.ParallelLoader(validation_tune_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "            para_loader = pl.ParallelLoader(test_loader, [self.device])\n",
    "            self.run_inference(para_loader.per_device_loader(self.device))\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(val_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(train_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    self.log(\n",
    "                        f'Train Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(self.device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "            targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs, attention_masks)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(self.optimizer)\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        self.model.eval()\n",
    "        self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_inference(self, test_loader):\n",
    "        self.model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, ids) in enumerate(test_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "        result = pd.DataFrame(result)\n",
    "        node_count = len(glob('node_submissions/*.csv'))\n",
    "        result.to_csv(f'node_submissions/submission_{node_count}_{datetime.utcnow().microsecond}_{random.random()}.csv', index=False)\n",
    "\n",
    "    def save(self, path):\n",
    "        xm.save(self.model.state_dict(), path)\n",
    "\n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            xm.master_print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            xm.master_print(f'{message}', logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uagKgJ0ezumi",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing = 0.1, dim=-1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.cls = 2\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            pred = x[:,:2].log_softmax(dim=self.dim)\n",
    "            aux=x[:, 2:]\n",
    "\n",
    "            toxic_target = target[:,:2]\n",
    "            aux_target = target[:, 2:]\n",
    "            with torch.no_grad():\n",
    "                # smooth_toxic = pred.data.clone()\n",
    "                smooth_toxic = self.smoothing + (1-self.smoothing*2)*toxic_target\n",
    "                # smooth_toxic.scatter_(1, toxic_target.data.unsqueeze(1), self.confidence) # only for 0 1 label, put confidence to related place\n",
    "                # for 0-1, 0 -> 0.1, 1->0.9.(if 1), if zero. 0->0.9, 1->0.1\n",
    "                smooth_aux = self.smoothing + (1-self.smoothing*2)*aux_target  # only for binary cross entropy, so for lable, it is (1-smooth)*\n",
    "\n",
    "            aux_loss = torch.nn.functional.binary_cross_entropy_with_logits(aux, smooth_aux)\n",
    "\n",
    "            return torch.mean(torch.sum(-smooth_toxic * pred, dim=self.dim)) + aux_loss/3\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x[:,:2], target[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_AtPrxN0zum2",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    \"\"\" Global Config for this notebook \"\"\"\n",
    "    num_workers = 0  #    loaders\n",
    "    batch_size = 16  # bs\n",
    "    n_epochs = 2  #    \n",
    "    lr = 0.5 * 1e-5 #  learning rate (     TPU   - )\n",
    "    fold_number = 0  #    \n",
    "\n",
    "    # -------------------\n",
    "    verbose = True  #  \n",
    "    verbose_step = 25  #     \n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  #  scheduler.step   optimizer.step\n",
    "    validation_scheduler = True  #  scheduler.step   loss (  )\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='max',\n",
    "        factor=0.7,\n",
    "        patience=0,\n",
    "        verbose=False,\n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0,\n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )\n",
    "    # --------------------\n",
    "\n",
    "    # -------------------\n",
    "    criterion = LabelSmoothing()\n",
    "    # -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExVGzty3zum4",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_init():\n",
    "    l = Shonenkov(loss_func=None, metrics=None)\n",
    "    assert l is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "qP2Y5pwL3JKI",
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1,
    "outputId": "d9d57770-2ba1-478e-ced0-cf20b5db116f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-14 14:08:29,071:utils:load ot.pkl\n",
      "[DEBUG]2020-06-14 14:08:30,555:utils:load ot.pkl\n",
      "[DEBUG]2020-06-14 14:08:34,006:utils:None -> KernelRunningState.SAVE_SUBMISSION_DONE\n",
      "[DEBUG]2020-06-14 14:08:34,008:utils:load train.pkl\n",
      "[DEBUG]2020-06-14 14:08:38,465:utils:load val.pkl\n",
      "[DEBUG]2020-06-14 14:08:38,995:utils:state KernelRunningState.PREPARE_DATA_DONE\n",
      "[DEBUG]2020-06-14 14:08:57,829:utils:state KernelRunningState.TRAINING_DONE\n",
      "[DEBUG]2020-06-14 14:08:57,831:utils:state KernelRunningState.EVL_DEV_DONE\n",
      "[DEBUG]2020-06-14 14:08:57,833:utils:load test.pkl\n",
      "[DEBUG]2020-06-14 14:08:58,765:utils:state KernelRunningState.SAVE_SUBMISSION_DONE\n"
     ]
    }
   ],
   "source": [
    "k = Shonenkov(metrics=None, loss_func=LabelSmoothing())\n",
    "k.run(dump_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvpPm-6k3FRa"
   },
   "outputs": [],
   "source": [
    "def test_model_fn(device=torch.device(\"cpu\")):\n",
    "    \"test with CPU, easier to debug\"\n",
    "    from kaggle_runner import logger\n",
    "\n",
    "    #k.run(dump_flag=True) # it seems it cannot save right\n",
    "    #k.run(dump_flag=False)\n",
    "    k.learner.lr_find()\n",
    "    k.learner.recorder.plot()\n",
    "\n",
    "    #k.peek_data()\n",
    "\n",
    "    self = k\n",
    "    assert self.validation_dataset is not None\n",
    "    assert self.learner is not None\n",
    "\n",
    "    net = k.model\n",
    "    assert net is not None\n",
    "    net.to(device)\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        self.validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        self.test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    validation_tune_loader = torch.utils.data.DataLoader(\n",
    "        self.validation_tune_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        #sampler=validation_tune_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    def validation(model, device, config, val_loader, criterion):\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(val_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "    def run_inference(model, device, config, test_loader):\n",
    "        model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(test_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(train_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    self.log(\n",
    "                        f'Train Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(self.device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "            targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs, attention_masks)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(self.optimizer)\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        self.model.eval()\n",
    "        #self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_tuning_and_inference(self, test_loader, validation_tune_loader):\n",
    "        for e in range(1):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*8\n",
    "\n",
    "            losses, final_scores = self.train_one_epoch(validation_tune_loader)\n",
    "            self.log(f'[RESULT]: Tune_Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
    "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
    "            self.log(f'[RESULT]: Tune_Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "\n",
    "    losses, final_scores = validation(net, device, TrainGlobalConfig, validation_loader, TrainGlobalConfig.criterion)\n",
    "    logger.info(f\"Val results: losses={losses}, final_scores={final_scores}\")\n",
    "\n",
    "    results = run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "    logger.info(f\"Test done, result len %d\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yf7HVOSB013k"
   },
   "outputs": [],
   "source": [
    "#test_model_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "x-2HyXf2G4et",
    "outputId": "6d1b4c41-d736-472e-b615-f8ce12b61b5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=DataBunch;\n",
       "\n",
       "Train: <__main__.DatasetRetriever object at 0x7fb69e66d2e8>;\n",
       "\n",
       "Valid: <__main__.DatasetRetriever object at 0x7fb5aaa19e80>;\n",
       "\n",
       "Test: None, model=ToxicSimpleNNModel(\n",
       "  (backbone): XLMRobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=2048, out_features=7, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function nll_loss at 0x7fb6a5f0ebf8>, metrics=[], true_wd=True, bn_wd=False, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Embedding(250002, 1024, padding_idx=1)\n",
       "  (1): Embedding(514, 1024, padding_idx=1)\n",
       "  (2): Embedding(1, 1024)\n",
       "  (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (4): Dropout(p=0.1, inplace=False)\n",
       "  (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (8): Dropout(p=0.1, inplace=False)\n",
       "  (9): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (10): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (11): Dropout(p=0.1, inplace=False)\n",
       "  (12): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (13): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (14): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (15): Dropout(p=0.1, inplace=False)\n",
       "  (16): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (17): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (18): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (19): Dropout(p=0.1, inplace=False)\n",
       "  (20): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (21): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (22): Dropout(p=0.1, inplace=False)\n",
       "  (23): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (24): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (25): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (26): Dropout(p=0.1, inplace=False)\n",
       "  (27): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (28): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (29): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (30): Dropout(p=0.1, inplace=False)\n",
       "  (31): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (32): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (33): Dropout(p=0.1, inplace=False)\n",
       "  (34): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (35): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (36): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (37): Dropout(p=0.1, inplace=False)\n",
       "  (38): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (39): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (40): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (41): Dropout(p=0.1, inplace=False)\n",
       "  (42): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (43): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (44): Dropout(p=0.1, inplace=False)\n",
       "  (45): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (46): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (47): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (48): Dropout(p=0.1, inplace=False)\n",
       "  (49): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (50): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (51): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (52): Dropout(p=0.1, inplace=False)\n",
       "  (53): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (54): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (55): Dropout(p=0.1, inplace=False)\n",
       "  (56): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (57): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (58): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (59): Dropout(p=0.1, inplace=False)\n",
       "  (60): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (61): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (62): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (63): Dropout(p=0.1, inplace=False)\n",
       "  (64): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (65): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (66): Dropout(p=0.1, inplace=False)\n",
       "  (67): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (68): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (69): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (70): Dropout(p=0.1, inplace=False)\n",
       "  (71): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (72): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (73): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (74): Dropout(p=0.1, inplace=False)\n",
       "  (75): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (76): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (77): Dropout(p=0.1, inplace=False)\n",
       "  (78): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (79): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (80): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (81): Dropout(p=0.1, inplace=False)\n",
       "  (82): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (83): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (84): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (85): Dropout(p=0.1, inplace=False)\n",
       "  (86): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (87): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (88): Dropout(p=0.1, inplace=False)\n",
       "  (89): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (90): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (91): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (92): Dropout(p=0.1, inplace=False)\n",
       "  (93): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (94): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (95): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (96): Dropout(p=0.1, inplace=False)\n",
       "  (97): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (98): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (99): Dropout(p=0.1, inplace=False)\n",
       "  (100): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (101): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (102): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (103): Dropout(p=0.1, inplace=False)\n",
       "  (104): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (105): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (106): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (107): Dropout(p=0.1, inplace=False)\n",
       "  (108): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (109): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (110): Dropout(p=0.1, inplace=False)\n",
       "  (111): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (112): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (113): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (114): Dropout(p=0.1, inplace=False)\n",
       "  (115): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (116): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (117): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (118): Dropout(p=0.1, inplace=False)\n",
       "  (119): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (120): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (121): Dropout(p=0.1, inplace=False)\n",
       "  (122): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (123): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (124): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (125): Dropout(p=0.1, inplace=False)\n",
       "  (126): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (127): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (128): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (129): Dropout(p=0.1, inplace=False)\n",
       "  (130): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (131): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (132): Dropout(p=0.1, inplace=False)\n",
       "  (133): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (134): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (135): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (136): Dropout(p=0.1, inplace=False)\n",
       "  (137): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (138): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (139): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (140): Dropout(p=0.1, inplace=False)\n",
       "  (141): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (142): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (143): Dropout(p=0.1, inplace=False)\n",
       "  (144): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (145): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (146): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (147): Dropout(p=0.1, inplace=False)\n",
       "  (148): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (149): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (150): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (151): Dropout(p=0.1, inplace=False)\n",
       "  (152): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (153): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (154): Dropout(p=0.1, inplace=False)\n",
       "  (155): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (156): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (157): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (158): Dropout(p=0.1, inplace=False)\n",
       "  (159): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (160): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (161): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (162): Dropout(p=0.1, inplace=False)\n",
       "  (163): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (164): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (165): Dropout(p=0.1, inplace=False)\n",
       "  (166): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (167): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (168): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (169): Dropout(p=0.1, inplace=False)\n",
       "  (170): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (171): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (172): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (173): Dropout(p=0.1, inplace=False)\n",
       "  (174): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (175): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (176): Dropout(p=0.1, inplace=False)\n",
       "  (177): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (178): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (179): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (180): Dropout(p=0.1, inplace=False)\n",
       "  (181): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (182): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (183): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (184): Dropout(p=0.1, inplace=False)\n",
       "  (185): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (186): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (187): Dropout(p=0.1, inplace=False)\n",
       "  (188): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (189): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (190): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (191): Dropout(p=0.1, inplace=False)\n",
       "  (192): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (193): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (194): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (195): Dropout(p=0.1, inplace=False)\n",
       "  (196): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (197): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (198): Dropout(p=0.1, inplace=False)\n",
       "  (199): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (200): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (201): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (202): Dropout(p=0.1, inplace=False)\n",
       "  (203): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (204): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (205): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (206): Dropout(p=0.1, inplace=False)\n",
       "  (207): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (208): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (209): Dropout(p=0.1, inplace=False)\n",
       "  (210): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (211): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (212): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (213): Dropout(p=0.1, inplace=False)\n",
       "  (214): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (215): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (216): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (217): Dropout(p=0.1, inplace=False)\n",
       "  (218): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (219): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (220): Dropout(p=0.1, inplace=False)\n",
       "  (221): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (222): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (223): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (224): Dropout(p=0.1, inplace=False)\n",
       "  (225): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (226): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (227): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (228): Dropout(p=0.1, inplace=False)\n",
       "  (229): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (230): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (231): Dropout(p=0.1, inplace=False)\n",
       "  (232): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (233): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (234): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (235): Dropout(p=0.1, inplace=False)\n",
       "  (236): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (237): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (238): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (239): Dropout(p=0.1, inplace=False)\n",
       "  (240): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (241): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (242): Dropout(p=0.1, inplace=False)\n",
       "  (243): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (244): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (245): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (246): Dropout(p=0.1, inplace=False)\n",
       "  (247): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (248): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (249): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (250): Dropout(p=0.1, inplace=False)\n",
       "  (251): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (252): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (253): Dropout(p=0.1, inplace=False)\n",
       "  (254): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (255): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (256): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (257): Dropout(p=0.1, inplace=False)\n",
       "  (258): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (259): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (260): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (261): Dropout(p=0.1, inplace=False)\n",
       "  (262): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (263): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (264): Dropout(p=0.1, inplace=False)\n",
       "  (265): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (266): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (267): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (268): Dropout(p=0.1, inplace=False)\n",
       "  (269): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (270): Tanh()\n",
       "  (271): Dropout(p=0.3, inplace=False)\n",
       "  (272): Linear(in_features=2048, out_features=7, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#k.learner\n",
    "#k.learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYersgVyN2Jx"
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch\n",
    "\n",
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.core import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.vision import *\n",
    "from fastai.basic_train import *\n",
    "\n",
    "def len_parallelloader(self):\n",
    "  return len(self._loader._loader)\n",
    "pl.PerDeviceLoader.__len__ = len_parallelloader\n",
    "\n",
    "\n",
    "class TPUDistributed(LearnerCallback):\n",
    "  def __init__(self, learn:Learner):\n",
    "    super().__init__(learn)\n",
    "    self.device = xm.xla_device()\n",
    "\n",
    "  def _change_dl(self,dl, shuffle):\n",
    "    old_dl = dl\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      dl.dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=shuffle\n",
    "    )\n",
    "    new_dl = dl.new(shuffle=False, sampler=sampler)\n",
    "\n",
    "    return old_dl,new_dl,sampler\n",
    "\n",
    "\n",
    "  def on_train_begin(self, **kwargs:Any)->None:\n",
    "    self.learn.model = self.learn.model.to(self.device)\n",
    "    self.learn.opt.lr = self.learn.opt.lr*xm.xrt_world_size()\n",
    "\n",
    "    shuffle = self.data.train_dl.init_kwargs['shuffle'] if hasattr(self.data.train_dl, 'init_kwargs') else True\n",
    "    self.old_sampler_train_dl,self.data.train_dl,self.train_sampler = self._change_dl(self.data.train_dl, shuffle)\n",
    "\n",
    "    if hasattr(self.data, 'valid_dl') and self.data.valid_dl is not None:\n",
    "      self.old_sampler_valid_dl,self.data.valid_dl,self.valid_sampler = self._change_dl(self.data.valid_dl, shuffle)\n",
    "  def on_epoch_begin(self,**kwargs:Any)->None:\n",
    "    self.old_train_dl = self.data.train_dl\n",
    "    self.learn.data.train_dl = pl.ParallelLoader(self.old_train_dl, [self.device]).per_device_loader(self.device)\n",
    "    self.learn.data.train_dl.dataset = None #self.old_train_dl.dataset\n",
    "\n",
    "    if hasattr(self.data, 'valid_dl') and self.data.valid_dl is not None:\n",
    "      self.old_valid_dl = self.learn.data.valid_dl\n",
    "      self.learn.data.valid_dl = pl.ParallelLoader(self.old_valid_dl, [self.device]).per_device_loader(self.device)\n",
    "\n",
    "      self.learn.data.valid_dl.dataset = self.old_valid_dl.dataset\n",
    "      self.learn.data.valid_dl.dl = self.learn.data.valid_dl._loader._loader\n",
    "\n",
    "  def on_backward_end(self, **kwargs:Any)->None:\n",
    "    xm.optimizer_step(self.learn.opt)\n",
    "\n",
    "    return {'skip_step': True}\n",
    "\n",
    "  def on_epoch_end(self,**kwargs:Any)->None:\n",
    "    self.learn.data.train_dl = self.old_train_dl\n",
    "    self.learn.data.valid_dl = self.old_valid_dl\n",
    "\n",
    "  def on_train_end(self,**kwargs:Any)->None:\n",
    "    self.learn.data.train_dl = self.old_sampler_train_dl\n",
    "    self.learn.data.valid_dl = self.old_sampler_valid_dl\n",
    "\n",
    "\n",
    "def _to_tpu_distributed(learn:Learner) -> Learner:\n",
    "  #Learner.fit = _fit_tpu\n",
    "  learn.callback_fns.append(TPUDistributed)\n",
    "\n",
    "  return learn\n",
    "\n",
    "\n",
    "Learner.to_tpu_distributed = _to_tpu_distributed\n",
    "\n",
    "\n",
    "path = untar_data(URLs.FOOD)\n",
    "def filelist2df(path):\n",
    "    df = pd.read_csv(path, delimiter='/', header=None, names=['label', 'name'])\n",
    "    df['name'] =  df['label'].astype(str) + \"/\" + df['name'].astype(str) + \".jpg\"\n",
    "\n",
    "    return df\n",
    "\n",
    "train_path = path/'train.txt'\n",
    "test_path = path/'test.txt'\n",
    "\n",
    "def train_loop(index):\n",
    "  #data = (ImageList.from_df(df=train_df, path=path/'images', cols=1)\n",
    "  #        .random_split_by_pct(0.2)\n",
    "  #        .label_from_df(cols=0)\n",
    "  #        .transform(get_transforms(), size=224)\n",
    "  #        .databunch(bs=32, num_workers=0)\n",
    "  #        .normalize(imagenet_stats))\n",
    "  #learn = cnn_learner(data, models.resnet152, metrics=accuracy).to_tpu_distributed()\n",
    "  learn = k.setup_learner(loss_func=LabelSmoothing()).to_tpu_distributed()\n",
    "  print('hello')\n",
    "  #learn.lr_find(start_lr=1e-7, end_lr=1e-4, num_it=200)\n",
    "  #earn.recorder.plot()\n",
    "  learn.fit_one_cycle(2, max_lr=5e-6)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  xmp.spawn(train_loop,args=(),  nprocs=8, start_method='fork')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Copy of shonenkov_training_pipeline.ipynb",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
