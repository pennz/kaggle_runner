{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pennz/kaggle_runner/blob/master/hub/shonenkov_training_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5g67DNAUQyh"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-Gf0MyQL8Ys"
   },
   "outputs": [],
   "source": [
    "#@title Kaggle thing\n",
    "\n",
    "text = \"{   \\\"username\\\": \\\"k1gaggle\\\",   \\\"key\\\": \\\"721ad312727847f609212568cf015532\\\",   \\\"competition\\\": \\\"siim-acr-pneumothorax-segmentation\\\" }\" #@param {type:\"string\"}\n",
    "#dropdown = '1st option' #@param [\"1st option\", \"2nd option\", \"3rd option\"]\n",
    "#text_and_dropdown = 'value' #@param [\"1st option\", \"2nd option\", \"3rd option\"] {allow-input: true}\n",
    "!mkdir -p ~/.kaggle/\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    f.write(text)\n",
    "#print(dropdown)\n",
    "#print(text_and_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A6Yz0kroUQy0",
    "lines_to_next_cell": 2,
    "outputId": "6143fcc0-c715-486b-e48e-1ff099dbbb29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job # 0 in a separate thread.\n"
     ]
    }
   ],
   "source": [
    "%%bash --bg\n",
    "#python3 -m pip show kaggle_runner || ( git clone https://github.com/pennz/kaggle_runner; \\\n",
    "#mv kaggle_runner k && \\\n",
    "#mv k/* . && mv k/.* .; \\\n",
    "#python3 -m pip install -e .; \\\n",
    "#git submodule update --init; \\\n",
    "#export PATH=$PWD/bin:$PATH; \\\n",
    "#entry.sh; echo You can wait to setup for remote access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "udc5hHgUUQzD"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run(\"\"\"python3 -m pip || apt install -y python3-pip; python3 -m pip show kaggle_runner || ( git clone https://github.com/pennz/kaggle_runner;\n",
    "mv kaggle_runner k && mv k/* . && mv k/.* .;\n",
    "python3 -m pip install -e .;\n",
    "git submodule update --init;\n",
    "export PATH=$PWD/bin:$PATH; entry.sh & make rvs & echo You can wait to setup for remote access)\n",
    "\"\"\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "udc5HhgUUQzD",
    "lines_to_next_cell": 2,
    "outputId": "c5629da8-33e5-4a64-9c95-0eedbe0a4213"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='make install_dep; mkdir -p /root/.ssh ; make kr; wait; make xla &', returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run(\"make install_dep; mkdir -p /root/.ssh ; make kr; wait; make gitlab; make xla &\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "9aQY9GJrVmGl"
   },
   "outputs": [],
   "source": [
    "#!yes | make vim &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6lwyNn2ONmLR",
    "outputId": "7bcf8eb0-016d-4d7e-8038-3d9acc3157d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: command: Command not found\n",
      "/bin/bash: xclip: command not found\n",
      "Makefile:277: recipe for target 'kaggle' failed\n",
      "make: [kaggle] Error 127 (ignored)\n",
      "mkdir -p /kaggle/input\n",
      "(cmp_name=\"jigsaw-multilingual-toxic-comment-classification\"; \\\n",
      "kaggle competitions download -p /kaggle/input/$cmp_name $cmp_name; \\\n",
      "cd /kaggle/input/$cmp_name; unzip '*.zip') &\n",
      "sed 's/\"\\(.*\\)\".*/\\1/' .datasets | xargs -I{} bash -xc 'folder=$(echo {} | sed \"s/.*\\///\"); kaggle datasets download --unzip -p /kaggle/input/${folder} {}' &\n",
      "++ echo gabrichy/nvidiaapex\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=nvidiaapex\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/nvidiaapex gabrichy/nvidiaapex\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "Downloading nvidiaapex.zip to /kaggle/input/nvidiaapex\n",
      "\r",
      "  0% 0.00/523k [00:00<?, ?B/s]\n",
      "\r",
      "100% 523k/523k [00:00<00:00, 79.0MB/s]\n",
      "++ echo matsuik/ppbert\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=ppbert\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/ppbert matsuik/ppbert\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading sample_submission.csv to /kaggle/input/jigsaw-multilingual-toxic-comment-classification\n",
      "  0% 0.00/612k [00:00<?, ?B/s]\n",
      "100% 612k/612k [00:00<00:00, 41.4MB/s]\n",
      "Downloading ppbert.zip to /kaggle/input/ppbert\n",
      "  0% 0.00/3.35M [00:00<?, ?B/s]\n",
      "100% 3.35M/3.35M [00:00<00:00, 112MB/s]\n",
      "++ echo maxjeblick/bert-pretrained-models\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=bert-pretrained-models\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/bert-pretrained-models maxjeblick/bert-pretrained-models\n",
      "Downloading jigsaw-toxic-comment-train-processed-seqlen128.csv.zip to /kaggle/input/jigsaw-multilingual-toxic-comment-classification\n",
      "  0% 0.00/79.6M [00:00<?, ?B/s]Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      " 16% 13.0M/79.6M [00:00<00:02, 26.5MB/s]Downloading bert-pretrained-models.zip to /kaggle/input/bert-pretrained-models\n",
      " 92% 73.0M/79.6M [00:02<00:00, 32.1MB/s]\n",
      "100% 79.6M/79.6M [00:02<00:00, 37.6MB/s]\n",
      "  1% 65.0M/8.10G [00:01<05:29, 26.2MB/s]Downloading test-processed-seqlen128.csv.zip to /kaggle/input/jigsaw-multilingual-toxic-comment-classification\n",
      " 84% 25.0M/29.8M [00:00<00:00, 31.3MB/s]\n",
      "100% 29.8M/29.8M [00:00<00:00, 40.8MB/s]\n",
      "  2% 131M/8.10G [00:03<02:49, 50.4MB/s]Downloading validation.csv.zip to /kaggle/input/jigsaw-multilingual-toxic-comment-classification\n",
      "  0% 0.00/1.35M [00:00<?, ?B/s]\n",
      "100% 1.35M/1.35M [00:00<00:00, 194MB/s]\n",
      "  2% 187M/8.10G [00:04<02:12, 64.1MB/s]Downloading jigsaw-unintended-bias-train-processed-seqlen128.csv.zip to /kaggle/input/jigsaw-multilingual-toxic-comment-classification\n",
      "  8% 640M/8.10G [00:11<03:17, 40.7MB/s]\n",
      "100% 650M/650M [00:07<00:00, 94.2MB/s]\n",
      "  8% 651M/8.10G [00:11<04:47, 27.9MB/s]Downloading test.csv.zip to /kaggle/input/jigsaw-multilingual-toxic-comment-classification\n",
      "  8% 659M/8.10G [00:11<04:14, 31.5MB/s]\n",
      "100% 12.4M/12.4M [00:00<00:00, 60.8MB/s]\n",
      "  8% 673M/8.10G [00:12<04:26, 30.0MB/s]Downloading jigsaw-unintended-bias-train.csv.zip to /kaggle/input/jigsaw-multilingual-toxic-comment-classification\n",
      " 99% 289M/292M [00:03<00:00, 96.0MB/s]\n",
      "100% 292M/292M [00:03<00:00, 81.0MB/s]\n",
      " 11% 909M/8.10G [00:16<02:27, 52.4MB/s]Downloading validation-processed-seqlen128.csv.zip to /kaggle/input/jigsaw-multilingual-toxic-comment-classification\n",
      "  0% 0.00/3.44M [00:00<?, ?B/s]\n",
      "100% 3.44M/3.44M [00:00<00:00, 234MB/s]\n",
      " 11% 942M/8.10G [00:17<02:25, 53.1MB/s]Downloading jigsaw-toxic-comment-train.csv.zip to /kaggle/input/jigsaw-multilingual-toxic-comment-classification\n",
      " 12% 0.99G/8.10G [00:19<02:55, 43.6MB/s]\n",
      "100% 37.3M/37.3M [00:02<00:00, 18.4MB/s]\n",
      " 13% 1.08G/8.10G [00:20<01:25, 87.8MB/s]Archive:  test.csv.zip\n",
      " 14% 1.11G/8.10G [00:20<01:14, 100MB/s] \n",
      "\n",
      "Archive:  test-processed-seqlen128.csv.zip\n",
      " 15% 1.21G/8.10G [00:22<03:42, 33.2MB/s]\n",
      "\n",
      "Archive:  validation-processed-seqlen128.csv.zip\n",
      " 15% 1.23G/8.10G [00:23<04:18, 28.6MB/s]\n",
      "\n",
      "Archive:  jigsaw-unintended-bias-train-processed-seqlen128.csv.zip\n",
      " 44% 3.55G/8.10G [02:15<03:38, 22.4MB/s]\n",
      "\n",
      "Archive:  jigsaw-toxic-comment-train-processed-seqlen128.csv.zip\n",
      " 48% 3.92G/8.10G [02:31<02:39, 28.3MB/s]\n",
      "\n",
      "Archive:  validation.csv.zip\n",
      " 48% 3.93G/8.10G [02:31<02:27, 30.3MB/s]\n",
      "\n",
      "Archive:  jigsaw-unintended-bias-train.csv.zip\n",
      " 57% 4.65G/8.10G [03:03<03:06, 19.9MB/s]\n",
      "\n",
      "Archive:  jigsaw-toxic-comment-train.csv.zip\n",
      " 58% 4.74G/8.10G [03:07<03:24, 17.7MB/s]\n",
      "\n",
      "8 archives were successfully processed.\n",
      "100% 8.09G/8.10G [04:17<00:00, 33.9MB/s]\n",
      "100% 8.10G/8.10G [04:17<00:00, 33.8MB/s]\n",
      "++ echo k1gaggle/clean-pickle-for-jigsaw-toxicity\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=clean-pickle-for-jigsaw-toxicity\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/clean-pickle-for-jigsaw-toxicity k1gaggle/clean-pickle-for-jigsaw-toxicity\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading clean-pickle-for-jigsaw-toxicity.zip to /kaggle/input/clean-pickle-for-jigsaw-toxicity\n",
      " 99% 1.00G/1.01G [00:10<00:00, 94.2MB/s]\n",
      "100% 1.01G/1.01G [00:10<00:00, 100MB/s] \n",
      "++ echo k1gaggle/jigsaw-multilingula-toxicity-token-encoded\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=jigsaw-multilingula-toxicity-token-encoded\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/jigsaw-multilingula-toxicity-token-encoded k1gaggle/jigsaw-multilingula-toxicity-token-encoded\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading jigsaw-multilingula-toxicity-token-encoded.zip to /kaggle/input/jigsaw-multilingula-toxicity-token-encoded\n",
      " 99% 457M/463M [00:07<00:00, 43.5MB/s]\n",
      "100% 463M/463M [00:07<00:00, 64.5MB/s]\n",
      "++ echo shonenkov/open-subtitles-toxic-pseudo-labeling\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=open-subtitles-toxic-pseudo-labeling\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/open-subtitles-toxic-pseudo-labeling shonenkov/open-subtitles-toxic-pseudo-labeling\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading open-subtitles-toxic-pseudo-labeling.zip to /kaggle/input/open-subtitles-toxic-pseudo-labeling\n",
      " 87% 36.0M/41.5M [00:01<00:00, 20.9MB/s]\n",
      "100% 41.5M/41.5M [00:01<00:00, 31.8MB/s]\n",
      "++ echo k1gaggle/jigsaw-toxicity-train-data-with-aux\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=jigsaw-toxicity-train-data-with-aux\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/jigsaw-toxicity-train-data-with-aux k1gaggle/jigsaw-toxicity-train-data-with-aux\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading jigsaw-toxicity-train-data-with-aux.zip to /kaggle/input/jigsaw-toxicity-train-data-with-aux\n",
      " 99% 477M/481M [00:05<00:00, 64.6MB/s]\n",
      "100% 481M/481M [00:05<00:00, 99.2MB/s]\n",
      "++ echo shonenkov/jigsaw-public-baseline-train-data\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=jigsaw-public-baseline-train-data\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/jigsaw-public-baseline-train-data shonenkov/jigsaw-public-baseline-train-data\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading jigsaw-public-baseline-train-data.zip to /kaggle/input/jigsaw-public-baseline-train-data\n",
      " 99% 471M/474M [00:09<00:00, 32.6MB/s]\n",
      "100% 474M/474M [00:09<00:00, 50.4MB/s]\n",
      "++ echo shonenkov/jigsaw-public-baseline-results\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=jigsaw-public-baseline-results\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/jigsaw-public-baseline-results shonenkov/jigsaw-public-baseline-results\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading jigsaw-public-baseline-results.zip to /kaggle/input/jigsaw-public-baseline-results\n",
      "  0% 0.00/297k [00:00<?, ?B/s]\n",
      "100% 297k/297k [00:00<00:00, 44.6MB/s]\n",
      "++ echo kashnitsky/jigsaw-multilingual-toxic-test-translated\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=jigsaw-multilingual-toxic-test-translated\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/jigsaw-multilingual-toxic-test-translated kashnitsky/jigsaw-multilingual-toxic-test-translated\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading jigsaw-multilingual-toxic-test-translated.zip to /kaggle/input/jigsaw-multilingual-toxic-test-translated\n",
      " 36% 9.00M/25.0M [00:00<00:00, 35.4MB/s]\n",
      "100% 25.0M/25.0M [00:00<00:00, 72.0MB/s]\n",
      "++ echo pranshu29/jigsaw-new-balanced-dataset\n",
      "++ sed 's/.*\\///'\n",
      "+ folder=jigsaw-new-balanced-dataset\n",
      "+ kaggle datasets download --unzip -p /kaggle/input/jigsaw-new-balanced-dataset pranshu29/jigsaw-new-balanced-dataset\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading jigsaw-new-balanced-dataset.zip to /kaggle/input/jigsaw-new-balanced-dataset\n",
      " 98% 545M/557M [00:16<00:00, 38.3MB/s]\n",
      "100% 557M/557M [00:16<00:00, 35.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "!make dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3ccg5MJlyvLy",
    "outputId": "9acd8cc0-d58a-4719-842f-3808b028d024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: command: Command not found\n",
      "Installing coverage\n",
      "#python3 -m pip show coverage &>/dev/null || python3 -m pip install -q coverage\n",
      "Installing ipdb\n",
      "#python3 -m pip show ipdb &>/dev/null || python3 -m pip install -q ipdb\n",
      "Installing pyicu\n",
      "#python3 -m pip show pyicu &>/dev/null || python3 -m pip install -q pyicu\n",
      "Installing pycld2\n",
      "#python3 -m pip show pycld2 &>/dev/null || python3 -m pip install -q pycld2\n",
      "Installing polyglot\n",
      "#python3 -m pip show polyglot &>/dev/null || python3 -m pip install -q polyglot\n",
      "Installing textstat\n",
      "#python3 -m pip show textstat &>/dev/null || python3 -m pip install -q textstat\n",
      "Installing googletrans\n",
      "#python3 -m pip show googletrans &>/dev/null || python3 -m pip install -q googletrans\n",
      "Installing transformers==2.5.1\n",
      "#python3 -m pip show transformers==2.5.1 &>/dev/null || python3 -m pip install -q transformers==2.5.1\n",
      "Installing pandarallel\n",
      "#python3 -m pip show pandarallel &>/dev/null || python3 -m pip install -q pandarallel\n",
      "Installing catalyst==20.4.2\n",
      "#python3 -m pip show catalyst==20.4.2 &>/dev/null || python3 -m pip install -q catalyst==20.4.2\n",
      "Installing colorama\n",
      "#python3 -m pip show colorama &>/dev/null || python3 -m pip install -q colorama\n",
      "Installing parse\n",
      "#python3 -m pip show parse &>/dev/null || python3 -m pip install -q parse\n",
      "Installing pysnooper\n",
      "#python3 -m pip show pysnooper &>/dev/null || python3 -m pip install -q pysnooper\n",
      "Installing ripdb\n",
      "#python3 -m pip show ripdb &>/dev/null || python3 -m pip install -q ripdb\n",
      "Installing pytest-logger\n",
      "#python3 -m pip show pytest-logger &>/dev/null || python3 -m pip install -q pytest-logger\n",
      "Installing python_logging_rabbitmq\n",
      "#python3 -m pip show python_logging_rabbitmq &>/dev/null || python3 -m pip install -q python_logging_rabbitmq\n",
      "python3 -m pip show pytest | grep \"Version: 5.\" &>/dev/null || (python3 -m pip install --upgrade pytest && python3 -m pip install --upgrade pytest-cov)\n",
      "Collecting pytest\n",
      "  Using cached pytest-5.4.3-py3-none-any.whl (248 kB)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (1.8.2)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest) (1.6.1)\n",
      "Collecting pluggy<1.0,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (8.4.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging->pytest) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest) (3.1.0)\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pluggy, pytest\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 0.7.1\n",
      "    Uninstalling pluggy-0.7.1:\n",
      "      Successfully uninstalled pluggy-0.7.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 3.6.4\n",
      "    Uninstalling pytest-3.6.4:\n",
      "      Successfully uninstalled pytest-3.6.4\n",
      "Successfully installed pluggy-0.13.1 pytest-5.4.3\n",
      "Collecting pytest-cov\n",
      "  Downloading pytest_cov-2.10.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied, skipping upgrade: pytest>=4.6 in /usr/local/lib/python3.6/dist-packages (from pytest-cov) (5.4.3)\n",
      "Collecting coverage>=4.4\n",
      "  Downloading coverage-5.1-cp36-cp36m-manylinux1_x86_64.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 5.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->pytest-cov) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->pytest-cov) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->pytest-cov) (1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->pytest-cov) (1.8.2)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->pytest-cov) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->pytest-cov) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->pytest-cov) (8.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest>=4.6->pytest-cov) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging->pytest>=4.6->pytest-cov) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=4.6->pytest-cov) (3.1.0)\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: coverage, pytest-cov\n",
      "  Attempting uninstall: coverage\n",
      "    Found existing installation: coverage 3.7.1\n",
      "    Uninstalling coverage-3.7.1:\n",
      "      Successfully uninstalled coverage-3.7.1\n",
      "Successfully installed coverage-5.1 pytest-cov-2.10.0\n",
      "for p in coverage ipdb pyicu pycld2 polyglot textstat googletrans transformers==2.5.1 pandarallel catalyst==20.4.2 colorama parse pysnooper ripdb pytest-logger python_logging_rabbitmq pytest; do (python3 -m pip show $p &>/dev/null || python3 -m pip install -q $p) & done; wait\n",
      "\u001b[K     |████████████████████████████████| 102 kB 2.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 225 kB 3.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 126 kB 3.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 499 kB 3.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 323 kB 3.3 MB/s \n",
      "\u001b[K     |████████████▍                   | 174 kB 2.1 MB/s eta 0:00:01\u001b[?25h\u001b[?25l     |████████████████████████████████| 3.0 MB 10.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 451 kB 2.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 2.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 55 kB 1.0 MB/s \n",
      "\u001b[K     |█████████████████               | 21.9 MB 1.3 MB/s eta 0:00:15     |████████████████████████████████| 42 kB 487 kB/s \n",
      "\u001b[K\n",
      "\u001b[K     |████████████████████████████████| 883 kB 22.5 MB/s \n",
      "\u001b[K  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone     |███████████████████▉            | 25.7 MB 1.3 MB/s eta 0:00:12\n",
      "\u001b[K     |████████████████████▉           | 27.0 MB 1.3 MB/s eta 0:00:11\u001b[?25h  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████▊   | 37.2 MB 1.3 MB/s eta 0:00:04     |████████████████████████████████| 63 kB 511 kB/s \n",
      "\u001b[K     |████████████████████████████████| 903 kB 1.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 18.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 41.4 MB 59 kB/s \n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
      "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 98 kB 4.2 MB/s \n",
      "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[?25h  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[?25h  Building wheel for pyicu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "#python3 -m pip install -q eumetsat expect &\n",
      "#conda install -y -c eumetsat expect & # https://askubuntu.com/questions/1047900/unbuffer-stopped-working-months-ago\n",
      "make install_dep coverage ipdb pyicu pycld2 polyglot textstat googletrans transformers==2.5.1 pandarallel catalyst==20.4.2 colorama parse pysnooper ripdb pytest-logger python_logging_rabbitmq pytest done\n"
     ]
    }
   ],
   "source": [
    "!make install_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "MYBRID_uUQzb",
    "lines_to_next_cell": 2,
    "outputId": "1157b6c5-9d2c-49bd-d37a-ae4a98e5876a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__call__', <function LevelMapper.__call__ at 0x7f19aba85378>), ('__init__', <function LevelMapper.__init__ at 0x7f19aba852f0>)]\n",
      "[('__call__', <function BalancedPositiveNegativeSampler.__call__ at 0x7f19ab9bd048>), ('__init__', <function BalancedPositiveNegativeSampler.__init__ at 0x7f19ab9acf28>)]\n",
      "[('__init__', <function BoxCoder.__init__ at 0x7f19ab9c5488>), ('decode', <function BoxCoder.decode at 0x7f19ab9c5620>), ('decode_single', <function BoxCoder.decode_single at 0x7f19ab9c56a8>), ('encode', <function BoxCoder.encode at 0x7f19ab9c5510>), ('encode_single', <function BoxCoder.encode_single at 0x7f19ab9c5598>)]\n",
      "[('__call__', <function Matcher.__call__ at 0x7f19ab9c5378>), ('__init__', <function Matcher.__init__ at 0x7f19ab9c57b8>), ('set_low_quality_matches_', <function Matcher.set_low_quality_matches_ at 0x7f19ab9c5400>)]\n",
      "[('__init__', <function ImageList.__init__ at 0x7f19ab9c5950>), ('to', <function ImageList.to at 0x7f19ab9c58c8>)]\n",
      "[('__init__', <function Timebase.__init__ at 0x7f19ab86f6a8>)]\n",
      "[('__init__', <function VideoMetaData.__init__ at 0x7f19ab86f9d8>)]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import kaggle_runner\n",
    "reload(kaggle_runner)\n",
    "from kaggle_runner import may_debug, logger\n",
    "from kaggle_runner.modules.ToxicSimpleNNModel import ToxicSimpleNNModel\n",
    "from kaggle_runner.kernels.Shonenkov import Shonenkov, ShonenkovChangeInner\n",
    "from kaggle_runner.callbacks import CheckGrad,_check_grad\n",
    "from kaggle_runner.metrics.meters import AverageMeter, RocAucMeter\n",
    "from kaggle_runner.losses import LabelSmoothing\n",
    "from kaggle_runner.datasets.transfomers import *\n",
    "from kaggle_runner import defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQfQoj4PUQzl",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['XLA_USE_BF16'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jryGiXHWUQzp"
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qyEzTQDUQzu",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njd0Gd2TUQz0",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0k4-gbQhUQz5",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63pHGleDUQ0D",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.core import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks.misc import StopAfterNBatches\n",
    "from fastai.callbacks import *\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from fastai.text.transform import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMbcpmJHUQ0H",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "-NTVjOreUQ0L",
    "outputId": "8d9f5c34-9082-4116-b035-51e72ce589b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !python3 -m pip install nltk > /dev/null\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfgDxZSiUQ0O"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JpTc5nfQUQ0T",
    "outputId": "d27fe5e5-1c44-4151-8cec-c521ae2aa9c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=4, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itAbsNWXUQ0W",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHro74jvUQ0d",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = f'/kaggle' # for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJD6tJMaUQ0i"
   },
   "outputs": [],
   "source": [
    "def get_toxic_comments(df):\n",
    "    df = df[~df['comment_text'].isna()]\n",
    "    df = df.drop_duplicates(subset='comment_text')\n",
    "    df['toxic'] = df['toxic'].round().astype(np.int)\n",
    "\n",
    "    return df[df['toxic'] == 1].comment_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MpvJJZsbUQ0n",
    "outputId": "bbf898b6-e2ba-4281-d3a2-b6bfa04f6be4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='[ -f train.pkl ] || cp /kaggle/input/clean-pickle-for-jigsaw-toxicity/*pkl .', returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #![ -f train.pkl ] || cp /kaggle/input/clean-pickle-for-jigsaw-toxicity/*pkl .\n",
    "subprocess.run('[ -f train.pkl ] || cp /kaggle/input/clean-pickle-for-jigsaw-toxicity/*pkl .', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmkH1Q0qUQ0t",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    \"\"\" Global Config for this notebook \"\"\"\n",
    "    num_workers = 0  # количество воркеров для loaders\n",
    "    batch_size = 16  # bs , 8 for GPU, 16 for TPU\n",
    "    n_epochs = 2  # количество эпох для обучения\n",
    "    lr = 0.5 * 1e-5 # стартовый learning rate (внутри логика работы с мульти TPU домножает на кол-во процессов)\n",
    "    fold_number = 0  # номер фолда для обучения\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True  # выводить принты\n",
    "    verbose_step = 25  # количество шагов для вывода принта\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # выполнять scheduler.step после вызова optimizer.step\n",
    "    validation_scheduler = True  # выполнять scheduler.step после валидации loss (например для плато)\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='max',\n",
    "        factor=0.7,\n",
    "        patience=0,\n",
    "        verbose=False,\n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0,\n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )\n",
    "    # --------------------\n",
    "\n",
    "    # -------------------\n",
    "    criterion = LabelSmoothing()\n",
    "    # -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB36YxahXT6Q"
   },
   "outputs": [],
   "source": [
    "from kaggle_runner.kernels.Shonenkov import Shonenkov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321,
     "referenced_widgets": [
      "ee035e7ededd4627bd0c945937717ade",
      "e327a32093be48ab9fabd6d2f93d4c0a",
      "8533de2bc7134b40abacf7a9b32f0012",
      "9c06456568914165afb73ac30228ba06",
      "5ba48862ccdf438c856405814423029e",
      "aa4533e1247846fcbf408a8be3a1359d",
      "00bdb7e7c8a0450da3d432a057de1ba1",
      "e09d32adcc504e85a050a2fd8a8ab55c",
      "ff36006e3f57454fa52fb7bf1647c21a",
      "4218600849cd4b2c8573917cf4e7b0cc",
      "9e062caa49984f7e9773e6d74a7cfadf",
      "552da08823e04c6693f3547e0e63f5cc",
      "5d534c0d831b4c3e8b25229ff5949366",
      "94309ba4763e4a74b447c70f15fd8e07",
      "88309c98031a4fbdbc0e247de4e70e4a",
      "e7bc1adc51564d86ae8be192335b184c",
      "72a4d2af4a164217843d68c79e473b6c",
      "e5783b109ca6436b8bb62f1b8a49706f",
      "68eb7fcbb8c544fcad5c271a10b2a303",
      "1098a4a2ecc84ac39c5ce37904f2a75d",
      "c6655ec11ee44c35be4ed66fec0bd0df",
      "ed53c651dc9d4ec598feff58b65fde84",
      "93ea82bc7a7545e29ad646a5f83b1358",
      "627f81a4d35742bba995678bb36436ad"
     ]
    },
    "colab_type": "code",
    "id": "furzEDtzUQ0x",
    "outputId": "4c57b26f-ff48-45b2-9f6f-a035e19ac52a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-28 00:50:16,617:utils:load ot.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee035e7ededd4627bd0c945937717ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-28 00:50:23,356:utils:None -> KernelRunningState.SAVE_SUBMISSION_DONE\n",
      "[DEBUG]2020-06-28 00:50:23,357:utils:load train.pkl\n",
      "[DEBUG]2020-06-28 00:50:27,761:utils:load val.pkl\n",
      "[DEBUG]2020-06-28 00:50:28,132:utils:state KernelRunningState.PREPARE_DATA_DONE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff36006e3f57454fa52fb7bf1647c21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a4d2af4a164217843d68c79e473b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2244861551.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-28 00:51:37,393:utils:state KernelRunningState.TRAINING_DONE\n",
      "[DEBUG]2020-06-28 00:51:37,394:utils:state KernelRunningState.EVL_DEV_DONE\n",
      "[DEBUG]2020-06-28 00:51:37,396:utils:load test.pkl\n",
      "[DEBUG]2020-06-28 00:51:38,242:utils:state KernelRunningState.SAVE_SUBMISSION_DONE\n"
     ]
    }
   ],
   "source": [
    "k = Shonenkov(torch.device(\"cpu\"), TrainGlobalConfig, metrics=None, loss_func=LabelSmoothing(), opt_func=None)\n",
    "k.run(dump_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "um8I7KBrY0Ur"
   },
   "outputs": [],
   "source": [
    "subprocess.run('mkdir ./models_xlmrobert/', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDt1SW3AUQ02",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "def save_model(self, output_dir=\"./models_xlmrobert/\"):\n",
    "    subprocess.run(f'mkdir {output_dir}', shell=True)\n",
    "    model = self.model\n",
    "\n",
    "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
    "\n",
    "# If we have a distributed model, save only the encapsulated model\n",
    "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    model_to_save.backbone.config.to_json_file(output_config_file)\n",
    "    #tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PN07kHKOUQ06",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
    "def test_model_fn(device=torch.device(\"cpu\")):\n",
    "    #device = xm.xla_device(devkind='TPU')\n",
    "    #device=torch.device(\"xla\")\n",
    "    logger.debug(\"Device used: %s\", device)\n",
    "\n",
    "    #k.run(dump_flag=True) # it seems it cannot save right\n",
    "    #k.run(dump_flag=False)\n",
    "    #k.peek_data()\n",
    "\n",
    "    self = k\n",
    "    assert self.validation_dataset is not None\n",
    "    #assert self.learner is not None\n",
    "\n",
    "    net = k.model\n",
    "    assert net is not None\n",
    "    net.to(device)\n",
    "\n",
    "    param_optimizer = list(self.model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    #optimizer = AdamW(optimizer_grouped_parameters, lr=TrainGlobalConfig.lr*xm.xrt_world_size())\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=TrainGlobalConfig.lr*8)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        self.train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        shuffle=False, # sampler is set, so shuffle here should be False\n",
    "        sampler=BalanceClassSampler(labels=k.train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        self.validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        self.test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    validation_tune_loader = torch.utils.data.DataLoader(\n",
    "        self.validation_tune_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        #sampler=validation_tune_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    def validation(model, device, config, val_loader, criterion):\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(val_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "    def run_inference(model, device, config, test_loader):\n",
    "        model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, ids) in enumerate(test_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "            break # just test one batch\n",
    "\n",
    "        return result\n",
    "\n",
    "    def train_one_epoch(model, device, config, train_loader, criterion, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(train_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.debug(\n",
    "                        f'Train Step {step}, bs: {batch_size}, loss: ' + \\\n",
    "                        f\"{losses.avg:.5f}, lr: {optimizer.param_groups[0]['lr']} final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, \" + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            _check_grad(optimizer)\n",
    "            #optimizer.step()\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "\n",
    "        model.eval()\n",
    "        #self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_tuning_and_inference(net, device, TrainGlobalConfig, validation_loader, train_loader):\n",
    "        for e in range(1):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*8\n",
    "\n",
    "            losses, final_scores = train_one_epoch(net, device, TrainGlobalConfig, train_loader, TrainGlobalConfig.criterion, )\n",
    "            self.log(f'[RESULT]: Tune_Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
    "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
    "            self.log(f'[RESULT]: Tune_Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "\n",
    "    #train_one_epoch(net, device, TrainGlobalConfig, train_loader, TrainGlobalConfig.criterion, optimizer)\n",
    "    #losses, final_scores = validation(net, device, TrainGlobalConfig, validation_loader, TrainGlobalConfig.criterion)\n",
    "    #logger.info(f\"Val results: losses={losses}, final_scores={final_scores}\")\n",
    "\n",
    "    results = run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "    logger.info(f\"Test done, result len %d\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qUwvlFnUQ0-",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_model(self, output_dir=\"./models_xlmrobert/\"):\n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    state_dict = torch.load(output_model_file)\n",
    "    self.model.load_state_dict(state_dict)\n",
    "\n",
    "    print(self.model)\n",
    "\n",
    "def test_save_model():\n",
    "    save_model(k)\n",
    "\n",
    "def test_load():\n",
    "    load_model(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9dqdFgwAZQgw"
   },
   "outputs": [],
   "source": [
    "test_save_model()\n",
    "test_load()\n",
    "test_model_fn()\n",
    "\n",
    "import pytest\n",
    "import sys\n",
    "\n",
    "def test_exit():\n",
    "    with pytest.raises(SystemExit) as exc:\n",
    "        # test goes here\n",
    "\n",
    "        if os.getenv(\"CI\") == \"true\":\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            sys.exit(0)\n",
    "    assert exc.value.code == 0\n",
    "\n",
    "test_exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSC6BrbwUQ2G"
   },
   "outputs": [],
   "source": [
    "\n",
    "if os.getenv(\"CI\") != \"true\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXNH-qcCUQ1B",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    from kaggle_runner.kernels.fastai_kernel import FastAIKernel\n",
    "    from kaggle_runner.runners.trainer import GPUTrainer\n",
    "    def _to_gpu(learn:Learner, k: FastAIKernel) -> Learner:\n",
    "        learn.callback_fns.append(partial(GPUTrainer, k=k))\n",
    "\n",
    "        return learn\n",
    "\n",
    "    Learner.to_gpu = _to_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Z1O-ZLfUQ1D",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    import pysnooper\n",
    "    from functools import partial\n",
    "\n",
    "    from hub.custom_fastai_callbacks.callbacks import GradientAccumulator\n",
    "    def debug_train(use_dist_cb=True):\n",
    "        logger.debug(f'debug train with{\" \" if use_dist_cb else \"OUT\"} to_tpu_distributed')\n",
    "        from kaggle_runner import defaults\n",
    "        _DEBUG = defaults.DEBUG\n",
    "        #defaults.DEBUG = True\n",
    "\n",
    "        param_optimizer = list(k.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'lr': 0., 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'lr': 0., 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        def AdamW_with_given_p(p_to_ignore, *args, **kargs):\n",
    "            kargs['lr']=TrainGlobalConfig.lr*8 #xm.xrt_world_size()\n",
    "\n",
    "            return AdamW(optimizer_grouped_parameters, *args, **kargs)\n",
    "\n",
    "        learn = k.create_learner(k, opt_func=AdamW_with_given_p,\n",
    "                                 loss_func=LabelSmoothing(),\n",
    "                                 wd=0.01,\n",
    "                                 callback_fns=[partial(GradientClipping, clip=0.5),\n",
    "                                               partial(CSVLogger, append=True),\n",
    "                                               partial(GradientAccumulator, num_iterations=4),\n",
    "                                               partial(CheckGrad, skip_loss_step=False, batch_size=k.config.batch_size)]\n",
    "                                 )\n",
    "        k.learner = learn\n",
    "\n",
    "        if use_dist_cb:\n",
    "            learn = learn.to_tpu_distributed()\n",
    "        else:\n",
    "            learn = learn.to_gpu(k)\n",
    "\n",
    "        #learn.callback_fns.append(CheckGrad)\n",
    "        #print('hello')\n",
    "        #learn.lr_find(start_lr=1e-7, end_lr=1e-2, num_it=200)\n",
    "        #learn.recorder.plot()\n",
    "        learn.fit_one_cycle(2, max_lr=3e-5)\n",
    "        #learn.fit(1, lr=4e-5) # original 0.5*e-5*8=4*e-5\n",
    "        defaults.DEBUG = _DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUzCsw_wUQ1F",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#debug_train(use_dist_cb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rO2ay06LUQ1I",
    "lines_to_next_cell": 2
   },
   "source": [
    "# XLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeL1PlTnUQ1J"
   },
   "outputs": [],
   "source": [
    "    import subprocess\n",
    "    subprocess.run('make xla', shell=True) # TODO add system call, we can get log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fUVZUIYUQ1M"
   },
   "outputs": [],
   "source": [
    "    import torch_xla\n",
    "    import torch_xla.distributed.data_parallel as dp\n",
    "    import torch_xla.utils.utils as xu\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWaNMw-rUQ1O"
   },
   "outputs": [],
   "source": [
    "    import fastai\n",
    "    from fastai import *\n",
    "    from fastai.core import *\n",
    "    from fastai.torch_core import *\n",
    "    from fastai.vision import *\n",
    "    from fastai.basic_train import *\n",
    "    from kaggle_runner.runners.tpu_trainer import TPUDistributed, TPUFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VomH5RztUQ1T",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
    "    def len_parallelloader(self):\n",
    "        return len(self._loader._loader)\n",
    "    pl.PerDeviceLoader.__len__ = len_parallelloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0d8Bzl8DUQ1X"
   },
   "outputs": [],
   "source": [
    "    def _to_tpu_distributed(learn:Learner) -> Learner:\n",
    "        learn.callback_fns.append(TPUDistributed)\n",
    "\n",
    "        return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6_DgUIyUQ1Z",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    Learner.to_tpu_distributed = _to_tpu_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_tjbI16UQ1g"
   },
   "outputs": [],
   "source": [
    "    from functools import partial\n",
    "    import pysnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nejQ0FwUQ1j",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    @pysnooper.snoop()\n",
    "    def train_loop(index, *args):\n",
    "        logger.debug(\"rank: %d entered train_loop\", index)\n",
    "\n",
    "        param_optimizer = list(k.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'lr': 4e-5, 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'lr': 4e-5, 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        def AdamW_with_given_p(p_to_ignore, *args, **kargs):\n",
    "            kargs['lr']=TrainGlobalConfig.lr*xm.xrt_world_size()\n",
    "\n",
    "            return AdamW(optimizer_grouped_parameters, *args, **kargs)\n",
    "\n",
    "        if index == 0:\n",
    "            time.sleep(1)\n",
    "        learn = k.create_learner(k, opt_func=AdamW_with_given_p,\n",
    "                                 loss_func=LabelSmoothing(),\n",
    "                                 wd=0.01,\n",
    "                                 callback_fns=[partial(GradientClipping, clip=0.5),\n",
    "                                               ShowGraph,\n",
    "                                               partial(CSVLogger, append=True),\n",
    "                                               partial(CheckGrad, skip_loss_step=False)]\n",
    "                                 ).to_tpu_distributed()\n",
    "        learn.lr_find(start_lr=1e-7, end_lr=1e-5, num_it=200)\n",
    "        learn.recorder.plot()\n",
    "        #learn.fit_one_cycle(3, max_lr=5e-6, wd=0.001)\n",
    "        learn.fit(1, lr=5e-6, wd=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTqfMd9JUQ1k",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    FLAGS={}\n",
    "    #xmp.spawn(train_loop, args=(FLAGS,),  nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oh5655pAUQ10"
   },
   "outputs": [],
   "source": [
    "    import pysnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YGBLtQ1UQ12",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    @pysnooper.snoop()\n",
    "    def _mp_fn(rank, flags, k=k):\n",
    "        device = xm.xla_device(devkind='TPU')\n",
    "        logger.debug(\"%s used for xla_device\" % device)\n",
    "        net = k.model\n",
    "        net.to(device)\n",
    "        logger.debug(\"%s used for xla_device, to device done\" % device)\n",
    "\n",
    "        train_sampler = DistributedSamplerWrapper(\n",
    "            sampler=BalanceClassSampler(labels=k.train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "            num_replicas=xm.xrt_world_size(),\n",
    "            rank=xm.get_ordinal(),\n",
    "            shuffle=True\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            k.train_dataset,\n",
    "            batch_size=TrainGlobalConfig.batch_size,\n",
    "            sampler=train_sampler,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "            num_workers=TrainGlobalConfig.num_workers,\n",
    "        )\n",
    "        validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            k.validation_dataset,\n",
    "            num_replicas=xm.xrt_world_size(),\n",
    "            rank=xm.get_ordinal(),\n",
    "            shuffle=False\n",
    "        )\n",
    "        validation_loader = torch.utils.data.DataLoader(\n",
    "            k.validation_dataset,\n",
    "            batch_size=TrainGlobalConfig.batch_size,\n",
    "            sampler=validation_sampler,\n",
    "            pin_memory=False,\n",
    "            drop_last=False,\n",
    "            num_workers=TrainGlobalConfig.num_workers\n",
    "        )\n",
    "        validation_tune_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            k.validation_tune_dataset,\n",
    "            num_replicas=xm.xrt_world_size(),\n",
    "            rank=xm.get_ordinal(),\n",
    "            shuffle=True\n",
    "        )\n",
    "        validation_tune_loader = torch.utils.data.DataLoader(\n",
    "            k.validation_tune_dataset,\n",
    "            batch_size=TrainGlobalConfig.batch_size,\n",
    "            sampler=validation_tune_sampler,\n",
    "            pin_memory=False,\n",
    "            drop_last=False,\n",
    "            num_workers=TrainGlobalConfig.num_workers\n",
    "        )\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            k.test_dataset,\n",
    "            num_replicas=xm.xrt_world_size(),\n",
    "            rank=xm.get_ordinal(),\n",
    "            shuffle=False\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            k.test_dataset,\n",
    "            batch_size=TrainGlobalConfig.batch_size,\n",
    "            sampler=test_sampler,\n",
    "            pin_memory=False,\n",
    "            drop_last=False,\n",
    "            num_workers=TrainGlobalConfig.num_workers\n",
    "        )\n",
    "\n",
    "        logger.debug(\"rank: %d. Will create TPU Fitter\", rank)\n",
    "\n",
    "        if rank == 0:\n",
    "            time.sleep(1)\n",
    "\n",
    "        fitter = TPUFitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "        fitter.fit(train_loader, validation_loader)\n",
    "        fitter.run_tuning_and_inference(test_loader, validation_tune_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVRPNFYuUQ18",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6x45fTI1UQ2B"
   },
   "outputs": [],
   "source": [
    "\n",
    "    FLAGS={}\n",
    "    xmp.spawn(_mp_fn, args=(FLAGS,),  nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJPxfj1OUQ2E",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    from kaggle_runner.kernels.kernel import KaggleKernelOnlyPredict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_etfRgnUQ2I",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    from datetime import date\n",
    "    today = date.today()\n",
    "    output_model_file='XLMRobertaModel_tpu_trained.bin'\n",
    "    torch.save(k.model.state_dict(), f\"{today}_{output_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Bu-tn3lUQ2J",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    submission = pd.concat([pd.read_csv(path) for path in glob('node_submissions/*.csv')]).groupby('id').mean()\n",
    "    submission['toxic'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1G896nEvUQ2L",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    submission.to_csv(f'{ROOT_PATH}/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RHAQ4PNUQ2N"
   },
   "outputs": [],
   "source": [
    "#!cp log.txt '/content/drive/My Drive/jigsaw2020-kaggle-public-baseline/'\n",
    "!make push_dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "shonenkov_training_pipeline.ipynb",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "id,colab_type,colab,language,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00bdb7e7c8a0450da3d432a057de1ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1098a4a2ecc84ac39c5ce37904f2a75d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_627f81a4d35742bba995678bb36436ad",
      "placeholder": "​",
      "style": "IPY_MODEL_93ea82bc7a7545e29ad646a5f83b1358",
      "value": " 2.24G/2.24G [00:52&lt;00:00, 42.9MB/s]"
     }
    },
    "4218600849cd4b2c8573917cf4e7b0cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "552da08823e04c6693f3547e0e63f5cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7bc1adc51564d86ae8be192335b184c",
      "placeholder": "​",
      "style": "IPY_MODEL_88309c98031a4fbdbc0e247de4e70e4a",
      "value": " 513/513 [01:07&lt;00:00, 7.58B/s]"
     }
    },
    "5ba48862ccdf438c856405814423029e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5d534c0d831b4c3e8b25229ff5949366": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "627f81a4d35742bba995678bb36436ad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68eb7fcbb8c544fcad5c271a10b2a303": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed53c651dc9d4ec598feff58b65fde84",
      "max": 2244861551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c6655ec11ee44c35be4ed66fec0bd0df",
      "value": 2244861551
     }
    },
    "72a4d2af4a164217843d68c79e473b6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68eb7fcbb8c544fcad5c271a10b2a303",
       "IPY_MODEL_1098a4a2ecc84ac39c5ce37904f2a75d"
      ],
      "layout": "IPY_MODEL_e5783b109ca6436b8bb62f1b8a49706f"
     }
    },
    "8533de2bc7134b40abacf7a9b32f0012": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa4533e1247846fcbf408a8be3a1359d",
      "max": 5069051,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5ba48862ccdf438c856405814423029e",
      "value": 5069051
     }
    },
    "88309c98031a4fbdbc0e247de4e70e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93ea82bc7a7545e29ad646a5f83b1358": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94309ba4763e4a74b447c70f15fd8e07": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c06456568914165afb73ac30228ba06": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e09d32adcc504e85a050a2fd8a8ab55c",
      "placeholder": "​",
      "style": "IPY_MODEL_00bdb7e7c8a0450da3d432a057de1ba1",
      "value": " 5.07M/5.07M [00:05&lt;00:00, 917kB/s]"
     }
    },
    "9e062caa49984f7e9773e6d74a7cfadf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94309ba4763e4a74b447c70f15fd8e07",
      "max": 513,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d534c0d831b4c3e8b25229ff5949366",
      "value": 513
     }
    },
    "aa4533e1247846fcbf408a8be3a1359d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6655ec11ee44c35be4ed66fec0bd0df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e09d32adcc504e85a050a2fd8a8ab55c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e327a32093be48ab9fabd6d2f93d4c0a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5783b109ca6436b8bb62f1b8a49706f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7bc1adc51564d86ae8be192335b184c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed53c651dc9d4ec598feff58b65fde84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee035e7ededd4627bd0c945937717ade": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8533de2bc7134b40abacf7a9b32f0012",
       "IPY_MODEL_9c06456568914165afb73ac30228ba06"
      ],
      "layout": "IPY_MODEL_e327a32093be48ab9fabd6d2f93d4c0a"
     }
    },
    "ff36006e3f57454fa52fb7bf1647c21a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e062caa49984f7e9773e6d74a7cfadf",
       "IPY_MODEL_552da08823e04c6693f3547e0e63f5cc"
      ],
      "layout": "IPY_MODEL_4218600849cd4b2c8573917cf4e7b0cc"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
