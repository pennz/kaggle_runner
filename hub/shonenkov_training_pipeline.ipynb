{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kaggle-runner\n",
      "Version: 0.1.6\n",
      "Summary: Run kaggle kernels, for fast model prototyping.\n",
      "Home-page: http://github.com/pennz/kaggle_runner\n",
      "Author: pennz\n",
      "Author-email: pengyuzhou.work@gmail.com\n",
      "License: MIT\n",
      "Location: /kaggle/working/kaggle_runner\n",
      "Requires: slug, parse, python-logging-rabbitmq, kaggle\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip3 show kaggle_runner || ( git clone https://github.com/pennz/kaggle_runner\n",
    "python3 -m pip install -e kaggle_runner\n",
    "export PATH=$PWD/kaggle_runner/bin:$PATH\n",
    "entry.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -c 'import torch_xla' || (curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null;\n",
    "                                   python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev;\n",
    "                                   python3 -m pip install transformers==2.5.1 > /dev/null;\n",
    "                                   python3 -m pip install pandarallel > /dev/null;\n",
    "                                   python3 -m pip install catalyst==20.4.2 > /dev/null;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['XLA_USE_BF16'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from fastai.text.transform import Vocab\n",
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !python3 -m pip install nltk > /dev/null\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(nb_workers=4, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__call__', <function LevelMapper.__call__ at 0x7f4daf8d1048>), ('__init__', <function LevelMapper.__init__ at 0x7f4daf910d08>)]\n",
      "[('__call__', <function BalancedPositiveNegativeSampler.__call__ at 0x7f4daf87cb70>), ('__init__', <function BalancedPositiveNegativeSampler.__init__ at 0x7f4daf87cae8>)]\n",
      "[('__init__', <function BoxCoder.__init__ at 0x7f4daf81c048>), ('decode', <function BoxCoder.decode at 0x7f4daf81c1e0>), ('decode_single', <function BoxCoder.decode_single at 0x7f4daf81c268>), ('encode', <function BoxCoder.encode at 0x7f4daf81c0d0>), ('encode_single', <function BoxCoder.encode_single at 0x7f4daf81c158>)]\n",
      "[('__call__', <function Matcher.__call__ at 0x7f4daf80cf28>), ('__init__', <function Matcher.__init__ at 0x7f4daf80cea0>), ('set_low_quality_matches_', <function Matcher.set_low_quality_matches_ at 0x7f4daf80cd90>)]\n",
      "[('__init__', <function ImageList.__init__ at 0x7f4daf81c510>), ('to', <function ImageList.to at 0x7f4daf81c488>)]\n",
      "[('__init__', <function Timebase.__init__ at 0x7f4daf6cd268>)]\n",
      "[('__init__', <function VideoMetaData.__init__ at 0x7f4daf6cd510>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/kaggle_runner/hub/kaggle_runner/kaggle_runner/utils/kernel_utils.py:154: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\g\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastai.basic_data import DataBunch\n",
    "from kaggle_runner.kernels.fastai_kernel import FastAIKernel\n",
    "from kaggle_runner import may_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 224\n",
    "BACKBONE_PATH = 'xlm-roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(BACKBONE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = f'/kaggle' # for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_runner.utils.kernel_utils import get_obj_or_dump\n",
    "def get_pickled_data(file_path):\n",
    "    obj = get_obj_or_dump(file_path)\n",
    "\n",
    "    if obj is None:\n",
    "        #may_debug(True)\n",
    "\n",
    "        return get_obj_or_dump(f\"{ROOT_PATH}/input/clean-pickle-for-jigsaw-toxicity/{file_path}\")\n",
    "\n",
    "    return obj\n",
    "vocab = get_pickled_data(\"vocab.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "if vocab is None: # vocab file read~~\n",
    "   vocab = [tokenizer.convert_ids_to_tokens(i) for i in range(tokenizer.vocab_size)]\n",
    "   get_obj_or_dump(\"vocab.pkl\", default=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from random import shuffle\n",
    "import random\n",
    "import albumentations\n",
    "from albumentations.core.transforms_interface import DualTransform, BasicTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "LANGS = {\n",
    "    'en': 'english',\n",
    "    'it': 'italian',\n",
    "    'fr': 'french',\n",
    "    'es': 'spanish',\n",
    "    'tr': 'turkish',\n",
    "    'ru': 'russian',\n",
    "    'pt': 'portuguese'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_sentences(text, lang='en'):\n",
    "    return sent_tokenize(text, LANGS.get(lang, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def exclude_duplicate_sentences(text, lang='en'):\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in get_sentences(text, lang):\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        if sentence not in sentences:\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, lang='en'):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[0-9\"]', '', text)\n",
    "    text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = exclude_duplicate_sentences(text, lang)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class NLPTransform(BasicTransform):\n",
    "    \"\"\" Transform for nlp task.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return {\"data\": self.apply}\n",
    "\n",
    "    def update_params(self, params, **kwargs):\n",
    "        if hasattr(self, \"interpolation\"):\n",
    "            params[\"interpolation\"] = self.interpolation\n",
    "\n",
    "        if hasattr(self, \"fill_value\"):\n",
    "            params[\"fill_value\"] = self.fill_value\n",
    "\n",
    "        return params\n",
    "\n",
    "    def get_sentences(self, text, lang='en'):\n",
    "        return sent_tokenize(text, LANGS.get(lang, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ShuffleSentencesTransform(NLPTransform):\n",
    "    \"\"\" Do shuffle by sentence \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = self.get_sentences(text, lang)\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        return ' '.join(sentences), lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeDuplicateSentencesTransform(NLPTransform):\n",
    "    \"\"\" Exclude equal sentences \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = []\n",
    "\n",
    "        for sentence in self.get_sentences(text, lang):\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if sentence not in sentences:\n",
    "                sentences.append(sentence)\n",
    "\n",
    "        return ' '.join(sentences), lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeNumbersTransform(NLPTransform):\n",
    "    \"\"\" exclude any numbers \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeHashtagsTransform(NLPTransform):\n",
    "    \"\"\" Exclude any hashtags with # \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeUsersMentionedTransform(NLPTransform):\n",
    "    \"\"\" Exclude @users \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExcludeUrlsTransform(NLPTransform):\n",
    "    \"\"\" Exclude urls \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'https?\\S+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_subtitles():\n",
    "    df_ot = get_pickled_data(\"ot.pkl\")\n",
    "\n",
    "    if df_ot is None:\n",
    "        df_ot = pd.read_csv(f'{ROOT_PATH}/input/open-subtitles-toxic-pseudo-labeling/open-subtitles-synthesic.csv', index_col='id')[['comment_text', 'toxic', 'lang']]\n",
    "        df_ot = df_ot[~df_ot['comment_text'].isna()]\n",
    "        df_ot['comment_text'] = df_ot.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "        df_ot = df_ot.drop_duplicates(subset='comment_text')\n",
    "        df_ot['toxic'] = df_ot['toxic'].round().astype(np.int)\n",
    "        get_obj_or_dump(\"ot.pkl\", default=df_ot)\n",
    "\n",
    "    return df_ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SynthesicOpenSubtitlesTransform(NLPTransform):\n",
    "    def __init__(self, always_apply=False, supliment_toxic=None, p=0.5, mix=False):\n",
    "        super(SynthesicOpenSubtitlesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "        df = get_open_subtitles()\n",
    "        self.synthesic_toxic = df[df['toxic'] == 1].comment_text.values\n",
    "        self.synthesic_non_toxic = df[df['toxic'] == 0].comment_text.values\n",
    "\n",
    "        if supliment_toxic is not None:\n",
    "            self.synthesic_toxic = np.concatenate((self.synthesic_toxic, supliment_toxic))\n",
    "        self.mix = mix\n",
    "\n",
    "        del df\n",
    "        gc.collect();\n",
    "\n",
    "\n",
    "    def _mix_both(self, texts):\n",
    "        for i in range(random.randint(0,2)):\n",
    "            texts.append(random.choice(self.synthesic_non_toxic))\n",
    "\n",
    "        for i in range(random.randint(1,3)):\n",
    "            texts.append(random.choice(self.synthesic_toxic))\n",
    "\n",
    "    def generate_synthesic_sample(self, text, toxic):\n",
    "        texts = [text]\n",
    "\n",
    "        if toxic == 0:\n",
    "            if self.mix:\n",
    "                self._mix_both(texts)\n",
    "                toxic = 1\n",
    "            else:\n",
    "                for i in range(random.randint(1,5)):\n",
    "                    texts.append(random.choice(self.synthesic_non_toxic))\n",
    "        else:\n",
    "            self._mix_both(texts)\n",
    "        random.shuffle(texts)\n",
    "\n",
    "        return ' '.join(texts), toxic\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, toxic = data\n",
    "        text, toxic = self.generate_synthesic_sample(text, toxic)\n",
    "\n",
    "        return text, toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return albumentations.Compose([\n",
    "        ExcludeUsersMentionedTransform(p=0.95),\n",
    "        ExcludeUrlsTransform(p=0.95),\n",
    "        ExcludeNumbersTransform(p=0.95),\n",
    "        ExcludeHashtagsTransform(p=0.95),\n",
    "        ExcludeDuplicateSentencesTransform(p=0.95),\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_synthesic_transforms(supliment_toxic, p=0.5, mix=False):\n",
    "    return SynthesicOpenSubtitlesTransform(p=p, supliment_toxic=supliment_toxic, mix=mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_toxic_comments(df):\n",
    "        df = df[~df['comment_text'].isna()]\n",
    "        df = df.drop_duplicates(subset='comment_text')\n",
    "        df['toxic'] = df['toxic'].round().astype(np.int)\n",
    "\n",
    "        return df[df['toxic'] == 1].comment_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def onehot(size, target, aux=None):\n",
    "    if aux is not None:\n",
    "        vec = np.zeros(size+len(aux), dtype=np.float32)\n",
    "        vec[target] = 1.\n",
    "        vec[2:] = aux\n",
    "        vec = torch.tensor(vec, dtype=torch.float32)\n",
    "    else:\n",
    "        vec = torch.zeros(size, dtype=torch.float32)\n",
    "        vec[target] = 1.\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, labels_or_ids, comment_texts, langs,\n",
    "                 severe_toxic=None, obscene=None, threat=None, insult=None, identity_hate=None,\n",
    "                 use_train_transforms=False, test=False, use_aux=True, transformers=None):\n",
    "        self.test = test\n",
    "        self.labels_or_ids = labels_or_ids\n",
    "        self.comment_texts = comment_texts\n",
    "        self.langs = langs\n",
    "        self.severe_toxic = severe_toxic\n",
    "        self.obscene = obscene\n",
    "        self.threat = threat\n",
    "        self.insult = insult\n",
    "        self.identity_hate = identity_hate\n",
    "        self.use_train_transforms = use_train_transforms\n",
    "        self.aux = None\n",
    "        assert transformers is not None\n",
    "        self.transformers = transformers\n",
    "        self.vocab = vocab\n",
    "\n",
    "        if use_aux:\n",
    "            self.aux = [self.severe_toxic, self.obscene, self.threat, self.insult, self.identity_hate]\n",
    "\n",
    "    def get_tokens(self, text):\n",
    "        encoded = self.transformers['tokenizer'].encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "\n",
    "        return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.comment_texts.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.comment_texts[idx]\n",
    "        lang = self.langs[idx]\n",
    "\n",
    "        if self.severe_toxic is None:\n",
    "            aux = [0., 0., 0., 0., 0.]\n",
    "        else:\n",
    "            aux = [self.severe_toxic[idx], self.obscene[idx], self.threat[idx], self.insult[idx], self.identity_hate[idx]]\n",
    "\n",
    "\n",
    "        label = self.labels_or_ids[idx]\n",
    "\n",
    "        if self.use_train_transforms and (not self.test):\n",
    "            text, _ = self.transformers['train_transforms'](data=(text, lang))['data']\n",
    "            tokens, attention_mask = self.get_tokens(str(text))\n",
    "            token_length = sum(attention_mask)\n",
    "\n",
    "            if token_length > 0.8*MAX_LENGTH:\n",
    "                text, _ = self.transformers['shuffle_transforms'](data=(text, lang))['data']\n",
    "            elif token_length < 60:\n",
    "                text, label = self.transformers['synthesic_transforms_often'](data=(text, label))['data']\n",
    "            else: # will not need to use transforms\n",
    "                #text, label = synthesic_transforms_low(data=(text, label))['data']\n",
    "                pass\n",
    "\n",
    "        # TODO add language detection and shuffle\n",
    "        # https://pypi.org/project/langdetect/\n",
    "        # if self.use_train_transforms and self.test:\n",
    "        #    text, _ = train_transforms(data=(text, lang))['data']\n",
    "        #    tokens, attention_mask = self.get_tokens(str(text))\n",
    "        #    token_length = sum(attention_mask)\n",
    "\n",
    "        #    if token_length > 0.8*MAX_LENGTH:\n",
    "        #        text, _ = shuffle_transforms(data=(text, lang))['data']\n",
    "        # to tensors\n",
    "        tokens, attention_mask = self.get_tokens(str(text))\n",
    "        tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n",
    "\n",
    "        if self.test:  # for test, return id TODO TTA\n",
    "            return [tokens, attention_mask], self.labels_or_ids[idx]\n",
    "\n",
    "        # label might be changed\n",
    "        target = onehot(2, label, aux=aux)\n",
    "\n",
    "        return [tokens, attention_mask], target\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(np.char.add(self.labels_or_ids.astype(str), self.langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_runner.kernels.fastai_kernel import FastAIKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shonenkov(FastAIKernel):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Shonenkov, self).__init__(**kargs)\n",
    "        self.data = None\n",
    "        self.transformers = None\n",
    "        self.setup_transformers()\n",
    "\n",
    "    def build_and_set_model(self):\n",
    "        self.model = ToxicSimpleNNModel()\n",
    "\n",
    "    def set_random_seed(self):\n",
    "        seed_everything(SEED)\n",
    "\n",
    "    def setup_transformers(self):\n",
    "        if self.transformers is None:\n",
    "            supliment_toxic = None # avoid overfit\n",
    "            train_transforms = get_train_transforms();\n",
    "            synthesic_transforms_often = get_synthesic_transforms(supliment_toxic, p=0.5)\n",
    "            synthesic_transforms_low = None\n",
    "            #tokenizer = tokenizer\n",
    "            shuffle_transforms = ShuffleSentencesTransform(always_apply=True)\n",
    "\n",
    "            self.transformers = {'train_transforms': train_transforms,\n",
    "                                 'synthesic_transforms_often': synthesic_transforms_often,\n",
    "                                 'synthesic_transforms_low': synthesic_transforms_low,\n",
    "                                 'tokenizer': tokenizer, 'shuffle_transforms':\n",
    "                                 shuffle_transforms}\n",
    "\n",
    "    def prepare_train_dev_data(self):\n",
    "        df_train = get_pickled_data(\"train.pkl\")\n",
    "\n",
    "        if df_train is None:\n",
    "            df_train = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-toxicity-train-data-with-aux/train_data.csv')\n",
    "            df_train['comment_text'] = df_train.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "            get_obj_or_dump(\"train.pkl\", default=df_train)\n",
    "\n",
    "        #supliment_toxic = get_toxic_comments(df_train)\n",
    "        self.train_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_train['toxic'].values,\n",
    "            comment_texts=df_train['comment_text'].values,\n",
    "            langs=df_train['lang'].values,\n",
    "            severe_toxic=df_train['severe_toxic'].values,\n",
    "            obscene=df_train['obscene'].values,\n",
    "            threat=df_train['threat'].values,\n",
    "            insult=df_train['insult'].values,\n",
    "            identity_hate=df_train['identity_hate'].values,\n",
    "            use_train_transforms=True,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "        df_val = get_pickled_data(\"val.pkl\")\n",
    "\n",
    "        if df_val is None:\n",
    "            df_val = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/validation.csv', index_col='id')\n",
    "            df_val['comment_text'] = df_val.parallel_apply(lambda x: clean_text(x['comment_text'], x['lang']), axis=1)\n",
    "            get_obj_or_dump(\"val.pkl\", default=df_val)\n",
    "\n",
    "        self.validation_tune_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_val['toxic'].values,\n",
    "            comment_texts=df_val['comment_text'].values,\n",
    "            langs=df_val['lang'].values,\n",
    "            use_train_transforms=True,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "        self.validation_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_val['toxic'].values,\n",
    "            comment_texts=df_val['comment_text'].values,\n",
    "            langs=df_val['lang'].values,\n",
    "            use_train_transforms=False,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "\n",
    "        del df_val\n",
    "#del df_val_unclean\n",
    "        gc.collect();\n",
    "\n",
    "        del df_train\n",
    "        gc.collect();\n",
    "\n",
    "    def prepare_test_data(self):\n",
    "        df_test = get_pickled_data(\"test.pkl\")\n",
    "\n",
    "        if df_test is None:\n",
    "            df_test = pd.read_csv(f'{ROOT_PATH}/input/jigsaw-multilingual-toxic-comment-classification/test.csv', index_col='id')\n",
    "            df_test['comment_text'] = df_test.parallel_apply(lambda x: clean_text(x['content'], x['lang']), axis=1)\n",
    "            get_obj_or_dump(\"test.pkl\", default=df_test)\n",
    "\n",
    "        self.test_dataset = DatasetRetriever(\n",
    "            labels_or_ids=df_test.index.values, ## here different!!!\n",
    "            comment_texts=df_test['comment_text'].values,\n",
    "            langs=df_test['lang'].values,\n",
    "            use_train_transforms=False,\n",
    "            test=True,\n",
    "            transformers=self.transformers\n",
    "        )\n",
    "\n",
    "        del df_test\n",
    "        gc.collect();\n",
    "    def after_prepare_data_hook(self):\n",
    "        \"\"\"Put to databunch here\"\"\"\n",
    "        self.data = DataBunch.create(self.train_dataset,\n",
    "                                     self.validation_dataset,\n",
    "                                     bs=TrainGlobalConfig.batch_size,\n",
    "                                     num_workers=TrainGlobalConfig.num_workers)\n",
    "\n",
    "    def peek_data(self):\n",
    "        if self.data is not None:\n",
    "            may_debug()\n",
    "            o = self.data.one_batch()\n",
    "            print(o)\n",
    "\n",
    "            return o\n",
    "        else:\n",
    "            if self.logger is not None:\n",
    "                self.logger.error(\"peek_data failed, DataBunch is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from kaggle_runner.metrics.metrics import matthews_correlation\n",
    "class RocAucMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = np.array([])\n",
    "        self.y_true_float = np.array([], dtype=np.float)\n",
    "        self.y_pred = np.array([])\n",
    "        self.score = 0\n",
    "        self.mc_score = 0\n",
    "        self.aux_part = 0\n",
    "\n",
    "    def update(self, y_true, y_pred, aux_part=0):\n",
    "        #y_true_ = y_true\n",
    "        y_true = y_true[:,:2].cpu().numpy().argmax(axis=1)\n",
    "        y_true_float = y_true.astype(np.float)\n",
    "        y_pred = nn.functional.softmax(y_pred[:,:2], dim=1).data.cpu().numpy()[:,1]\n",
    "        self.y_true = np.hstack((self.y_true, y_true))\n",
    "        self.y_true_float = np.hstack((self.y_true_float, y_true_float))\n",
    "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
    "        try:\n",
    "            self.score = sklearn.metrics.roc_auc_score(self.y_true, self.y_pred, labels=np.array([0, 1]))\n",
    "        except Exception:\n",
    "            self.score = 0\n",
    "        self.mc_score = matthews_correlation(self.y_true_float, self.y_pred)\n",
    "        self.aux_part = aux_part\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.score\n",
    "    @property\n",
    "    def mc_avg(self):\n",
    "        return self.mc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ToxicSimpleNNModel(nn.Module):\n",
    "    def __init__(self, use_aux=True):\n",
    "        super(ToxicSimpleNNModel, self).__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(BACKBONE_PATH)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        aux_len = 0\n",
    "\n",
    "        if use_aux:\n",
    "            aux_len = 5\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.backbone.pooler.dense.out_features*2,\n",
    "            out_features=2+aux_len,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        bs, seq_length = input_ids.shape\n",
    "        seq_x, _ = self.backbone(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        apool = torch.mean(seq_x, 1)\n",
    "        mpool, _ = torch.max(seq_x, 1)\n",
    "        x = torch.cat((apool, mpool), 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TPUFitter:\n",
    "\n",
    "    def __init__(self, model, device, config):\n",
    "        if not os.path.exists('node_submissions'):\n",
    "            os.makedirs('node_submissions')\n",
    "\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        self.log_path = 'log.txt'\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr*xm.xrt_world_size())\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "\n",
    "        self.criterion = config.criterion\n",
    "        xm.master_print(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(train_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
    "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
    "\n",
    "            self.log(f'[RESULT]: Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=final_scores.mc_avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def run_tuning_and_inference(self, test_loader, validation_tune_loader):\n",
    "        for e in range(2):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*xm.xrt_world_size()\n",
    "            para_loader = pl.ParallelLoader(validation_tune_loader, [self.device])\n",
    "            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n",
    "            para_loader = pl.ParallelLoader(test_loader, [self.device])\n",
    "            self.run_inference(para_loader.per_device_loader(self.device))\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(val_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(train_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    self.log(\n",
    "                        f'Train Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(self.device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "            targets = targets.to(self.device, dtype=torch.float)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs, attention_masks)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            logger.debug(\"step: %d, loss: %f\", step, loss)\n",
    "            _check_grad(self.optimizer)\n",
    "            xm.optimizer_step(self.optimizer)\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        self.model.eval()\n",
    "        self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_inference(self, test_loader):\n",
    "        self.model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, ids) in enumerate(test_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    xm.master_print(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(self.device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n",
    "                outputs = self.model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "        result = pd.DataFrame(result)\n",
    "        node_count = len(glob('node_submissions/*.csv'))\n",
    "        result.to_csv(f'node_submissions/submission_{node_count}_{datetime.utcnow().microsecond}_{random.random()}.csv', index=False)\n",
    "\n",
    "    def save(self, path):\n",
    "        xm.save(self.model.state_dict(), path)\n",
    "\n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            xm.master_print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            xm.master_print(f'{message}', logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing = 0.1, dim=-1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.cls = 2\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            pred = x[:,:2].log_softmax(dim=self.dim)\n",
    "            aux=x[:, 2:]\n",
    "\n",
    "            toxic_target = target[:,:2]\n",
    "            aux_target = target[:, 2:]\n",
    "            with torch.no_grad():\n",
    "                # smooth_toxic = pred.data.clone()\n",
    "                smooth_toxic = self.smoothing + (1-self.smoothing*2)*toxic_target\n",
    "                # smooth_toxic.scatter_(1, toxic_target.data.unsqueeze(1), self.confidence) # only for 0 1 label, put confidence to related place\n",
    "                # for 0-1, 0 -> 0.1, 1->0.9.(if 1), if zero. 0->0.9, 1->0.1\n",
    "                smooth_aux = self.smoothing + (1-self.smoothing*2)*aux_target  # only for binary cross entropy, so for lable, it is (1-smooth)*\n",
    "\n",
    "            aux_loss = torch.nn.functional.binary_cross_entropy_with_logits(aux, smooth_aux)\n",
    "\n",
    "            return torch.mean(torch.sum(-smooth_toxic * pred, dim=self.dim)) + aux_loss/3\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x[:,:2], target[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    \"\"\" Global Config for this notebook \"\"\"\n",
    "    num_workers = 0  # количество воркеров для loaders\n",
    "    batch_size = 16  # bs\n",
    "    n_epochs = 2  # количество эпох для обучения\n",
    "    lr = 0.5 * 1e-5 # стартовый learning rate (внутри логика работы с мульти TPU домножает на кол-во процессов)\n",
    "    fold_number = 0  # номер фолда для обучения\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True  # выводить принты\n",
    "    verbose_step = 1  # количество шагов для вывода принта\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # выполнять scheduler.step после вызова optimizer.step\n",
    "    validation_scheduler = True  # выполнять scheduler.step после валидации loss (например для плато)\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='max',\n",
    "        factor=0.7,\n",
    "        patience=0,\n",
    "        verbose=False,\n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0,\n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )\n",
    "    # --------------------\n",
    "\n",
    "    # -------------------\n",
    "    criterion = LabelSmoothing()\n",
    "    # -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_init():\n",
    "    l = Shonenkov(loss_func=None, metrics=None)\n",
    "    assert l is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/clean-pickle-for-jigsaw-toxicity/*pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_runner import may_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-18 09:24:52,746:utils:load ot.pkl\n",
      "[DEBUG]2020-06-18 09:24:54,285:utils:None -> KernelRunningState.SAVE_SUBMISSION_DONE\n",
      "[DEBUG]2020-06-18 09:24:54,286:utils:load train.pkl\n",
      "[DEBUG]2020-06-18 09:24:58,719:utils:load val.pkl\n",
      "[DEBUG]2020-06-18 09:24:59,079:utils:state KernelRunningState.PREPARE_DATA_DONE\n",
      "[DEBUG]2020-06-18 09:25:18,031:utils:state KernelRunningState.TRAINING_DONE\n",
      "[DEBUG]2020-06-18 09:25:18,033:utils:state KernelRunningState.EVL_DEV_DONE\n",
      "[DEBUG]2020-06-18 09:25:18,034:utils:load test.pkl\n",
      "[DEBUG]2020-06-18 09:25:18,687:utils:state KernelRunningState.SAVE_SUBMISSION_DONE\n"
     ]
    }
   ],
   "source": [
    "k = Shonenkov(metrics=None, loss_func=LabelSmoothing(), opt_func=None)\n",
    "k.run(dump_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_grad(raw_opt):\n",
    "    pg = raw_opt.param_groups\n",
    "    pg0pl = pg[0]['params'] # pg0pl[0] is a Parameter\n",
    "    pg1pl = pg[1]['params'] # pg0pl[0] is a Parameter\n",
    "\n",
    "    #logger.debug(\"grad info: %s\", raw_opt)\n",
    "\n",
    "    norms = torch.tensor([torch.norm(p) for p in pg0pl])\n",
    "    normsg = torch.tensor([torch.norm(p.grad) for p in pg0pl if p.grad is not None])\n",
    "    logger.debug(\"params info pg0: norm std(%f) mean(%f)\", *torch.std_mean(norms))\n",
    "    logger.debug(\"grad info pg0: norm std(%f) mean(%f)\", *torch.std_mean(normsg))\n",
    "\n",
    "    norms1 = torch.tensor([torch.norm(p) for p in pg1pl])\n",
    "    norms1g = torch.tensor([torch.norm(p.grad) for p in pg1pl if p.grad is not None])\n",
    "    logger.debug(\"params info pg1: norm std(%f) mean(%f)\", *torch.std_mean(norms1))\n",
    "    logger.debug(\"grad info pg1: norm std(%f) mean(%f)\", *torch.std_mean(norms1g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "from kaggle_runner import logger\n",
    "\n",
    "def test_model_fn(device=torch.device(\"cpu\")):\n",
    "    #device = xm.xla_device(devkind='TPU')\n",
    "    #device=torch.device(\"xla\")\n",
    "    logger.debug(\"Device used: %s\", device)\n",
    "\n",
    "    #k.run(dump_flag=True) # it seems it cannot save right\n",
    "    #k.run(dump_flag=False)\n",
    "    #k.peek_data()\n",
    "\n",
    "    self = k\n",
    "    assert self.validation_dataset is not None\n",
    "    #assert self.learner is not None\n",
    "\n",
    "    net = k.model\n",
    "    assert net is not None\n",
    "    net.to(device)\n",
    "\n",
    "    param_optimizer = list(self.model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    #optimizer = AdamW(optimizer_grouped_parameters, lr=TrainGlobalConfig.lr*xm.xrt_world_size())\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=TrainGlobalConfig.lr*8)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        self.train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=train_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        self.validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        self.test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "    #    sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    validation_tune_loader = torch.utils.data.DataLoader(\n",
    "        self.validation_tune_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        #sampler=validation_tune_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    def validation(model, device, config, val_loader, criterion):\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(val_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(\n",
    "                        f'Valid Step {step}, loss: ' + \\\n",
    "                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                batch_size = inputs.size(0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "    def run_inference(model, device, config, test_loader):\n",
    "        model.eval()\n",
    "        result = {'id': [], 'toxic': []}\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(test_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.info(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "                outputs = model(inputs, attention_masks)\n",
    "                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n",
    "\n",
    "            result['id'].extend(ids.cpu().numpy())\n",
    "            result['toxic'].extend(toxics)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def train_one_epoch(model, device, config, train_loader, criterion, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "\n",
    "        for step, (inputs_masks, targets) in enumerate(train_loader):\n",
    "            inputs=inputs_masks[0]\n",
    "            attention_masks=inputs_masks[1]\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            if config.verbose:\n",
    "                if step % config.verbose_step == 0:\n",
    "                    logger.debug(\n",
    "                        f'Train Step {step}, bs: {batch_size}, loss: ' + \\\n",
    "                        f\"{losses.avg:.5f}, lr: {optimizer.param_groups[0]['lr']} final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, \" + \\\n",
    "                        f'time: {(time.time() - t):.5f}'\n",
    "                    )\n",
    "\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            attention_masks = attention_masks.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "\n",
    "            final_scores.update(targets, outputs)\n",
    "\n",
    "            losses.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "            _check_grad(optimizer)\n",
    "            optimizer.step()\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "\n",
    "        model.eval()\n",
    "        #self.save('last-checkpoint.bin')\n",
    "\n",
    "        return losses, final_scores\n",
    "\n",
    "    def run_tuning_and_inference(net, device, TrainGlobalConfig, validation_loader, train_loader):\n",
    "        for e in range(1):\n",
    "            self.optimizer.param_groups[0]['lr'] = self.config.lr*8\n",
    "\n",
    "            losses, final_scores = train_one_epoch(net, device, TrainGlobalConfig, train_loader, TrainGlobalConfig.criterion, )\n",
    "            self.log(f'[RESULT]: Tune_Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            t = time.time()\n",
    "            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n",
    "            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n",
    "            self.log(f'[RESULT]: Tune_Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, mc_score: {final_scores.mc_avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "\n",
    "            run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "\n",
    "    train_one_epoch(net, device, TrainGlobalConfig, train_loader, TrainGlobalConfig.criterion, optimizer)\n",
    "    losses, final_scores = validation(net, device, TrainGlobalConfig, validation_loader, TrainGlobalConfig.criterion)\n",
    "    logger.info(f\"Val results: losses={losses}, final_scores={final_scores}\")\n",
    "\n",
    "    results = run_inference(net, device, TrainGlobalConfig, validation_loader)\n",
    "    logger.info(f\"Test done, result len %d\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG]2020-06-18 10:06:34,840:utils:Device used: cpu\n",
      "[DEBUG]2020-06-18 10:06:47,497:utils:Train Step 0, loss: 0.00000, lr: 5e-06 final_score: 0.00000, mc_score: 0.00000, time: 0.01384\n"
     ]
    }
   ],
   "source": [
    "test_model_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k.learner\n",
    "#k.learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch\n",
    "\n",
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.core import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.vision import *\n",
    "from fastai.basic_train import *\n",
    "from kaggle_runner import logger\n",
    "\n",
    "def len_parallelloader(self):\n",
    "    return len(self._loader._loader)\n",
    "pl.PerDeviceLoader.__len__ = len_parallelloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckGrad(LearnerCallback):\n",
    "    def __init__(self, learn:Learner, skip_loss_step=False):\n",
    "        super().__init__(learn)\n",
    "        self.skip_loss_step = skip_loss_step\n",
    "        logger.debug(\"Callback CheckGrad skip_loss_step: \" +str(self.skip_loss_step))\n",
    "\n",
    "    def on_backward_end(self, **kwargs:Any)->None:\n",
    "        raw_opt = self.learn.opt.opt\n",
    "        _check_grad(raw_opt)\n",
    "\n",
    "        return {'skip_step': self.skip_loss_step}\n",
    "\n",
    "class TPUDistributed(LearnerCallback):\n",
    "    def __init__(self, learn:Learner, debug=True):\n",
    "        super().__init__(learn)\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "        if debug:\n",
    "            self.device = xm.xla_device(devkind='TPU')\n",
    "            logger.debug(\"TPUDistributed in DEBUG mode\")\n",
    "            #self.device = xm.xla_device(devkind='CPU')\n",
    "        else:\n",
    "            self.device = xm.xla_device(devkind='TPU')\n",
    "            logger.debug(\"%s used for xla_device for TPUDistributed\" % self.device)\n",
    "\n",
    "    def _change_dl(self,dl, shuffle):\n",
    "        old_dl = dl\n",
    "        train_sampler = DistributedSamplerWrapper(\n",
    "            sampler=BalanceClassSampler(labels=k.train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "            num_replicas=xm.xrt_world_size(),\n",
    "            rank=xm.get_ordinal(),\n",
    "            shuffle=True\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            k.train_dataset,\n",
    "            batch_size=TrainGlobalConfig.batch_size,\n",
    "            sampler=train_sampler,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "            num_workers=TrainGlobalConfig.num_workers,\n",
    "        )\n",
    "        new_dl = train_loader\n",
    "\n",
    "        return old_dl,new_dl,train_sampler\n",
    "\n",
    "    def _change_dl_val(self,dl, shuffle):\n",
    "        old_dl = dl\n",
    "        validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            k.validation_dataset,\n",
    "            num_replicas=xm.xrt_world_size(),\n",
    "            rank=xm.get_ordinal(),\n",
    "            shuffle=False\n",
    "        )\n",
    "        validation_loader = torch.utils.data.DataLoader(\n",
    "            k.validation_dataset,\n",
    "            batch_size=TrainGlobalConfig.batch_size,\n",
    "            sampler=validation_sampler,\n",
    "            pin_memory=False,\n",
    "            drop_last=False,\n",
    "            num_workers=TrainGlobalConfig.num_workers\n",
    "        )\n",
    "\n",
    "        return old_dl,validation_loader,validation_sampler\n",
    "\n",
    "    def on_train_begin(self, **kwargs:Any)->None:\n",
    "        self.learn.model = self.learn.model.to(self.device)\n",
    "\n",
    "        if self.debug:\n",
    "            self.learn.opt.lr = self.learn.opt.lr\n",
    "            logger.debug(\"opt info: %s, type %s\", self.learn.opt, type(self.learn.opt))\n",
    "        else:\n",
    "            self.learn.opt.lr = self.learn.opt.lr*xm.xrt_world_size()\n",
    "        logger.debug(\"%s used for xla_device, to device done\" % self.device)\n",
    "        shuffle = self.data.train_dl.init_kwargs['shuffle'] if hasattr(self.data.train_dl, 'init_kwargs') else True\n",
    "        self.old_sampler_train_dl,self.data.train_dl,self.train_sampler = self._change_dl(self.data.train_dl, shuffle)\n",
    "\n",
    "        if hasattr(self.data, 'valid_dl') and self.data.valid_dl is not None:\n",
    "            self.old_sampler_valid_dl,self.data.valid_dl,self.valid_sampler = self._change_dl_val(self.data.valid_dl, shuffle)\n",
    "\n",
    "    def on_epoch_begin(self,**kwargs:Any)->None:\n",
    "        logger.debug(\"Epoch begins on device %s\" % self.device)\n",
    "\n",
    "        self.old_train_dl = self.data.train_dl\n",
    "        self.learn.data.train_dl = pl.ParallelLoader(self.old_train_dl, [self.device]).per_device_loader(self.device)\n",
    "        self.learn.data.train_dl.dataset = None #self.old_train_dl.dataset\n",
    "\n",
    "        if hasattr(self.data, 'valid_dl') and self.data.valid_dl is not None:\n",
    "            self.old_valid_dl = self.learn.data.valid_dl\n",
    "            self.learn.data.valid_dl = pl.ParallelLoader(self.old_valid_dl, [self.device]).per_device_loader(self.device)\n",
    "\n",
    "            self.learn.data.valid_dl.dataset = self.old_valid_dl.dataset\n",
    "            self.learn.data.valid_dl.dl = self.learn.data.valid_dl._loader._loader\n",
    "\n",
    "    def on_backward_end(self, **kwargs:Any)->None:\n",
    "        xm.optimizer_step(self.learn.opt, barrier=self.debug) # copied from https://github.com/tmabraham/fastai_tpu/blob/8b73018cf705da1a73d9be1f105a8e886051a90c/fastai_v1/tpu_distributed_fastai.py, and needed a fix\n",
    "        #may_debug(True)\n",
    "        #return {'skip_step': True}\n",
    "\n",
    "    def on_epoch_end(self,**kwargs:Any)->None:\n",
    "        self.learn.data.train_dl = self.old_train_dl\n",
    "        self.learn.data.valid_dl = self.old_valid_dl\n",
    "\n",
    "    def on_train_end(self,**kwargs:Any)->None:\n",
    "        self.learn.data.train_dl = self.old_sampler_train_dl\n",
    "        self.learn.data.valid_dl = self.old_sampler_valid_dl\n",
    "\n",
    "\n",
    "def _to_tpu_distributed(learn:Learner) -> Learner:\n",
    "  #Learner.fit = _fit_tpu\n",
    "    learn.callback_fns.append(TPUDistributed)\n",
    "\n",
    "    return learn\n",
    "\n",
    "\n",
    "Learner.to_tpu_distributed = _to_tpu_distributed\n",
    "\n",
    "def setup_food():\n",
    "    path = untar_data(URLs.FOOD)\n",
    "\n",
    "def filelist2df(path):\n",
    "    df = pd.read_csv(path, delimiter='/', header=None, names=['label', 'name'])\n",
    "    df['name'] =  df['label'].astype(str) + \"/\" + df['name'].astype(str) + \".jpg\"\n",
    "\n",
    "    return df\n",
    "\n",
    "#train_path = path/'train.txt'\n",
    "#test_path = path/'test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from fastai.callbacks.misc import StopAfterNBatches\n",
    "from fastai.callbacks import *\n",
    "def debug_train():\n",
    "    from kaggle_runner import defaults\n",
    "    _DEBUG = defaults.DEBUG\n",
    "    defaults.DEBUG = True\n",
    "\n",
    "    param_optimizer = list(k.model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'lr': 0., 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'lr': 0., 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    def AdamW_with_given_p(p_to_ignore, *args, **kargs):\n",
    "        kargs['lr']=TrainGlobalConfig.lr*xm.xrt_world_size()\n",
    "\n",
    "        return AdamW(optimizer_grouped_parameters, *args, **kargs)\n",
    "\n",
    "    learn = k.create_learner(k, opt_func=AdamW_with_given_p,\n",
    "                             loss_func=LabelSmoothing(),\n",
    "                             wd=0.01,\n",
    "                             callback_fns=[partial(GradientClipping, clip=0.1),\n",
    "                                           ShowGraph,\n",
    "                                           partial(CSVLogger, append=True),\n",
    "                                           partial(CheckGrad, skip_loss_step=False)]\n",
    "                             ).to_tpu_distributed()\n",
    "    learn.callbacks.append(StopAfterNBatches(n_batches=200))\n",
    "    #learn.callback_fns.append(CheckGrad)\n",
    "    #print('hello')\n",
    "    learn.lr_find(start_lr=1e-7, end_lr=1e-4, num_it=200)\n",
    "    #learn.recorder.plot()\n",
    "    #learn.fit_one_cycle(1, max_lr=4e-5)\n",
    "    #learn.fit(1, lr=5e-5) # original 0.5*e-5*8=4*e-5\n",
    "    defaults.DEBUG = _DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "debug_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def train_loop(index, *args):\n",
    "  #data = (ImageList.from_df(df=train_df, path=path/'images', cols=1)\n",
    "  #        .random_split_by_pct(0.2)\n",
    "  #        .label_from_df(cols=0)\n",
    "  #        .transform(get_transforms(), size=224)\n",
    "  #        .databunch(bs=32, num_workers=0)\n",
    "  #        .normalize(imagenet_stat))\n",
    "  #learn = cnn_learner(data, models.resnet152, metrics=accuracy).to_tpu_distributed()\n",
    "    logger.debug(\"rank: %d entered train_loop\", index)\n",
    "\n",
    "    param_optimizer = list(k.model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'lr': 4e-5, 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'lr': 4e-5, 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    def AdamW_with_given_p(p_to_ignore, *args, **kargs):\n",
    "        kargs['lr']=TrainGlobalConfig.lr*xm.xrt_world_size()\n",
    "\n",
    "        return AdamW(optimizer_grouped_parameters, *args, **kargs)\n",
    "\n",
    "    if index == 0:\n",
    "        time.sleep(1)\n",
    "    learn = k.create_learner(k, opt_func=AdamW_with_given_p,\n",
    "                             loss_func=LabelSmoothing(),\n",
    "                             wd=0.01,\n",
    "                             callback_fns=partial(GradientClipping, clip=0.1)).to_tpu_distributed()\n",
    "    learn.lr_find(start_lr=1e-7, end_lr=1e-4, num_it=200)\n",
    "    learn.recorder.plot()\n",
    "    #learn.fit_one_cycle(3, max_lr=9e-6, wd=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "FLAGS={}\n",
    "xmp.spawn(train_loop, args=(FLAGS,),  nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(k.learner.data.train_dl.dl),k.learner.data.train_dl.dl.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mp_fn(rank, flags, k=k):\n",
    "    device = xm.xla_device(devkind='TPU')\n",
    "    logger.debug(\"%s used for xla_device\" % device)\n",
    "    net = k.model\n",
    "    net.to(device)\n",
    "    logger.debug(\"%s used for xla_device, to device done\" % device)\n",
    "\n",
    "    train_sampler = DistributedSamplerWrapper(\n",
    "        sampler=BalanceClassSampler(labels=k.train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        k.train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        k.validation_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        k.validation_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=validation_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    validation_tune_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        k.validation_tune_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "    validation_tune_loader = torch.utils.data.DataLoader(\n",
    "        k.validation_tune_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=validation_tune_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        k.test_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        k.test_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=test_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers\n",
    "    )\n",
    "\n",
    "    logger.debug(\"rank: %d\", rank)\n",
    "\n",
    "    if rank == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "    fitter = TPUFitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    fitter.fit(train_loader, validation_loader)\n",
    "    fitter.run_tuning_and_inference(test_loader, validation_tune_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FLAGS={}\n",
    "    #xmp.spawn(_mp_fn, args=(FLAGS,),  nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "output_model_file='XLMRobertaModel_tpu_trained.bin'\n",
    "torch.save(k.model.state_dict(), f\"{today}_{output_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wu0VhhZAFuYs"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-72827bbe0aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{ROOT_PATH}/submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{ROOT_PATH}/submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from kaggle_runner import logger\n",
    "ROOT_PATH='/content'\n",
    "os.chdir('/content')\n",
    "\n",
    "submission = pd.concat([pd.read_csv(path) for path in glob('node_submissions/*.csv')]).groupby('id').mean()\n",
    "submission['toxic'].hist(bins=100)\n",
    "\n",
    "submission.to_csv(f'{ROOT_PATH}/submission.csv')\n",
    "logger.info(f'{ROOT_PATH}/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRr-yzJ_yVTW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make[1]: Entering directory '/content'\n",
      "make[1]: command: Command not found\n",
      "make[1]: type: Command not found\n",
      "make[1]: type: Command not found\n",
      "xclip ~/.kaggle/kaggle.json -selection clipboard\n",
      "/bin/bash: xclip: command not found\n",
      "Makefile:270: recipe for target 'kaggle' failed\n",
      "make[1]: [kaggle] Error 127 (ignored)\n",
      "[ -d datas ] || mkdir datas\n",
      "kaggle datasets metadata -p datas/ k1gaggle/bert-for-toxic-classfication-trained\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "Downloaded metadata to datas/dataset-metadata.json\n",
      "cp datas/dataset-metadata.json datas/dm.json\n",
      "ls *.bin | grep -v \"last\" | xargs -I{} mv {} datas/\n",
      "cp node_submissions/* log.txt /kaggle/submission.csv datas\n",
      "cp: cannot stat '/kaggle/submission.csv': No such file or directory\n",
      "Makefile:280: recipe for target 'push_dataset' failed\n",
      "make[1]: [push_dataset] Error 1 (ignored)\n",
      "kaggle datasets version -p datas/ -m \"$(git show --no-patch --oneline) $(date)\"\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "Starting upload for file submission_0_299370_0.41567039716436693.csv\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "100% 139k/139k [00:01<00:00, 108kB/s]\n",
      "Upload successful: submission_0_299370_0.41567039716436693.csv (139KB)\n",
      "Starting upload for file submission_9_911062_0.6829431563806916.csv\n",
      "100% 142k/142k [00:01<00:00, 99.2kB/s]\n",
      "Upload successful: submission_9_911062_0.6829431563806916.csv (142KB)\n",
      "Starting upload for file log.txt\n",
      "100% 840/840 [00:02<00:00, 416B/s]\n",
      "Upload successful: log.txt (840B)\n",
      "Starting upload for file submission_0_299273_0.5080901901105664.csv\n",
      "100% 139k/139k [00:01<00:00, 80.5kB/s]\n",
      "Upload successful: submission_0_299273_0.5080901901105664.csv (139KB)\n",
      "Starting upload for file submission_12_912424_0.33505313566112316.csv\n",
      "100% 142k/142k [00:01<00:00, 96.1kB/s]\n",
      "Upload successful: submission_12_912424_0.33505313566112316.csv (142KB)\n",
      "Starting upload for file submission_0_299555_0.5825802709962914.csv\n",
      "100% 139k/139k [00:01<00:00, 94.0kB/s]\n",
      "Upload successful: submission_0_299555_0.5825802709962914.csv (139KB)\n",
      "Starting upload for file submission_9_911198_0.520737560877973.csv\n",
      "100% 143k/143k [00:01<00:00, 99.5kB/s]\n",
      "Upload successful: submission_9_911198_0.520737560877973.csv (143KB)\n",
      "Starting upload for file submission_0_299513_0.12126369429676764.csv\n",
      "100% 140k/140k [00:01<00:00, 85.8kB/s]\n",
      "Upload successful: submission_0_299513_0.12126369429676764.csv (140KB)\n",
      "Starting upload for file submission_14_932470_0.7551943254213536.csv\n",
      "100% 142k/142k [00:01<00:00, 95.9kB/s]\n",
      "Upload successful: submission_14_932470_0.7551943254213536.csv (142KB)\n",
      "Starting upload for file submission_0_299394_0.9897005345498059.csv\n",
      "100% 139k/139k [00:03<00:00, 43.6kB/s]\n",
      "Upload successful: submission_0_299394_0.9897005345498059.csv (139KB)\n",
      "Starting upload for file submission_0_299376_0.07493334396133755.csv\n",
      "100% 139k/139k [00:01<00:00, 127kB/s]\n",
      "Upload successful: submission_0_299376_0.07493334396133755.csv (139KB)\n",
      "Starting upload for file 2020-06-18_XLMRobertaModel_tpu_trained.bin\n",
      "100% 2.09G/2.09G [00:23<00:00, 96.7MB/s]\n",
      "Upload successful: 2020-06-18_XLMRobertaModel_tpu_trained.bin (2GB)\n",
      "Starting upload for file submission_8_910395_0.9327874057936533.csv\n",
      "100% 142k/142k [00:01<00:00, 90.1kB/s]\n",
      "Upload successful: submission_8_910395_0.9327874057936533.csv (142KB)\n",
      "Starting upload for file submission_15_353762_0.9786145791547287.csv\n",
      "100% 142k/142k [00:01<00:00, 87.5kB/s]\n",
      "Upload successful: submission_15_353762_0.9786145791547287.csv (142KB)\n",
      "Starting upload for file submission_9_911444_0.23613424934225313.csv\n",
      "100% 142k/142k [00:01<00:00, 90.2kB/s]\n",
      "Upload successful: submission_9_911444_0.23613424934225313.csv (142KB)\n",
      "Starting upload for file dm.json\n",
      "100% 228/228 [00:01<00:00, 161B/s]\n",
      "Upload successful: dm.json (228B)\n",
      "Starting upload for file submission_12_912741_0.5135713118649211.csv\n",
      "100% 142k/142k [00:01<00:00, 125kB/s]\n",
      "Upload successful: submission_12_912741_0.5135713118649211.csv (142KB)\n",
      "Starting upload for file submission_0_299495_0.03942352232779944.csv\n",
      "100% 139k/139k [00:01<00:00, 101kB/s]\n",
      "Upload successful: submission_0_299495_0.03942352232779944.csv (139KB)\n",
      "Starting upload for file submission_0_299271_0.5407010774021088.csv\n",
      "100% 139k/139k [00:01<00:00, 122kB/s]\n",
      "Upload successful: submission_0_299271_0.5407010774021088.csv (139KB)\n",
      "Dataset version is being created. Please check progress at https://www.kaggle.com/k1gaggle/bert-for-toxic-classfication-trained\n",
      "make[1]: Leaving directory '/content'\n"
     ]
    }
   ],
   "source": [
    "#!cp log.txt '/content/drive/My Drive/jigsaw2020-kaggle-public-baseline/'\n",
    "import os\n",
    "os.chdir('/content')\n",
    "\n",
    "!cp kaggle_runner/Makefile .\n",
    "!make push_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "id,colab_type,colab,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
